Herzlich willkommen zu meinem ersten Vortrag zum Master Thema Question Answering auf dem Lehrbuch Health Information Systems mit Hilfe von unüberwachten Training eines pre-trained Transformers.
In diesem Vortrag möchte ich den Moodle folgend anfangs über die Einleitung sprechen, den Gegenstand die Problematik die hier angesprochen wird und welche Zielen und Aufgaben ich diese Problematik lösen möchte.
Dann möchte ich etwas genauer auf den Stand der Forschung eingehen, insbesondere auf die Grundlagenforschung des continal pretraining was hier angewendet wird, welche aktuellen Modelle zur Verfügung stehen und welche interessanten Erforschung wir in diesem Modellen sehen um dann den Vortrag abzuschließen mit meiner aktuellen Zeitplanung.

Kommen wir zum Gegenstand und der Motivation - die Grundidee hinter dieser Arbeit ist dass die Informationsbeschaffung in der Medizininformatik und in der Medizin eine grundlegende Notwendigkeit ist um eine gute klinische Praxis durchzuführen.
Sowohl die Medizin als auch die Medizinformatik kann nur erfolgreich durchgeführt werden, wenn wir eine gute Informationsbeschaffung haben auf Basis guter Informationen
Ein Beispielprojekt dafür ist die SNIK-ontologie die hier verschiedene Literaturen kombiniert hat um Informationen über das Management von Informationssystemen im Gesundheitswesen, also für Informationen in der Medizininformatik, besser zu strukturieren und auch zugänglich zu machen.

Es gibt viele verschiedene Projekte die hier die Ontologie benutzten, um daraus Information zu extrahieren. Eins davon, ist die BeLL Arbeit von Brunch von 2022, in der versucht wurde auf der Ontologie ein QAS System zu erstellen, indem man hier Fragen in englischer natürliche Sprache stellen konnte und dann Antworten durch eine Query im Hintergrund von der Onkologie erhielt.
Allerdings in jüngster Zeit seit 2022 aufstrebend sind ja die sogenannten Sprachmodelle oder ChatGPT als Vorreiter.
Wenn man ChatGPT schon mal benutzt hat, dann sieht es so aus, als würde man da jetzt irgendwelche Fragen stellen können und erhält eine Antwor.
Da stellt sich ja die Frage, ist ChatGPT eine Wissensgrundlage, inwiefern können wir das hier als Wissensgrundlagen nutzen, inwiefern hilft das vielleicht auch uns bei der Informationsbeschaffung

ChatGPT VS question answering ist auch eine Fragestellung die sich Omar und andere 2023 gestellt haben. In ihrem Artikel haben sie dieses Modell verglichen mit einem SOTA QAS System  - das wäre hier das Knowledge Graph question answering System und dieser Vergleich hat heraus gebracht, dass chatgpt deutliche Vorteile bietet gegenüber einem state of art System und zwar in der Robustheit gegenüber Eingabe Fehlern, in der Erklärbarkeit von Antworten und in dem Fragen Verständnis von komplexeren Fragen.
Jedoch gibt es hier auch große Probleme.
Der Artikel selber zeigt, dass es nicht unbedingt viele Fragen beantworten kann - die Anzahl der korrekten Antworten war sehr gering.

Man muss allerdings auch hier immer bedenken dass in dem Fall ChatGPT ohne zusätzliche Bearbeitung benutzt wurde, hier wurde kein Modell weiter trainiert hier sondern das Grundmodell benutzt.
da stellt sich die Frage, vielleicht bekommen wir bessere Ergebnisse wenn wir auf eine Domäne Trainieren, können die Vorteile von GPT benutzen und die Nachteile hier minimieren.

über den Sachverhalt die Zielsetzung dieser Arbeit basiert dann darauf dass wir gerne ein konversationski erstellen möchten die als question Armstrong System funktioniert also ich möchte hier natürlich gestellte Fragen an dieses konversationski stellen und natürliche Sprache natürliche Sprache Antworten erhalten über das Thema Informationssystem im Gesundheitswesen explizit über das Buch von Winter und anderen 2023 und dann möchte ich gerne diese KI die diese Fragen beantworten kann evaluieren mit selbstgestellten Frage und welche Fragen bieten sich da am besten an natürlich Klausurfragen Klausur Fragen haben den Vorteil dass man hier sowohl einfache Fragen Fakten Fragen hat als auch komplexere Fragen Transfer Fragen über mehrere Informationen hinaus und das ist am besten wiedergespiegelt in Beispiel Klausuren und da gibt es hier z.B das Modul in der medizinformatik ich Architektur von Informationssystem Gesundheitswesen welches inhaltlich auch auf dem Buch aufbaut und da ist passt es quasi wie die Faust aufs Auge dass wir hier die Klausur benutzen um auch das Modell zu evaluieren das Ziel ist allerdings kein Produktivsystem ihr sollt kein TBT rauskommen am Ende womit man einfach chatten kann sondern es soll lediglich die Machbarkeit da aufzeigen dass man ein Modell auf gewisse Domäne und gewisse Literaturen anpassen kann um dann mit Hilfe dessen quasi eine einfragt das Buch zu erstellen die Aufgabenstellung dem folgend ist dementsprechend dass wir verschiedene Vorteile der sprachmodelle uns anschauen welche nutzen bereits sie haben welche Leistung sie haben können wir sie überhaupt nutzen würden sie überhaupt veröffentlicht ähm und dann dementsprechend eins dieser sprachmodelle auswählen als vortrainierte Sprachmodell bevor wir das Modell allerdings weiter trainieren müssen wie die Daten von dem blauen Buch erstmal kurieren solche Sachen wie Bilder Seitenzahlen Überschriften Vordruck oder den Buchband oder wie auch immer diese ganzen Sachen müssen entfernt werden damit wir so wenig wie möglich unnütze Information natürlich in das Modell einspeisen vielleicht Informationen die verwirrend wirken da auch entfernen so dass das Modell besser diese Daten versteht und dann werden wir kontinual pre-training anwenden um dieses Modell weiter zu trainieren Kontinent ist ein Fachbegriff der hier einfach nur beschreibt das ein Vorteil dieses Sprachmodell mit einem neuen Datensatz weiter trainiert wird über unüberwacht das Ziel zwei zu Evaluierung ist natürlich dass wir jetzt Klausur Fragen heran nehmen und ähm mit Hilfe dieser per Fuß oder diese generierten Antworten bewerten die werden diese Klausur fallen richtig beantwortet worden wir müssen natürlich das Modell vor und nach dem Training evaluieren um herauszufinden ob sich das Training überhaupt gelohnt hat ob sich da das eine Verbesserung gebracht hat oder vielleicht sogar Verschlechterung und dann möchte ich gerne dieses Modell mit der dem Date auf die Art Modell in dem Fall wäre das GPT 4 vergleichen und dann herausfinden ob es sich überhaupt auch in dem Fall lohnt hier auf Domäne und Literatur anzupassen oder ob vielleicht ein größeres Modell der bessere Weg ist kommen wir zum Grundlagenforschung und dem ersten Kapitel sozusagen der stand dem Stand der Forschung bzw das ist mittlerweile eigentlich Teil des Grundlagen Kapitels da möchte ich nur kurz hinweisen dass das Thema sprachmodelle das Thema Transformers was sind die Architektur dahinter ist gar nicht mal so alt ist im Vergleich zu unserem letzten die ja in dem 20 Jahrhundert schon aufkam Techniken nicht sehr gleich benannt sind dadurch es schwierig ist über gleiche Techniken zu reden wenn jedes Mal einen neuen Arme aufkommt hat sich kalian und andere zusammengesetzt um hier eine Taxonomie zu entwerfen für die Transformer und dieser Taxonomie werde ich größtenteils auch an dieser Arbeit Folgen continual pre-training wird da ist auch ein großer Begriff der vier untersucht ist und zeigt eindeutig dass wir eine Leistungssteigerung des Modells erhalten wenn wir dieses durch Kontinent weiter trainieren ähm die erste veröffentlichte dazu veröffentlich dazu war von Bradford Nachricht 1 von 2018 ähm und weiterführend war dann kam dann biobert von Leon und anderen 2019 witzigerweise ein paar von den paar von den Papern heißen tatsächlich don't stop preaching und zeigen einfach dass das Trainieren auf eine Domäne das Trainieren auf gewisse Aufgaben oder als generelle weiter trainieren die Leistung eines Modells deutlich steigert guangan und andere haben dann in ihrem paper herausgestellt welche wir wie deutlich die Leistung gestaltet werden wir reden von hier zehnmal so gut 60 mal so gut abhängig davon wie man hier weiter trainiert das heißt hier sieht man deutlich Verbesserungen da stellt sich natürlich dann die Frage warum sind wir eigentlich eine Leistungssteigerung wenn das wenn wir weiter trainieren und das haben dann Kapern und anderen in ihrem paper dann auch festgehalten und herausgefunden sie haben herausgefunden dass es drei Einflussgrößen gibt die die Leistung eines Modell steigern das ist einerseits die Größe des Modells dann die Länge der Trainingszeit und die Größe des Datensatzes die genutzt wurde alle drei Größen beeinflussen die Leistungssteigerung eines Hotels unterschiedlich wenn man also Kontinent anwendet dann würde man hier zwei dieser Einflussgrößen erhöhen und dadurch dann auch ein besseres Modell herausbekommen aber nicht nur Kontinent free Training verbessert das Modell sondern auch Feintuning ähm das haben Ziegel und andere 2019 ähm also sehr gut herausgehoben sie haben unterschiedliche feintuningarten hier angewendet und vereintuning bedeutet einfach dass ich das Modell nicht mehr unüberwacht weiter trainiere sondern dass ich diesbar ist überwacht weiter trainiere dass ich Datensätze benutze in den ich die Antworten kenne den Ground truth kenne und die Antworten des Modells sozusagen Abend abhängig von den korrekten Antworten bewerte in diese Arbeit werde ich kein Plan für den anwenden weil wir einfach nicht die Datensätze dafür haben die Label sind ohne Datensätze bzw nicht in der Menge und dadurch können wir hier keinen fein Tuning mehr anwenden was in irgendeiner Weise Sinn macht bei den aktuellen Modellen habe ich mich auf äh free trade Transformer Genesis Transformers fokussiert es gibt unterschiedliche Arten von Transformer Architekturen es gibt die direktionalen und die Uni direktionalen Architekturen es gibt ähm Text Klassifikationen Sentiment analysis und Text Generation Übersetzung Architekturen die unterschiedlich Aufgaben erstellen ein sehr bekanntes beliebtes Modell ist das wird Modell das wird Modell ist allerdings nur sinnvoll für Text Klassifikation und viel in the blank wäre das das Transformer Modell in dem Fall dafür spricht weil man hier eine Frage hat oder einen Anfangssatz wie man das in tschechische kennt und danach der Text danach kommt einfach weiter trainiert oder weiter generiert wird das ist das was wir wollen wir möchten antworten generieren auf Basis eines Textes oder Fragestellung vorher zu kommen und dir bitte Neo X ist dementsprechend das erste Modell was die Größe von jpt3 erreicht im Gegensatz zu Satz dazu ist das Lama Modell welches ein Modell von der Facebook Research Group ist zwischen 23 veröffentlicht interessanterweise nehmen Sie hier einen anderen Ansatz die erstellen nicht immer größere Modelle und mehr Leistung zu erhalten sondern sie gehen die anderen beiden Wege der Leistungssteigerung sie haben immer mehr Daten zum Training benutzen immer länger trainiert wir sprechen hier von Billionen von Tokens und Wörtern auf denen das trainiert wurde die GPU Modelle sind noch bei Milliarden bzw Millionen Wörtern und sie zeigen in ihrem Artikel und ihrem Modell dass sie hier bessere Leistungen als GPT erreichen mit wesentlich weniger und kleineren Modellen deswegen spricht auch Lama sehr gut für die Anwendungen dieser Arbeit da sie sehr kleine im Vergleiche Modelle benutzen die dann tatsächlich auch auf dem rechten Zentrum von der Universität Leipzig laufen können und trotzdem gleiche ähnliche Leistungen erreichen zur Erforschung von Modellen gibt es auch unterschiedliche Richtungen die man nehmen kann eine interessante Richtung ist die von Hausbier und andere auf dem aufbauen und dann Pfeife und andere ein ganzes Modell ganzes System aufgebaut haben und das sind die Adapter und der Adapter hat die Idee ist hier dass wir das general pre-training oder das Container Pre Training ersetzen und nicht mehr das Modell weiter trainieren müssen sondern dass wir nur einen Bruchteil jedes Modells weiter trainieren nämlich die Adapter man verändert hier die Architektur des neuronalen Netzes ist Transformers in dem man unterschiedliche kleinere neuronale Netze in die Ebenen einschiebt wie so ein Adapter und dann nur diese Adapter trainiert und alles andere bleibt so wie es ist der Vorteil davon ist dass man nicht eine Prozent aller Parameter trainieren muss dadurch nicht die große Menge an Rechenleistung aufbringen muss damit man es überhaupt erstmal trainieren kann und trotzdem vergleichbare Leistungen erhält sie haben mir gezeigt dass mit einem Unterschied von 0,4% Leistung sozusagen das Modell nur mit 4% Parametern trainiert werden kann weitere Untersuchungen in die Transformer Richtung insbesondere Transformer als Informationsquelle sind von petrole und anderen geführt die haben sie untersucht inwiefern das bird-modell als Wissensbasis oder als Informationsbasis genutzt werden kann inwiefern kann es wirklich Fakten wiedergeben sie haben hier glaube ich äh 30% von Fakten Wiedergabe gezeigt also das ist da in dem Moment schon relativ gut ja andere haben darauf aufgebaut und haben aber herausgefunden dass mehrsprachige Modelle deutlich schlechter abschneiden gegenüber ja gegenüber normalen einsprachigen Modellen weil hier die der Großteil der Modelle sozusagen ein Großteil der Kapazität darauf aufgeht dass man erstmal die Besetzung lernen muss eine interessante Untersuchung ist von drei und anderen die herausgefunden haben das gewisse Neuronen relativ weit hinten in dem neuronalen Netz mit Fakten korrigieren das bedeutet wenn in der Eingabe ein Fakt genannt wird über den Fakt besprochen wird oder erwähnt wird dann leuchtet dieses eine holen auf sozusagen wird aktiviert und wenn er nicht vorhanden ist dann bleibt dieses inaktiv der Vorteil davon ist dass ich dieses Neuron unterdrücken kann bzw verbessern oder das beschleunigen kann und dadurch erhalte ich dann sozusagen ein Modell was diesen Fakt vergiss vergessen hat oder sich mehr auf diesen Fakt beruft ganzes paper dazu auch dediziert und das zweite payPal ist hier von der husch von 2021 der sich Fragen zu über urbericht stellt hier interessantes halt dass dieses Thema aktuell 2023 sozusagen sehr heiß her geht in den USA und da keine klare Frage Antwort auf diese Fragestellung ist aber die Frage ist ja wenn ich jetzt Text generiere oder Bilder generieren lasse oder wie auch immer wer ist der Urheberrecht ist der dem das Modell benutzt der Urheberrecht ist der hat auf dem die Daten basieren der urbericht oder ist der der dieses Modell angeleitet hat um den Text zu generieren das Urheberrecht da kann man aktuell keine klare Grenze ziehen auf alle Fälle ein interessantes paper und drüber nachzudenken zum Schluss möchte ich über meine Zeitplanung lesen wir sind hier folgend dem Moodle angesagten Zeitplan ungefähr ein grünes abgeschlossen gelb bin ich dabei rot habe ich noch vor mir ähm ich habe hier den Zeitplan nur auf 5 Monate aufgestreckt tatsächlich weil die Definition der Aufgabe nicht zum Zeit äh. Gehört allerdings fange ich jetzt an mit der Lösungsansatz Erarbeitung und dem Schreiben da stand davor bin ich schon fast Färber durch ich fange jetzt eher in dem Schreiben der Grundlagen an wer das Eisen Lösungsansatz arbeiten und danach hoffentlich eine Besprechung mit meinem Betreuer haben um dann sozusagen die zweiten Vortrag auch hier halten zu können mit einem Lösungsansatz dann wird die Implementierung dieses Lösungsansatz kommen und dann möchte ich ja über diese Implementierung natürlich reden und gewisse Diskussions topics mir noch überlegen die dann auch weiter hinten geschrieben werden und dann habe ich mir hinten sozusagen mit dem zusammenfassen und Probleme lesen und dem zweiten Monat der dahinten noch kommt zwei Monate Zeit genommen indem ich jetzt nicht frei machen möchte sondern das sind Puffer Monate so weil ich weiß dass ich das ganze implementieren und schreiben darüber auch durchaus nach hinten Schrecken kann möchte ich hier auf alle Fälle ein Puffer haben den ich ausnutzen kann und zum Abschluss dann hier noch die Quellen die ich in diesem Vortrag benutzt habe die sind auch hauptsächlich dann die sind eigentlich alle dann in der Arbeit auch so genannt vielen Dank für die Aufmerksamkeit