#!/bin/bash
#SBATCH --job-name=lora1        # name
#SBATCH --nodes=1                           # nodes
#SBATCH --ntasks-per-node=1                 # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=2
#SBATCH --partition=clara
#SBATCH --mem=64GB
#SBATCH --time=2-00:00:00                     # time limit hrs:min:sec
#SBATCH --gres=gpu:v100:1                   # number of gpus
#SBATCH --output=logs/%x-%j.out                  # output file name
#SBATCH --mail-type=ALL

module load Python
module load PyTorch
source .env/bin/activate

srun pip install bitsandbytes datasets accelerate loralib torch
srun pip install git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
srun pip install sentencepiece

srun nvidia-smi

srun python 03_train_lora.py