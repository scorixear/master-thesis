Attention is all you need
- none

AMMUS
- Scaling Laws - DONE

Deep Residual Learning
- none

Dropout
- none

ChatGPT versus
- none

Improving Language Understanding
- none

Dont't Stop Pretraining
- continued pretraining on domain-specific unlabeled data shows benefit
  - BioBERT: a pre-trained biomedical language representation model for biomedical text mining (DONE)
- 

Fine-Tuning
- in some scases fine-tuning not necessary
  - Language Models are Unsupervised Multitask Learners GPT2 (DONE)

Scaling Laws
- none

Few-shot Learners
- none

GPT4 Technical
- none

GPt-NeoX
- GPT-J

LLaMa
- bytepair encoding

AdapterHub
- ff-nn with bottleneck

Knowledge Neurons
- softmax und key/value

GPT4 System Card
- none

Plagiarism
- none