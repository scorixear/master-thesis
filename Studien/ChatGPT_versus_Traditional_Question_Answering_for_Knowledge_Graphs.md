# Paper Information
- arxiv publish
- missing journal publish
- Feb, 2023
https://arxiv.org/abs/2302.06466

# 0. Abstract
## Conversational AI
- simulates conversation with humans
- limited by data caputred in training datasets

## Question Answering Systems
- retrieve most recent information from knowledge graph
- understand and translate natural language questions into queries

## Goal
- combine both world into novel kg chatbots
- compare chatgpt and galactica vs KGQAn

# 1. Introduction
- conversational ai simulate conversations to achieve tasks, participate in  general discussion (cites)
- qas focus on retrieving information from knowledge graphs
- qas answer is structured (excel, csv), no explanation

## envison
- meet diverse needs for end users
- maximize usability of knowledge graphs
- maintain knowledge graphs with dynamic context

## Question Answered in this paper
- how does a language model, such as ChatGPT, compare against traditional QUASs for KGs?

## Future KG Chatbots
- update response generated by language model
- get responses that reflect recent informattion

## KGQAn
- question answwering is formalized in three-fold problem
    - Understanding: Seq2Seq pre-trained language model (such as  BART, GPT-3)
    - Linking & Filtering: just-in-time approach based on word embedding models (FastText)

## Evaluation
- comparative framework for systematically reviews
- four real KGs from various application domains

# 2. Background
- BERT, T5, GPT-3 solve question answering using text generation
    - given the input, directly produce output
- other models have to be finetuned using prompting

- language model classification
    - general models (PALM, OPT, GPT-NeoX-20B)
    - specific domains models (Galactica, SciBERT, BioMegatron)
- answer questions by engagingwith users in conversation (ChatGPT, LaMDA)
- webGPT = fine-tuned GPT-3 with internet access

## ChatGPT
- answers questions
- ask for clarification
- creates dialogues
- applies reasoining for self correction
- trained  using reinforcment learning with human feedback (similar to InstructGPT)
- uses new dialogue dataset and InstructGPT dataset

## Galactica
- general purpose scientific language model
- prediciting citations, reasoning, question answering, prediciton protein annotations
- trained on open-access scientific resources, general knowledge such as Wikipedia

## QAS for KG
- understanding: extract key entities and relations from given question, form graph from that
- linking: map entities and relations to verticies and predicates from KG
- organizing: create SPARQL query, retrieve information, optional filtering

## EDGQA
- understanding: Stanford core NLP parser
    - human curates heuristic rules, transforms tree into root-acyclic graph (entity description graph)
- linking: linked to verticies with Falcon method
- preprocessing: extract all entity types
- organizing: create SPARQL query

## KGQAn
- understanding: triple-patterns generation model (Seq2Seq pretrained models, BART, GPT-3)
    - converted to Phrase Graph Pattern
- linking: just in time, built in indices in KG engine
    - word embedding models (FastText)
    - assess semantic affinity between differnet phrases
- not dependend on any pre-processing index

# 3. Comparative Framework
## 3.1 Criteria Towards KG Chatbots
### Correctness
- QA Task focues on extracting acurate answers
- essential criteria
- comparing answers to included benchmarks ground truth
- micro-F1 Score = primary metric
- evaluation script for QAS
- manual process for GPT
- $m_{prec}=\frac{\sum_{i=1}^{N}{|C_{qi}|}}{\sum_{i=1}^{N}{|S_{qi}|}}$
- $m_{recall}=\frac{\sum_{i=1}^{N}{|C_{qi}|}}{\sum_{i=1}^{N}{|G_{qi}|}}$
- $microF1=\frac{2*m_{prec}*m_{recall}}{m_{prec}+m_{recall}}$
- $S_q$ = answers generated by system
- $G_q$ = ground truth answers
- $C_q$ = correct answers generated by system
- $m_{prec}$ = micro-precision
- $m_{recall}$ = micro-recall

### Determinism
- QAS by design deterministic
- GPT have degree of non-deterministic behaviour
- running each question three times
- calculating correctness based on best answers

### Robustness
- tolerate erronous input (spelling, grammar)
- QAS simply mismatch leads to wrong answers
- GPT robust by design
- injecting spelling/grammar mistakes in subset of questions
- evaluating correctness

### Explainability
- provided explanations, additional details
- increases confidence about answers
- common in language models
- qas not possible
- evaluated manually by observing outputs

### Question understanding
- understand question, regardless of correct answer
- different degrees of complexity: aggregate, temporal, multiple-intentions
- QAS = separate phase, observing output of first phase
- language models = analyze generated answers, decide if model understood the question

### Incorporating recent information
- frequently updates
- criterion depends on adopted process for training models

### Generality across different domains
- support question answering of various domains without retraining
- use four real KGs of two domain
- human-curated questions of different complexity and styles

## 3.2 Fairness and Manual Assessment
- use differnt version of same knowledge
- challening to evaluate models based on same criteria
- manual evaluation, considering multiple factors
- list-question easy to compare (both answers are lists)
- "Who founded Intel?" produces paragraph
    - manually search for expected answer in text
    - compare to ground truth
- consider answers in period covered by KG (2016, vs ChatGPT until 2021)

# 4.Experiment Evaluation
## 4.1 Compared Models and Systems
- KGQAN, EDGQA as QAS
- Galactica, ChatGPT as Language models
    - 6.7B parameters Galactica model
    - ChatGPT Models
        - Default: three runs per question
        - Follow-up: entire list of answers, by promting continue
        - Excel: request structured format with predefined Message
    - ChatGPT Answers classification
        - correct
        - wrong
        - no answer: did not understand entity, was not trained on dataset
## 4.2 Benchmarks
- four Benchmarks
- two on general-fact KGs (DBpedia,YAGO)
  - general-facts: places and persons
- two on academia related (DBLP, MAG)
  - scientific publicaton, citations, authors, institutions
- 3 categories of questions
  - boolean
  - list
  - WH-questions
- QUAlD-9 benchmark for QASs (DBpedia), 150 questions, human generated
- YAGO, DBLP, MAG, 100 questions, human generated

## 4.3 Performance Evaluation
- table 1 for precision, recall and micro F1
- precision = number of correct answers / number of answers
- recall = number of correct answers / number of ground truth answers

- KGQAN comparable for general KGs

- ChatGPT significantly better in general KGs vs academic KGs
- does not add irrelevant answers => high precision
- struggles with recall, not fully answer questions

- EDGQA comparable for general KGs

- KGQAN better precision & recall for academica vs EDGQA (mainly trained on QALD-9)
- ChatGPT-Excel better results than ChatGPT
  - surpasses KGQAN in YAGO KG
- ChatGPT-Follow-up outperformed ChatGPT-Excel
    - for full potential, engage ChatGPT in conversations

- Galactica bad performance on list questions
- affecting recall and F1 score

## 4.4 Determinism
- KGQAN always deterministic
- ChatGPT
  - slight change in explanation
  - output utterly changes
    - no answer => correct answer
    - no answer => wrong answer
  - QALD-9 68%
  - YAGO 85%
  - DBLP 94%
  - MAG 91%

## 4.5 Chatbot-oriented Criteria
- Robustness
  - five random questions, spelling and gramatical mistakes
  - no problem for ChatGPT
  - not possible for KGQAN
- Explainability
  - ChatGPT reasonable explanations
  - KGQAN no explanations
- Question Understanding
  - total 450 questions
  - ChatGPT perfect understanding of questions
  - if no answer provided, still helpful tip for user (understands question, has Data not in dataset)
- different linguistic complexity
    - Single Fact questions = single piece of information
    - Single Fact questions with type = single piece, but answer is in the question
    - multi fact questions = more than one fact in question
    - boolean questions
    - Chat GPT correct answer in any of three answers
    - good performance in general knowledge, but not reflected in F1 score due to low recall

## 4.6 KG-oriented Criteria
- KGQAN good precision, recall and f1 score
- ChatGPT good in general benchmarks (>50%)
- ChatGPT not good in academia (max(20%)) => not trained enough on academic information
- KGQAN independent from KGs, if KG updates, KGQAN still works
- ChatGPT requires further training if updated information

# 4.7 Discussion
- both high percentage of questions in general knowledge graph
- both deterministic
- ChatGPT lower recall for questions with long list answers
- ChatGPT not good with DBLP and MAG
- ChatGPT does not generalize to unseen domain information
- ChatGPT outstanding in explainability and robustness
- KGQAN good performance across different KGs
- KGQAN need improvement in explainability, robustness and question understanding

# 5. KG Chatbots: Open Research Challenges
Dialogue:
- question understanding and linking
- modify graph with followup dialogue
- new benchmark that considers dialogue
Advanced Features:
- two intentions questions = two questions in one
- count = question about number of answers
- temporal = facts in time frame
- show steps how system got answer (Explainability)
- privelege for user feedback (followup or correction of information)
- show why no answers are produced (no answer in KG or error in pipeline)

# 6. Conclusion
