%*****************************************
\chapter{Stand der Forschung}\label{ch:relatedWork}
%*****************************************
In diesem Kapitel werden aktuelle Artikel und Forschungsergebnisse zu den Themen Sprachmodelle, Continual Pretraining und Problemen bei der Nutzung von Sprachmodellen vorgestellt.

\section[Continual Pretraining]{Continual Pretraining und die Nutzung von Sprachmodellen}
Ein Transformer-Modell als Wissensbasis wird in \citet{chatgpt_qas} mit verschiedenen \ac{sota}-Modellen verglichen.
Sie zeigen eine deutliche Verbesserung der Robustheit gegenüber Eingabefehlern, der Erklärbarkeit von Antworten und des Fragenverständnisses bei komplexeren Fragen mit mehreren Fakten durch ChatGPT,
zeigen aber Probleme bei der Aktualität von Wissen, dem Wissen über spezifische Domänen und vor allem bei der korrekten Beantwortung von Fragen.
Der Grund dafür ist eine grundlegende Eigenschaft von \ac{gpt}-Modellen, nämlich die fehlende Inkorporation aktuellen Wissens.
Das Training von \ac{gpt}-Modellen ist ein aufwendiger Prozess und kann nicht bei jeder Inferenz (Nutzung des Modells durch Generierung von Text) durchgeführt werden.
Darüber hinaus wurde ChatGPT ohne zusätzliches Continual Pretraining eingesetzt.
Eine Anpassung an die Domänen und eine Verbesserung der Korrektheit der Antworten stehen daher noch aus.\\

\citet{gpt1} zeigen die Verbesserung der Leistung von Transformer-Modellen durch generatives Pretraining und eine weitere Verbesserung durch überwachtes Finetuning.
Auch hier ist ein klarer Trend erkennbar.
Die Ergebnisse der Modelle verbessern sich mit zunehmender Größe des Datensatzes, mit zunehmender Länge des Trainingsprozesses und mit zunehmender Größe der Modelle.\\

Diese Tendenz wird durch \citet{scaling_laws} bestätigt, die hier den Einfluss verschiedener Einflussgrößen auf die Gesamtleistung eines Modells berechnen.
Die hier verwendeten Einflussgrößen erlauben eine Vorhersage der Leistung eines Modells.
Der Artikel schließt mit einer Abschätzung der theoretischen Maximalleistung und damit der Maximalgröße von Transformer-Modellen.\\

Um den beschriebenen Skalierungsregeln zu folgen, aber die Trainingszeit und die benötigte Datenmenge zu reduzieren, gibt es die Möglichkeit des Continual Pretraining.
\citet{dont_stop_pretraining} hat diese Methodik angewandt und gezeigt, dass Modelle immens davon profitieren,
domänenspezifisches Wissen zu adaptieren und aus der großen Menge an Basisdaten bessere korrekte Antworten in einer spezifischen Domäne zu generieren.
Erstmals in \citet{biobert} eingesetzt, um das Basismodell \ac{bert} an die biomedizinische Domäne anzupassen,
erweiterten \citet{dont_stop_pretraining} diese Methode und zeigten ihre Anwendbarkeit auf verschiedene Domänen und Aufgaben.
Continual Pretraining auf aufgabenspezifischen Daten verbessert wiederum die Leistung für spezifische Aufgaben,
während die Trainingszeit um den Faktor 60 kürzer ist im Vergleich zu Continual Pretraining auf Domänen.
Eine Kombination beider Arten liefert die besten Ergebnisse.\\

Aber nicht nur das Continual Pretraining verbessert die Ausgabe der Modelle, sondern auch das überwachte Finetuning.
\citet{finetuning} beschreiben in ihrem Artikel die Effektivität von Reinforcement Learning als Finetuning-Methode zur Lösung von Aufgaben der Weiterführung und Zusammenfassung von Texten.
Finetuning benötigt jedoch gekennzeichnete Daten (engl. \enquote{labeled data}, Daten mit bekannten korrekten Ausgaben),
die in der Regel aufwendig zu erstellen sind und nicht immer in der benötigten Menge zur Verfügung stehen.
In dieser Arbeit wird aufgrund der mangelnden Verfügbarkeit von gekennzeichneten Daten auf ein Finetuning verzichtet.\\

\section{Aktuelle Modelle und deren Nutzbarkeit}

Die Erkenntnis, dass die Leistung von Modellen mit zunehmender Größe, Trainingszeit und Datenmenge steigt, hat zur Entwicklung einer Reihe von Modellen mit unterschiedlichen Architekturen, Anwendungsfällen und Leistungen geführt.
Ein erster Durchbruch in der Leistungsfähigkeit von Transformer-Modellen wurde von \citet{gpt2} mit dem Modell \ac{gpt}-2 erzielt.
Im Vergleich zum ersten veröffentlichten \ac{gpt}-1 Modell \citep{gpt1} zeigten sie, dass Sprachmodelle Aufgaben lösen können, ohne explizit überwacht zu werden.
Ebenso stellten sie fest, dass die Größe eines Modells, sei es hier die Anzahl der Parameter,
die Größe des Datensatzes oder die Länge des Trainings, eine grundlegende Notwendigkeit für den erfolgreichen Einsatz der ZeroShot-Methode ist.
Bereits hier konnten sie ohne jegliche Änderung der Architektur im ZeroShot Rahmen je nach Aufgabenstellung erfolgreiche, \ac{sota}-kompetitive Ergebnisse erzielen.\\

OpenAI gelang mit \ac{gpt}-3 ein Durchbruch in der Popularität und beschrieben ihren Ansatz in \citet{gpt3}.
In ihrem Artikel demonstrierten sie die Leistungssteigerung durch größere Modelle und zeigten, dass diese Leistung auch ohne Finetuning erreicht werden kann.
Sie verglichen auch das Antwortverhalten in Abhängigkeit von FewShot- und ZeroShot-Eingaben, wobei erstere bessere Ergebnisse lieferten.
Diese Ergebnisse unterstützen die Hypothese, dass das in dieser Arbeit verwendete Modell auch ohne Finetuning eine gute Leistung erzielen kann.\\

Weiter im Jahr 2023 veröffentlichte OpenAI \ac{gpt}-4 und stellten es in \citet{gpt4} vor.
Neben deutlich besseren Ergebnissen durch ein noch größeres Modell mit mehr Parametern gelang es nun auch, Bilddaten als Eingabe zu verarbeiten.
Dieser Artikel unterstreicht erneut die Annahme, dass größere Modelle eine bessere Leistung und ein besseres Verständnis der natürlichen Sprache haben.
Eine Verwendung dieses Modells sowie des Modells \ac{gpt}-3 ist nicht möglich, da diese Modelle derzeit nicht veröffentlicht sind.\\

Im Gegensatz zu den Modellen, die von OpenAI publiziert wurden, veröffentlichten \citet{gpt_neox} \ac{gpt}-NeoX.
Ein Modell, das in Größe und Leistungsfähigkeit \ac{gpt}-3 ähnelt, jedoch auf der Architektur von
\ac{gpt}-J\footnote{Ben Wang \url{https://github.com/kingoflolz/mesh-transformer-jax} (abgerufen am 3.6.2023)} basiert und im Open Source Rahmen veröffentlicht wurde.
Sie zeigten, dass die meisten interessanten Fähigkeiten eines Modells erst ab einer bestimmten Anzahl von Parametern sichtbar werden.\\

Zuletzt wurden von \citet{llama} die \ac{llama}-Modelle in verschiedenen Größen veröffentlicht.
Ein klarer Vorteil gegenüber anderen Modellen in ihrer Anwendbarkeit ist hier der Fokus auf eine längere Trainingszeit und einen größeren Datensatz gegenüber der Modellgröße. 
Sie zeigten in fast allen Aufgabenbereichen bessere Ergebnisse als andere Modelle wie \ac{gpt}-3 und \ac{palm} mit deutlich weniger Parametern.
Damit sind diese Modelle billiger, einfacher und schneller im Training und in der Anwendung mit gleichen oder besseren Ergebnissen.
Diese Modelle wurden ebenfalls veröffentlicht und sind daher für diese Arbeit verfügbar.

\section{Forschung und Probleme von Modellen}
Neben der Entwicklung neuer Modelle wurden auch neue Ansätze zur Verbesserung des Continual Pretraining und der Adaption von Modellen entwickelt.
\citet{adapterhub} stellten in ihrem Artikel Adapter vor, die es ermöglichen, zusätzliche \ac{nn}s auf verschiedenen Ebenen der Transformer-Architektur einzusetzen.
Mit ihrer Hilfe kann die Adaption an andere Aufgaben und Domänen erreicht werden, ohne dass das gesamte Modell kontinuierlich vortrainiert werden muss,
da während des Trainings alle Parameter des Ausgangsmodells fixiert bleiben, während die neu eingefügten Adapter trainiert werden.
Darüber hinaus können bereits vortrainierte Adapter zu weiteren Domänen und Aufgaben in bestehende Modelle eingefügt werden, ohne dass ein erneutes Training erforderlich ist.
Das veröffentlichte System basiert auf dem Artikel von \citet{adapter_build_on}, in dem die Autoren das \ac{bert}-Modell auf 26 verschiedene \ac{nlp}-Aufgaben trainierten,
mit einer Anpassung von nur \SI{3,6}{\percent} der Parameter und einer Leistungsminimierung von \SI{0,4}{\percent} (ein ursprüngliches Training der Modelle auf diese Aufgaben hätte \SI{100}{\percent} aller Parameter angepasst).
Sie bewiesen damit die Effizienz dieses Ansatzes, ohne die Leistung wesentlich zu beeinträchtigen.\\

\citet{knowledge_neurons} untersuchten die Fähigkeiten von \ac{llm}s im Hinblick auf ihre Fähigkeit,
faktisches Wissen wiedergeben zu können, ohne eine Datenbank mit Fakten als Grundlage während des Betriebs zur Verfügung zu haben.
Sie stellten fest, dass vor allem in tieferen Ebenen die neuronalen Netze so genannte \enquote{Wissensneuronen} besitzen, die mit bestimmten Fakten korrelieren.
Diese Wissensneuronen werden aktiv, wenn ein bestimmter Fakt in der Eingabe angesprochen wird und können durch Verstärkung oder Unterdrückung dazu führen, dass das Modell diesen Fakt besser berücksichtigt oder \enquote{vergisst}.\\

Die Untersuchung von Modellen und ihrer Fähigkeit, Sachverhalte zu erlernen und zu reproduzieren, wurde erstmals von \citet{knowledge_base} vorgestellt.
Die hier verwendeten Modelle \ac{bert} und \ac{elmo} wurden auf ihr Potential als unüberwachte offene Domäne \ac{qas} untersucht und zeigten gute Ergebnisse im Vergleich zu anderen \ac{sota}-Systemen.
Mehrsprachige Modelle wurden von \citet{xfactr} auf die gleichen Eigenschaften hin untersucht, schnitten jedoch deutlich schlechter ab.
Diese Ergebnisse deuten darauf hin, dass ein mehrsprachiges Modell für den Einsatz als \ac{qas} deutlich schlechter geeignet ist,
da ein Großteil der Leistung dieser Modelle in das Verstehen von Übersetzungen in andere Sprachen fließt.\\

Neben den großen Erfolgen der neuen Modelle treten jedoch auch neue Probleme bei der Anwendung dieser Modelle auf.
Neben Falschaussagen ergeben sich Probleme durch soziale und andere Biases in den Antworten, Selbstüberschätzung bei Falschaussagen,
die wiederum zu schwerwiegenden Problemen in der Anwendung dieser Modelle führen können, Generierung von schädlichen Inhalten, Unterstützung von Kriminalität durch Expertise und andere Probleme.
Eine Analyse der Ergebnisse dieser Arbeit in Bezug auf die ethischen Richtlinien der \ac{gmds} findet sich in \cref{sec:gmds_ethik}.
\citet{gpt4} widmeten einen eigenen Abschnitt ihres Artikels der Untersuchung dieser Probleme und ihrer Adressierung.
Sie zeigen darin grundsätzliche Probleme bei der Anwendung von Sprachmodellen auf, halten sich aber mit konkreten Lösungsvorschlägen zurück.\\

In \citet{plagiarism} beschreibt der Autor weitere Fragen zum Umgang mit Antworten von Sprachmodellen und deren Konflikt mit dem Urheberrecht.
Wem gehört der generierte Text - den Autoren der Datensätze, auf denen das Modell trainiert wurde, der Firma, der das Modell gehört, dem Benutzer, der das Modell anleitet? 
Auch hier zeigen sich ungelöste Probleme in der Anwendung von Sprachmodellen und bieten Raum für weitere Forschung.
Eine abschließende Antwort auf diese Fragen gibt es noch nicht,
so dass die Verwendung eines Sprachmodells zum Zeitpunkt dieser Arbeit nur von der Lizenzierung des jeweiligen Modells durch die Autoren des Modells und der Lizenzierung der Datensätze,
die für das Continual Pretraining verwendet werden, abhängt.
Das hier verwendete Buch \citet{bb} steht unter einer Open-Access-Lizenz und kann daher uneingeschränkt für ein Continual Pretraining verwendet werden.\\
