%*****************************************
\chapter{Ergebnisse}\label{ch:results}
%*****************************************
\newcommand{\gpt}{\texttt{gpt$4$}}
\newcommand{\lo}{\texttt{llama$2$\_$0$e}}
\newcommand{\liv}{\texttt{llama$2$\_\allowbreak{}$1$e\_v$100$}}
\newcommand{\lia}{\texttt{llama$2$\_\allowbreak{}$1$e\_a$30$}}
\newcommand{\lev}{\texttt{llama$2$\_\allowbreak{}$3$e\_v$100$}}
\newcommand{\lea}{\texttt{llama$2$\_\allowbreak{}$3$e\_a$30$}}
\newcommand{\lsa}{\texttt{llama$2$\_\allowbreak{}$5$e\_a$30$}}
\newcommand{\lioa}{\texttt{llama$2$\_\allowbreak{}$10$e\_a$30$}}

\newcommand{\pic}[5][1]{
    \begin{figure}
        \makebox[\textwidth][c]{\includegraphics[width=#1\textwidth]{#2}}
        \caption[#3]{#4}\label{#5}
    \end{figure}
}
\newcommand{\pich}[5][1]{
    \begin{figure}[H]
        \makebox[\textwidth][c]{\includegraphics[width=#1\textwidth]{#2}}
        \caption[#3]{#4}\label{#5}
    \end{figure}
}
\marginpar[]{Ausführliche Ergebnisse und die verwendeten Datensätze sind unter \url{https://doi.org/10.5281/zenodo.8363501} verfügbar.}
In dieser Arbeit wurde die Konzeption und Implementierung eines Continual Pretrainings von Llama-Modellen beschrieben.
Diese Modelle wurden anschließend nach den in \cref{sec:approach:comparison} genannten Kriterien Korrektheit, Erklärbarkeit, Fragenverständnis und Robustheit verglichen.
Die Durchführung dieses Vergleichs ist in \cref{sec:evaluation} beschrieben.
In diesem Kapitel werden die Ergebnisse dargestellt und analysiert, wobei auch hier eine Unterteilung nach den Kriterien erfolgt.
Die zu vergleichenden Modelle sind in \cref{tab:eval-models} aufgeführt.


\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Modell} & \textbf{Epochen} & \textbf{Grafikkarten} & \textbf{Bezeichnung} \\
        \midrule
        GPT4            & -                & -                     & \gpt{}               \\
        Llama 2 7B      & 0                & Nvidia Tesla A100     & \lo{}                \\
                        & 1                & Nvidia Tesla V100     & \liv{}               \\
                        & 1                & Nvidia Tesla A30      & \lia{}               \\
                        & 3                & Nvidia Tesla V100     & \lev{}               \\
                        & 3                & Nvidia Tesla A30      & \lea{}               \\
                        & 5                & Nvidia Tesla A30      & \lsa{}               \\
                        & 10               & Nvidia Tesla A30      & \lioa{}              \\
        \bottomrule
    \end{tabular}
    \caption[Evaluierte Modelle]{Evaluierte Modelle, deren Anzahl an Epochen, genutzte Grafikkarten und Bezeichnungen in den Grafiken.}\label{tab:eval-models}
\end{table}

\section{Analyse Korrektheit}\label{sec:results:correctness}
\subsection{Vergleich totaler Zahlen}
\pich{results/answers_total.png}{Vergleich evaluierter Modelle}{Vergleich evaluierter Modelle und ihren totalen Leistungen bei der Beantwortung von Fragen.}{fig:results:answers_total}

\cref{fig:results:answers_total} zeigt die Verteilung der richtigen, falschen und unbeantworteten Fragen der evaluierten Modelle.
Allen Modellen wurden dieselben $95$ Fragen mit identischem Kontext vorgelegt.
Diese absoluten Zahlen zeigen eine grobe Tendenz der Ergebnisse, spiegeln aber nicht die tatsächlichen Leistungen wider.
Eine Frage gilt als richtig beantwortet, wenn mindestens eine richtige Antwort gegeben wurde.
Dies führt zu irreführend guten Ergebnissen.
Dennoch ist deutlich zu erkennen, dass GPT4 den Llama-Modellen deutlich überlegen ist.
Das untrainierte Llama-Modell beantwortete \SI{28}{\percent} der Fragen mit mindestens einer richtigen Antwort, konnte aber auch einen Großteil der Fragen beantworten und lieferte bei \SI{11}{\percent} keine Antwort. \\

Das \liv-Modell liefert mit \SI{2}{\percent} die wenigsten korrekten Antworten und zeigt ein generelles Verlernen grundlegender sprachlicher Fähigkeiten.
Im Gegensatz dazu zeigt das \lia-Modell eine vergleichbare, wenn auch etwas schlechtere Leistung als das untrainierte Modell.\\

Mit dem \lev-Modell konnte eine signifikante Leistungssteigerung im Vergleich zu einer Epoche erzielt werden, die Gesamtleistung ist jedoch dauerhaft schlechter als beim untrainierten Modell.
Das \lea-Modell hingegen übertraf erstmals die Leistung des untrainierten Modells und beantwortete \SI{43}{\percent} der Fragen mit mindestens einer richtigen Antwort.\\

Modelle mit höherer Epoche wurden ausschließlich auf A30-Grafik-\allowbreak{karten} trainiert und zeigten keine weitere Leistungssteigerung, beantworteten aber kontinuierlich weniger Fragen.
Hier zeigt sich eine Präferenz für keine Antwort gegenüber einer falschen Antwort.\\

\subsection{Stärken und Schwächen der Modelle}\label{subsec:results:correctness:strengths}
Neben der Gesamtleistung der Modelle wurde auch eine Unterteilung nach Fragetypen und Fragequellen vorgenommen.
Ähnliche Grafiken wie in \cref{fig:results:answers_total} sind in den ergänzenden Materialien zu dieser Arbeit enthalten.

\begin{table}
    \centering
    \begin{tabular}{lcl}
        \toprule
        \textbf{Fragetyp} & \textbf{Anzahl der Fragen} & \textbf{Bezeichnung} \\
        \midrule
        Singulär-Fakt     & 34                         & \enquote{single}     \\
        Multi-Fakten      & 38                         & \enquote{multi}      \\
        Transfer          & 23                         & \enquote{transfer}   \\
        \bottomrule
    \end{tabular}
    \caption[Fragetypen des Evaluierungsdatensatzes]{Fragetypen die im Evaluierungsdatensatz enthalten sind.}\label{tab:eval-question-types}
\end{table}


Die verschiedenen Fragetypen sind in \cref{tab:eval-question-types} aufgelistet.
In den folgenden Grafiken werden die entsprechenden Bezeichnungen zur Beschriftung der Achsen verwendet.\\
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{p{4.5cm}cl}
        \toprule
        \textbf{Fragequelle}                                                                                                                               & \textbf{Anzahl der Fragen} & \textbf{Bezeichnung}       \\
        \midrule
        \citetitle{bb} von \citet{bb}                                                                                                & 33                         & \enquote{book}             \\
        \midrule
        Mündliche Klausurfragen aus dem Modul \enquote{Architektur von Informationssystemen im Gesundheitswesen} vom Jahr 2021                             & 22                         & \enquote{A\_2021}          \\
        \midrule
        Schriftliche Klausurfragen aus dem Modul \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022                     & 9                          & \enquote{IS\_2022\_07\_18} \\
        \midrule
        Schriftliche Klausurfragen aus der Nachholklausur des Moduls \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022 & 31                         & \enquote{IS\_2022\_09\_27} \\
        \bottomrule
    \end{tabularx}
    \caption[Fragequellen des Evaluierungsdatensatzes]{Fragequellen die im Evaluierungsdatensatz enthalten sind.}\label{tab:eval-question-sources}
\end{table}


Unterschiedliche Fragequellen sind in \cref{tab:eval-question-sources} aufgelistet.
Da die Quelle \enquote{IS\_2022\_07\_18} nur 9 Fragen enthält, ist eine Bewertung der Leistung in dieser Rubrik ungenau und anfällig für kleine Änderungen.
Aus diesem Grund erfolgt der Vergleich der Modellleistungen unabhängig von dieser Fragequelle.\\

Das GPT4-Modell hat keine Präferenz für bestimmte Fragetypen, beantwortete aber die Fragen aus dem Buch mit \SI{93}{\percent} korrekten Antworten signifikant besser.
Vergleichsweise wenige Antworten wurden bei den beiden schriftlichen Prüfungen gegeben.\\

Das untrainierte Llama-Modell zeigte mit \SI{39}{\percent} richtigen Antworten eine deutliche Stärke bei Multi-Fakten-Fragen und konnte \SI{27}{\percent} der mündlichen Klausurfragen beantworten.
Die schlechtesten Kategorien ergaben sich bei den Ein-Fakt-Fragen mit \SI{20}{\percent} und den Fragen aus dem Buch mit \SI{24}{\percent} richtigen Antworten.\\

Während das \liv-Modell mit nur 2 richtig beantworteten Fragen grundsätzlich schlecht abschnitt,
zeigte das \lia-Modell mit \SI{36}{\percent} eine deutlich bessere Leistung bei Fragen mit mehreren Fakten
und bei der mündlichen Prüfung mit \SI{40}{\percent} richtig beantworteten Fragen.
Dagegen verlor dieses Modell deutlich bei Ein-Fakten-Fragen mit \SI{5}{\percent} und Klausurfragen mit nur \SI{12}{\percent} richtigen Antworten.\\\

Modelle, die auf 3 Epochen trainiert wurden, zeigen einen deutlichen Leistungssprung gegenüber einer Epoche.
Sowohl das \lev-Modell als auch das \lea-Modell verbessern ihre Leistung in den zuvor als schwach identifizierten Fragetypen und Fragequellen.
Im Falle des \lea-Modells bedeutet dies eine Verdoppelung der richtig beantworteten Transferfragen sowie eine Verdoppelung der richtig beantworteten Fragen aus dem Buch.
Diese Leistungssteigerung zeigt sich auch in den MakroF1-Werten.\\

Epoche 5 und Epoche 10 enthalten nur Modelle, die mit Nvidia A30 Grafikkarten trainiert wurden.
Die gezeigten Leistungen unterscheiden sich kaum von denen des \lea-Modells und weisen nur minimale Leistungsschwankungen auf.
Allerdings verschiebt sich die Verteilung der richtig beantworteten Fragen hin zu Fragen aus dem Buch.
Die anderen Fragequellen werden von den Modellen mit zunehmender Epoche immer schlechter beantwortet, während die Fragen aus dem Buch immer besser beantwortet werden.
Diese Verschiebung deutet auf eine Überanpassung hin, da die Fragen aus dem Buch auch in den Trainingsdaten enthalten sind.\\

\subsection{Verbesserungen durch Training}
\pic{results/loss.png}{Fehlerwerte des Trainingsdatensatzes}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Trainingsdatensatzes.}{fig:results:loss}
\pic{results/validation_loss.png}{Fehlerwerte des Validierungsdatensatzes}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Validierungsdatensatzes.}{fig:results:validation_loss}
Während der Trainingsphase wurden die Modelle mit Hilfe eines Validierungsdatensatzes evaluiert.
Diese zusätzliche Evaluierung berechnete den Fehlerwert des Modells auf einem gegebenen Datensatz.
\cref{fig:results:loss} zeigt die Fehlerwerte der Trainingsdaten und \cref{fig:results:validation_loss} die Fehlerwerte der Validierungsdaten für jedes Modell.
Fehlerwerte beschreiben nicht konkret die Leistung eines Modells, sondern sind ein Indikator für die Verbesserung der Leistung und den Beginn der Überanpassung.
Ein abnehmender Fehlerwert des Trainingsdatensatzes bedeutet, dass das Modell den Trainingsdatensatz besser nachahmen kann, während ein abnehmender Fehlerwert des Validierungsdatensatzes zeigt, dass das Modell diese Nachahmung generalisierend auf ungesehenen Text anwenden kann.\\

Wenn der Fehlerwert des Validierungsdatensatzes zu steigen beginnt, deutet dies auf eine Überanpassung hin.
Das Modell verliert dann die Fähigkeit, ungesehenen Text zu imitieren und beginnt stattdessen, den Trainingsdatensatz auswendig zu lernen.
Dieses Phänomen ist bei beiden Modelltypen bereits ab Epoche 3 zu beobachten und setzt sich konstant bis Epoche 10 fort.\\

Für die Beantwortung der Fragen ist das Auswendiglernen jedoch in gewissem Maße vorteilhaft, da Definitionen von Begriffen oder Aufzählungen von Fakten vom Modell besser zitiert werden können.
Dies spiegelt sich auch in den Ergebnissen wider.
Modelle der Epoche 3 können Fragen aus allen Fragequellen und Fragetypen besser beantworten, da hier die Zitierfähigkeit des Modells erhöht wird.
Ein zu hoher Grad an Überanpassung führt jedoch zu anderen Problemen.
Treten unbekannte Formulierungen wie z.B. Rechtschreibfehler auf, kann das Modell diese nicht mehr korrekt beantworten.
Diese Problematik nimmt mit steigender Epoche zu, weshalb Modelle ab Epoche 5 in ihrer Leistungsfähigkeit nachlassen.\\

\subsection{Vergleich MakroF1}
\pic{results/makro_total.png}{Vergleich der MakroF1-Werte}{Vergleich evaluierter Modelle und Makro F1 Werten bei der Beantwortung von Fragen.}{fig:results:makro_total}

\cref{fig:results:makro_total} zeigt die MakroF1-Werte der evaluierten Modelle.
Auch hier sind die Tendenzen aus \cref{subsec:results:correctness:strengths} erkennbar.
GPT4 erreicht einen MakroF1-Wert von \num{0.7} und ist damit deutlich besser als die Llama-Modelle.
Während die auf 1 Epoche trainierten Modelle noch unter der Leistung des untrainierten Llama-Modells liegen, verdoppeln sich die MakroF1-Werte der Modelle ab 3 Epochen.
Danach bleiben die MakroF1-Werte der Modelle mit 5 und 10 Epochen konstant.\\

Der MakroF1-Wert hat einen Wertebereich von \numrange{0}{1} und repräsentiert die tatsächliche Leistung des Modells besser.
Modelle, die Fragen richtig beantwortet haben, aber auch falsche oder unvollständige Antworten gegeben haben, erhalten hier einen niedrigeren F1-Wert.
Ein Modell mit einem MakroF1-Wert von \num{0} beantwortet keine Frage richtig, während ein Modell mit einem Wert von \num{1} alle Fragen richtig und vollständig beantwortet.
So beantwortet das GPT4-Modell im Durchschnitt eine Frage zu \SI{70}{\percent} richtig, während das \lea-Modell im Durchschnitt eine Frage zu \SI{30}{\percent} richtig beantwortet.\\

\pic[0.7]{results/makrof1_total_type_heat.png}{Heatmap der MakroF1-Werte nach Fragetypen}{Heatmap der MakroF1-Werte der evaluierten Modelle nach Fragetypen.}{fig:results:makrof1_total_type_heat}

Eine genauere Aufschlüsselung der MakroF1-Werte nach Fragetypen ist in \cref{fig:results:makrof1_total_type_heat} zu sehen.
Hier sieht man die Leistungssteigerung in allen Fragetypen ab 3 Epochen.
Nicht intuitiv werden Transferfragen von den Modellen besser beantwortet als Einzelfragen, obwohl letztere für den Menschen deutlich einfacher sind.
Der Grund dafür könnte in der günstigen Grundstruktur der Modelle liegen.
Durch die inhärente Architektur mit Hilfe einer Aufmerksamkeitsmaske sind Transformermodelle darauf spezialisiert, Texte und Token miteinander zu korrelieren und könnten daher verschiedene Textstellen aus dem Buch und Fakten besser miteinander verknüpfen.
Diese Beobachtung kann jedoch nicht auf das GPT4-Modell übertragen werden.
Da der technische Hintergrund des GPT4-Modells nicht bekannt ist, kann diese Beobachtung nicht weiter begründet werden.
Das GPT4-Modell wurde nach normalem Training durch umfangreiches Finetuning mit menschlichem Feedback an die Beantwortung von Fragen angepasst, wodurch die Leistung bei der Beantwortung von einfachen Einzelfragen hier durchaus gesteigert werden könnte.\\

\pic{results/makrof1_total_source_heat.png}{Heatmap der MakroF1-Werte nach Fragequellen}{Heatmap der MakroF1-Werte der evaluierten Modelle aufgeteilt nach Fragequellen.}{fig:results:makrof1_total_source_heat}

\cref{fig:results:makrof1_total_source_heat} zeigt die MakroF1-Werte der Modelle unterteilt nach Fragequellen.
Die Entwicklung der Modelle von einem untrainierten über einen trainierten zu einem übertrainierten Zustand ist hier gut zu erkennen.
Da die Fragequelle \enquote{IS\_2022\_07\_18} nur aus \num{9} Fragen besteht, ist eine generelle Aussage hier nicht möglich.
Kleinere Schwankungen, wie z.B. die richtige Beantwortung einer Frage, führen zu großen Ausschlägen, die fälschlicherweise als sehr gute Leistung interpretiert werden könnten.\\

Bis zur Epoche 3 ist eine stetige Leistungssteigerung in allen Fragequellen zu beobachten, wobei Fragen aus der Quelle \enquote{A\_2021} am meisten profitieren.
Dies ist zu erwarten, da das zugrundeliegende Modul inhaltlich sehr eng mit dem Buch \citetitle{bb} zusammenhängt.
Ab Epoche 5 ist eine Verschiebung in Richtung der Fragequelle \enquote{book} zu erkennen, was auf ein Überanpassen hindeutet.
Ab diesem Zeitpunkt können die Modelle Fragen aus dem Buch besser beantworten, indem sie die darin enthaltenen Antworten zitieren, während andere Formulierungen desselben Wissens zu Unverständnis führen.
Dennoch ist auch diese Leistungssteigerung ein guter Fortschritt und zeigt die Stärken der Informationsextraktion von Transformermodellen aus Text.
Die im Buch gestellten Fragen werden nicht unmittelbar nach der Frage beantwortet, sondern erst am Ende des Buches.
Das bedeutet, dass die Transformermodelle hier trotz der großen Distanz eine klare Verbindung zwischen Frage und Antwort herstellen konnten.

\subsection{Zusammenfassung}
Die MakroF1-Werte bestätigen die zuvor gemachten Aussagen über die Gesamtanzahl der Fragen zur Leistungsfähigkeit der Modelle.
GPT4 erreicht hier mit \num{0.7} den höchsten MakroF1-Wert, vergleichbar mit den Ergebnissen aus \citet{gpt4} zum Thema \enquote{Medical Knowledge}.
Das Llama 2 7B startet im untrainierten Zustand deutlich unter dem normalen Leistungsniveau.
Seine Leistung kann durch ein Continual Pretraining ab Epoche 3 verdoppelt werden.
Eine Epoche, d.h.\ das einmalige Lesen des Buches, scheint nicht auszureichen, um eine bessere Leistung zu erzielen.
Dies liegt auch an der Größe des Trainingsdatensatzes.
Da dieser relativ klein ist, kann das Modell seine Gewichte nicht schnell genug anpassen, um die darin enthaltene Information zu reproduzieren.
Eine höhere Lernrate würde eine schnellere Anpassung des Modells an den gesehenen Text verbessern, aber die Gefahr erhöhen,
dass sich das Modell zu sehr an Formulierungen und Formatierungen anpasst und dadurch unbrauchbar wird.\\

Ab Epoche 5 verliert das Modell an Leistung, obwohl die ermittelten MakroF1-Werte konstant bleiben.
Ab diesem Zeitpunkt beginnt das Modell die Verallgemeinerung zu verlernen und kann nur noch Fragen aus dem Buch mit zitierten Antworten richtig beantworten.
Aus diesem Grund scheint das Modell seine beste Leistung ab Epoche 3 zu erreichen.
Hier erreicht das Modell besonders gute MakroF1
durch eine übermäßig gute Leistung bei Fragen aus der Fragequelle \enquote{A\_2021}.\\

GPT4 ist durch seine Größe und die dahinter stehende Trainingsmenge unschlagbar.
Es ist zu erwarten, dass dieses Modell die Leistung eines Modells mit 7 Milliarden Parametern übertrifft.
Die Leistungssteigerung des Llama-Modells zeigt jedoch einen Trend, der durch größere Modelle noch weiter gesteigert werden könnte.
Zum jetzigen Zeitpunkt sind die trainierten Modelle aus dieser Arbeit jedoch nicht verwendbar.
Insbesondere in Epoche 1 enthalten die Antworten grundlegende Fehler in der Ausgabe durch Imitation von Formatierungen (siehe \cref{fig:formatting-errors}).
Weiter trainierte Modelle zeigen eine Präferenz für keine Antwort gegenüber einer falschen Antwort,
was für die Anwendung des Modells durchaus vorteilhaft sein kann.
Allerdings ist diese Tendenz auf eine Überanpassung an den Trainingsdatensatz zurückzuführen.\\

\begin{table}
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        Frage              & Definieren Sie den Begriff \enquote{Krankenhausinformationssystem}.                                                                                                                                                                                                    \\
        Übersetzt          & Define the term ``hospital information system''.                                                                                                                                                                                                                       \\
        Erwartete Antwort  & A Hospital Information system is the socio-technical subsystem of hospitals. It comprises all data, information, and knowledge processing as well as the associated human or technical actors in their respective data, information, and knowledge processing roles. \\
        Generierte Antwort & \#\#\# 2.2.2.2.2.2.1.1.1.1.1.1.2.1.1.2.1.2.1.2.1.2.1.2. $[\dots]$                                                                                                                                                                                                    \\
        \bottomrule                                                                                                                                                                                                                                                                                 \\
    \end{tabularx}
    \caption[Beispiel für Formatierungsfehler]{Beispiel für Formatierungsfehler in der Ausgabe des Llama 2 7B Modells trainiert auf 1 Epoche}\label{fig:formatting-errors}
\end{table}

Modelle, die mit V100-Grafikkarten trainiert wurden, zeigen eine sehr starke Verschlechterung der Leistung.
Dies ist auf das notwendige Training im FP16 Datenformat gegenüber dem ursprünglich verwendeten BF16 Datenformat zurückzuführen.
Wie auch von Huggingface beschrieben, sollten vortrainierte Modelle weiterhin im vorgesehenen Datenformat trainiert werden.
Diese Beobachtung wird auch durch die Ergebnisse dieser Arbeit bestätigt.\\


\section{Analyse Erklärbarkeit}\label{sec:results:explainability}
\pic{results/explained.png}{Vergleich der Modelle nach Erklärbarkeit}{Vergleich der evaluierten Modelle und der Anzahl der Antworten mit Erklärungen.}{fig:results:explained}
Die hier evaluierten Modelle weisen eine hohe Fähigkeit auf, Erklärungen zu den generierten Antworten zu liefern.
Insbesondere GPT4 ist in der Lage, zu \SI{97}{\percent} richtig beantworteten Fragen eine Erklärung zu liefern.
\cref{fig:results:explained} zeigt hier die Gesamtzahl der Antworten, die Erklärungen enthalten.
Dabei wurden nur Fragen berücksichtigt, die das jeweilige Modell zuvor mit mindestens einer richtigen Antwort beantwortet hat.
Das untrainierte Modell sowie die trainierten Modelle einer Epoche können hier nur für die Hälfte der Fragen eine Erklärung liefern.
Ab Epoche 3 steigt diese Leistung ebenso deutlich auf \SI{85}{\percent}.\\

\pich[0.7]{results/explainability_total_type.png}{Heatmap der Erklärbarkeit nach Fragetypen}{Heatmap der Erklärbarkeit der evaluierten Modelle nach Fragetypen.}{fig:results:explainability_total_type}
\pic{results/explainability_total_source.png}{Heatmap der Erklärbarkeit nach Fragequellen}{Heatmap der Erklärbarkeit der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:explainability_total_source}

\cref{fig:results:explainability_total_type} und \cref{fig:results:explainability_total_source} zeigen die Unterteilung dieser Leistung nach Fragetypen und Fragequellen.
Hier ist zunächst kein klarer Trend der Verbesserung in diesen Unterteilungen zu erkennen.
Insbesondere Fragen des Fragetyps \enquote{Transfer} enthalten wenig Erklärungen, während Fragen des Fragetyps \enquote{Multi} am meisten Erklärungen enthalten.
Die zuvor gut erscheinenden Ergebnisse bei Fragen des Fragetyps \enquote{A\_2021} zeigen hier, dass die generierten Antworten zwar korrekt sind, aber überwiegend keine Erklärungen enthalten.
Hervorzuheben ist die grundsätzlich gute Leistung von GPT4, die durch das eingesetzte Finetuning mit menschlicher Bewertung (engl.
\enquote{Human Reinforcement Learning}) begründet werden können.
Mit dieser Trainingsmethode könnte auch das Llama-Modell belohnt werden, wenn es Erklärungen für seine Antworten gibt.\\

\section{Analyse Fragenverständnis}\label{sec:results:questionunderstanding}
\pic{results/understood.png}{Vergleich der Modelle nach Fragenverständnis}{Vergleich der evaluierten Modelle und der Anzahl der verstandenen Fragen.}{fig:results:understood}
Das Kriterium Fragenverständnis ist nicht äquivalent zur Anzahl der beantworteten Fragen des Kriteriums Korrektheit.
Eine Frage gilt nicht als verstanden, wenn sie beantwortet wurde, da falsche Antworten zwar falsche Fakten enthalten können, die Frage aber nicht vollständig beantwortet wurde und daher als nicht verstanden gezählt wird.
Ebenso kann eine unbeantwortete Frage als verstanden gezählt werden.
Beispielsweise generierte GPT4 in einigen Fällen eine Antwort, die erklärte, warum das Modell die Frage nicht beantworten konnte, die Frage aber eindeutig verstanden hatte.\\

\cref{fig:results:understood} zeigt die Gesamtzahl der verstandenen Fragen der evaluierten Modelle.
Im Gegensatz zum Kriterium Erklärbarkeit wurden hier alle vorhandenen Fragen auf ihr Verständnis hin überprüft.
Auch hier schneidet GPT4 mit \SI{97}{\percent} verstandenen Fragen deutlich besser ab als die Llama-Modelle.
Lediglich das \lea-Modell erreicht mit \SI{72}{\percent} verstandenen Fragen eine etwas bessere Leistung als das untrainierte Modell mit \SI{64}{\percent}.
Insbesondere Modelle mit höherer Epoche verlieren die Fähigkeit, Fragen zu verstehen.
Diese Beobachtung bestätigt somit auch die steigende Anzahl unbeantworteter Fragen aus dem Kriterium Korrektheit, da immer mehr Fragen nicht verstanden werden.
Grund hierfür ist die zunehmende Überanpassung der Modelle, die es den Modellen erschwert, andere Formulierungen gleichen Wissens zu verstehen.\\

\pic[0.7]{results/question_understanding_total_type.png}{Heatmap des Fragenverständnis nach Typ}{Heatmap des Fragenverständnisses der evaluierten Modelle nach Fragetypen.}{fig:results:question_understanding_total_type}
\pic{results/question_understanding_total_source.png}{Heatmap des Fragenverständnis nach Quelle}{Heatmap des Fragenverständnisses der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:question_understanding_total_source}

\cref{fig:results:question_understanding_total_type} und \cref{fig:results:question_understanding_total_source} zeigen die Unterteilung dieser Statistik nach Fragetyp und Fragequelle.
Auch hier ist ein deutlicher Leistungsabfall ab Epoche 5 zu erkennen, insbesondere beim Fragetyp \enquote{transfer}. Das \lea-Modell zeigt hier die beste Leistung aller Llama-Modelle, obwohl es in der Kategorie \enquote{multi} schlechter als das untrainierte Modell abschneidet.
Die Unterteilung in Fragequellen bestätigt diese Beobachtungen, wobei die Quelle \enquote{IS\_2022\_09\_27} besonders von der Überanpassung der Modelle ab Epoche 5 betroffen ist.
Die Fragen dieser Quelle scheinen grundsätzlich andere Formulierungen als im Buch zu verwenden und müssen daher vermehrt mit generalisiertem Wissen beantwortet werden.

\section{Analyse Robustheit}\label{sec:results:robustness}
\pic{results/makro_comparison.png}{Vergleich der Robustheit von Modellen}{Vergleich der evaluierten Modelle und ihrer MakroF1-Werte nach Einführung von Rechtschreibfehlern.}{fig:results:makro_comparison}
\cref{fig:results:makro_comparison} zeigt die MakroF1-Werte der Modelle nach Einführung von Rechtschreibfehlern.
Dazu wurden alle richtig beantworteten Fragen der Modelle mit zusätzlichen Rechtschreibfehlern versehen und erneut ausgewertet.
Rechtschreibfehler in Fachausdrücken erschweren die Wiedergabe von Sachverhalten, da die Tokenisierung die Wörter anders unterteilt.
Wenn Fakten direkt mit einer bestimmten Tokenisierung verknüpft sind, kann das Modell diese Fakten bei einer anderen Unterteilung nicht mehr wiedergeben.
Aus diesem Grund ist eine schlechtere Leistung der Modelle zu erwarten und zeigt die Fähigkeit zur Generalisierung des gelesenen Wissens.\\

Auch hier schneidet das GPT4-Modell mit einem MakroF1-Wert von \num{0.59} gegenüber dem Ausgangswert \num{0.7} am besten ab.
Dies entspricht einer Leistungsminimierung von \SI{16}{\percent}. Trainierte Modelle zeigen deutlich höhere Leistungsminimierungen von etwa \SI{50}{\percent}.
Diese großen Unterschiede deuten auf einen oberflächlichen Wissenserwerb mit Fokus auf korrekt geschriebene Fachbegriffe hin.
Bei wenigen Trainingsdaten können die Modelle das darin enthaltene Wissen nur grob erlernen.
Hier kann die Leistung verbessert werden, indem der Trainingsdatensatz erweitert wird, um mehr Arten der Wissensformulierung zu enthalten.
Dies würde die Generalisierung der Modelle und damit die Leistung in diesem Kriterium verbessern.\\

Das untrainierte Llama-Modell erzielt etwas bessere Ergebnisse mit eingebauten Rechtschreibfehlern.
Diese Verbesserung wird aufgrund der geringen Veränderung als zufällig angesehen und bedeutet nicht, dass das Modell bessere Ergebnisse liefert, wenn Fehler enthalten sind.
Besser erklärt sind hier etwa gleiche Werte.
Die Llama-Modelle wurden zunächst auf sehr großen Datensätzen trainiert, die auch die Größe der Datensätze für GPT4 überstiegen.
Das erlernte Wissen dahinter ist daher fundierter und kann deutlich besser verallgemeinert auf verschiedene Formulierungen angewendet werden.
Durch das hier durchgeführte Continual Pretraining werden diese fundierten Wissensknoten überschrieben und durch oberflächlichen Wissenserwerb ersetzt.
Dadurch kann das Modell zwar zunächst mehr Wissen abbilden, verliert aber die Fähigkeit, dieses Wissen mit unterschiedlichen Formulierungen zu verknüpfen.\\

\section{Zusammenfassung}\label{sec:results:summary}

\begin{table}
    \centering
    \begin{adjustbox}{width=\textwidth,center}
        \begin{tabular}{lllll}
            \toprule
            \textbf{Modell} & \textbf{MakroF1}                   & \textbf{Erklärbarkeit}                      & \textbf{Fragenverständnis}                  & \textbf{Robustheit (Leistungsverlust)}       \\
            \midrule
            GPT4            & \textbf{\num[detect-weight]{0.7}}  & \textbf{\SI[detect-weight]{97.3}{\percent}} & \textbf{\SI[detect-weight]{97.9}{\percent}} & \SI{15.7}{\percent}                          \\
            \lo{}           & \num{0.13}                         & \SI{48.1}{\percent}                         & \SI{64.2}{\percent}                         & \textbf{\SI[detect-weight]{-0.23}{\percent}} \\
            \midrule
            \liv{}          & \num{0.01}                         & \SI{0}{\percent}                            & \SI{2.1}{\percent}                          & \SI{100}{\percent}                           \\
            \lia{}          & \num{0.11}                         & \SI{50}{\percent}                           & \SI{54.7}{\percent}                         & \SI{54.5}{\percent}                          \\
            \lev{}          & \num{0.1}                          & \SI{47.4}{\percent}                         & \SI{40}{\percent}                           & \SI{60}{\percent}                            \\
            \lea{}          & \num{0.3}                          & \textbf{\SI[detect-weight]{85.4}{\percent}} & \textbf{\SI[detect-weight]{72.6}{\percent}} & \SI{46.7}{\percent}                          \\
            \lsa{}          & \textbf{\num[detect-weight]{0.33}} & \SI{82.5}{\percent}                         & \SI{60}{\percent}                          & \SI{60.6}{\percent}                          \\
            \lioa{}         & \num{0.32}                         & \SI{81}{\percent}                           & \SI{55.8}{\percent}                         & \textbf{\SI[detect-weight]{43.8}{\percent}}  \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
        \caption[Zusammenfassung der Evaluierungsergebnisse]{Zusammenfassung der Evaluierungsergebnisse. Fettgedruckte Werte stellen die beste Leistung im Kriterium dar. Die Zeilen sind unterteilt in nur evaluierte Modelle und trainiert und evaluierte Modelle. }\label{tab:results:summary}
    \end{table}
    
    \cref{tab:results:summary} zeigt eine Zusammenfassung der Evaluationsergebnisse.
    Dabei werden die Modelle in die Gruppen \enquote{nur evaluiert} und \enquote{trainiert und evaluiert} unterteilt.
    Im Allgemeinen übertrifft GPT4 die Llama-Modelle in allen Kriterien mit Ausnahme des Leistungsverlustes im Kriterium Robustheit.
    Von den Llama-Modellen erreicht \lsa{} die beste MakroF1-Leistung, \lioa{} den geringsten Leistungsverlust im Kriterium Robustheit und \lea{} die besten Leistungen in den Kriterien Erklärbarkeit und Fragenverständnis.
    Unabhängig vom Robustheitskriterium scheint ein optimales Modell mit 3 bis 5 Epochen trainiert zu werden.
    Von den derzeit trainierten Modellen ist bei genauerer Betrachtung der Kriterien das \lea-Modell am besten geeignet, die Fragen zu beantworten, da ab 5 Epochen die Überanpassung der Modelle einen zu starken Einfluss hat.\\
    
    Hinsichtlich der Erklärbarkeit und des Fragenverständnisses ist ab Epoche 3 eine deutliche Verbesserung zu erkennen.
    Die Leistungseinbußen durch Rechtschreibfehler zeigen jedoch, dass das Wissen nicht fundiert erworben wurde und die Modelle anfällig für andere Formulierungen sind.\\
    
    Mögliche Leistungssteigerungen können durch größere Modelle erreicht werden.
    Die Llama2-Modelle sind zusätzlich mit 13 Milliarden und 70 Milliarden Parametern verfügbar.
    Größere Modelle sind besser in der Lage, komplexes Wissen zu erlernen und zu verallgemeinern.
    Dies kann dazu führen, dass ein längeres Training ohne Überanpassung möglich ist.
    Darüber hinaus hilft ein größerer Datensatz, insbesondere zur Vermeidung von Überanpassung, Informationen in verschiedenen Formulierungen darzustellen und damit die Robustheit zu verbessern.
    Ebenso ist eine generelle Verbesserung der Ergebnisse im Kriterium Korrektheit zu erwarten.
    Als letzte Möglichkeit kann ein Training mit Hilfe von Human Reinforcement Learning durchgeführt werden.
    Dieses ermöglicht es, das Erklären und Verstehen von Fragen zu belohnen und somit auch diese beiden Kriterien zu verbessern.