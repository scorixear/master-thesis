%*****************************************
\chapter{Diskussion}\label{ch:discussion}
%*****************************************
Die Ergebnisse dieser Arbeit zeigen, dass ein Continual Pretraining die Leistung eines Modells bei der Beantwortung von Fragen verbessern kann. Ebenso wirkt das Modell demnach unterstützend bei der Wissensbeschaffung aus \citet{bb}.
Dennoch erreichte das hier trainierte Modell eine maximale Leistung von \num{0.3} MakroF1, was nicht mit derzeit verfügbaren Modellen wie GPT4 vergleichbar ist.
Um diese Leistung zu erreichen, werden in \cref{ch:futurework} einige Ansätze vorgestellt, die in zukünftigen Arbeiten untersucht werden können.

\section{Grenzen der Modelle}
Die hier vorgestellten Modelle stellen eine Momentaufnahme in einer bestimmten Umgebung dar, können aber keine generelle Aussage über die zu erwartende Leistung von Continual Pretraining machen.
Das Training der Modelle ist durch die Ressourcenanforderungen begrenzt und stellt auch nicht die optimale Leistung dar, die ein Llama 2 Modell erreichen kann.\\

Während des Trainings wurde ein Zustand der Überanpassung erreicht, wobei der genaue Zeitpunkt nicht bestimmt werden konnte.
Bereits nach 3 Epochen ist ein deutlicher Anstieg der Fehlerwerte gegenüber dem Validierungsdatensatz zu erkennen, so dass davon auszugehen ist, dass dies bereits vorher begonnen hat.
Zudem wurde die Unterteilung des Fortschritts nur in 1, 3, 5 und 10 Epochen vorgenommen, weshalb nicht ausgeschlossen werden kann, dass z.B. bei 4 Epochen bessere Ergebnisse erzielt werden können.\\

Auch die geringe Datenmenge für das Training reduziert die mögliche Leistung in allen Kriterien.
Kleine Datenmengen führen zu Überanpassung und stellen Informationen in zu wenig unterschiedlichen Ausprägungen dar, was die Generalisierbarkeit des Modells verringert.\\

Die Evaluierung des Modells erfolgte anhand von \num{95} Fragen.
Auch dieser Fragenkatalog ist kleiner als erwartet und enthält meist ähnliche Wissensfragen in nur einer Formulierung.
Insbesondere die Fragen aus der schriftlichen Prüfung des Moduls \enquote{Informationssysteme im Gesundheitswesen} umfassen insgesamt nur 9 Fragen und unterliegen damit sehr starken Schwankungen.\\

Die Ergebnisse der Evaluation zeigen eine Tendenz zur Verbesserung, die trainierten Modelle selbst sind in diesem Zustand jedoch nicht verwendbar.
Dies liegt zum einen an dem fehlenden Finetuning, welches die Gefährlichkeit des Modells deutlich erhöht, da es schädliche und falsche Antworten generieren kann, zum anderen an der allgemeinen Formulierung der Antworten.
Wie bereits in \cref{ch:results} gezeigt, beenden Modelle ihre Antwort nicht eindeutig mit einem Token, sondern generieren fortlaufend Text.
Daher enthalten alle generierten Antworten zumeist am Ende zitierte Textpassagen, die keinen Bezug zur Frage haben.
Dies erschwert die Verwendung dieses Modells erheblich.
Abhilfe schafft hier das bereits erwähnte Finetuning, bei dem einem Modell abschließende Token beigebracht werden, nach denen die Generierung gestoppt werden kann.

\section{Probleme bei Kernfragen}
Das Thema Informationssysteme im Gesundheitswesen und damit auch der hier verwendete Evaluationsdatensatz enthalten grundlegende Kerninformationen,
die in ihrem Verständnis wichtiger sind als anderes Wissen. Dazu gehören z.B. die Definition von Daten, Information und Wissen, die Definition eines Informationssystems, so dass Menschen Teil des Systems sind, oder die Unterscheidung zwischen Anwendungssystemen und Softwareprodukten (ein Anwendungssystem ist die Installation und Nutzung eines Softwareprodukts durch Akteure, deren Rollen und Aufgaben).\\

Diese grundlegenden Wissensfragen werden in der Literatur zu diesem Thema unterschiedlich definiert und sollte von den Modellen mit dem Wissen aus dem Buch von \citet{bb} beantwortet werden.
Auch widersprechen einige Definitionen aus dem Buch der Definition aus dem allgemeinen Sprachgebrauch. Auch hier ist eine Beantwortung mit dem Wissen aus dem Buch erforderlich.\\

Die hier trainierten Modelle wurden jedoch auf Literatur vortrainiert, die durchaus widersprüchliches Wissen zum Buch enthält.
So beantworten nicht weitertrainierte Modelle Kernfragen wie \enquote{Was ist ein Informationssystem} oft falsch und beziehen den Menschen nicht mit ein.
Mit fortschreitender Epoche verbessert sich dieses Verhalten, da die Modelle den Zustand der Überanpassung erreichen und nur noch Wissen aus dem Buch zitieren.
Dennoch kann eine grundsätzliche Falschbeantwortung von Kernfragen in dieser Arbeit nicht ausgeschlossen werden.
Hier wäre eine genauere Evaluierung mit nur Kernfragen notwendig, um die Leistung der Modelle in diesem Bereich zu untersuchen.\\

\section{Bewertung der Fragen mit Prüfungspunkten}
Zur Berechnung des Kriteriums Korrektheit wurden jeder Frage drei Werte zugeordnet: die Anzahl der korrekten Antworten des Modells, die Anzahl der Antworten des Modells im Allgemeinen und die Anzahl der erwarteten Antworten für diese Frage.
Anhand dieser Werte wurden die F1- und MakroF1-Werte der Modelle berechnet.\\

Da der Evaluationsdatensatz jedoch größtenteils aus Prüfungsfragen besteht, wäre hier auch eine Auswertung der Fragen nach den Original-Prüfungspunkten möglich.
Dadurch würden schwierigere Fragen bzw. Fragen mit mehr Antworten stärker gewichtet und hätten somit einen größeren Einfluss auf die Leistung der Modelle.
Diese Art der Auswertung könnte die Modellleistung in den Kontext der menschlichen Leistung stellen und die Aussagen über die Modellleistung nachvollziehbarer machen.

\section{Lösung des Problems}
Das in \cref{sec:problemstellung} formulierte Problem kann mit den hier vorgestellten Modellen nur teilweise gelöst werden.
Die Verwendung des Modells erleichtert die Wissensextraktion aus dem Buch, gibt aber in weiten Teilen den Inhalt des Buches nicht oder nur unvollständig wieder.
Mit zunehmender Epoche der Modelle steigt die Qualität der Wissensextraktion durch Zitieren des Buches, jedoch sinkt die Anwendbarkeit auf andere Formulierungen und die Verallgemeinerung des gleichen Wissens, insbesondere bei der Verwendung in einem anderen Kontext.
Die Fragmentierung der Definitionen wird durch das Modell zusammengeführt, was sich insbesondere an den Ergebnissen der Auswertung bei den Fragen zum Buch zeigt.
Hier gelang es den Modellen, die im Buch enthaltenen Wissensfragen mit zunehmender Epoche den entsprechenden Antworten zuzuordnen.\\

Das Ziel Z1 wurde mit den hier vorgestellten Modellen teilweise erreicht.
Die trainierten Modelle sind in der Lage, Fragen zu Informationssystemen im Gesundheitswesen mit einem MakroF1-Wert von \num{0.3} korrekt zu beantworten, steigern damit die Ausgangsergebnisse des untrainierten Modells, fallen aber in ihrer Leistung hinter das \ac{sota} Modell GPT4 zurück.\\

Ziel Z2 wird wie Ziel Z1 teilweise erreicht.
Das Modell Llama 2 7B, trainiert mit Nvidia Tesla A30 Grafikkarten über 3 Epochen, erreicht bei den Klausurfragen des Moduls \enquote{Architektur von Informationssystemen im Gesundheitswesen} einen MakroF1-Wert von \num{0.45} und beantwortet \num{12} von \num{22} Fragen mit mindestens einer richtigen Antwort.
Diese Ergebnisse liegen weit über dem untrainierten Modell mit einem MakroF1-Wert von \num{0.12} und \num{6} von \num{22} richtig beantworteten Fragen, werden aber immer noch vom GPT4-Modell mit einem MakroF1-Wert von \num{0.64} und \num{17} von \num{22} richtig beantworteten Fragen geschlagen.
Die hier vorgestellten Ergebnisse zeigen, dass die Leistung bei der Lösung dieser Beispielklausur durch Continual Pretraining deutlich gesteigert werden kann, jedoch durch die Größe des Modells und den verwendeten Datensatz begrenzt ist.\\