%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

Die Lösung gliedert sich in sechs Schritte, die für die Durchführung des Trainings und die Bewertung der Ergebnisse notwendig sind.
Im Folgenden werden diese Schritte näher erläutert. Für jeden Schritt sind die verwendeten Techniken und Bibliotheken separat aufgeführt.
Die Schritte sind wie folgt gegliedert:
\begin{enumerate}
    \item Laden des Modells
    \item Modelltraining
    \item Generierung von Antworten auf dem Evaluierungsdatensatz
    \item Bewertung der erzeugten Antworten
    \item Auswertung basierend auf den Bewertungen
\end{enumerate}

\section{Herunterladen des Modells}
Die Llama-Modelle werden unter einer nicht-kommerziellen Lizenz für Forschungszwecke zur Verfügung gestellt. Der Zugang zu den Modellen wird im Einzelfall auf Anfrage gewährt. Diese Anfrage wurde im Rahmen dieser Arbeit gestellt und bestätigt.
Anschließend kann ein von den Autoren vorbereitetes Skript oder die \ac{api} von Huggingface mit zugehörigem Authentifizierungs-Token verwendet werden, um die trainierten Gewichte des Modells herunterzuladen.
Das Skript benötigt eine explizite \ac{url}, die nach einer vorgegebenen Zeit von einer Woche nach Freigabe der Modelle ungültig wird. Aus diesem Grund ist diese \ac{url} nicht im Skript enthalten.

\section{Training des Modells}
Um das Llama-Modell zu trainieren, wird die Transformers Bibliothek von Huggingface verwendet \citep{transformers}.
Das in Python geschriebene Trainingsskript basiert auf dem Beispielskript zum Training von kausalen Sprachmodellen
aus dem Huggingface GitHub Repository\footnote{\url{https://github.com/huggingface/transformers/blob/v4.31.0/examples/pytorch/language-modeling/run_clm.py} abgerufen am 16.8.2023}
und wurde teilweise an die Anforderungen des hier durchgeführten Trainings angepasst.
Die Konfiguration des Trainings gliedert sich in vier Bereiche: Einstellungen für das Modell, Einstellungen für die Trainingsdaten, Einstellungen für das Training selbst und Einstellungen für DeepSpeed.
Im Folgenden werden diese Einstellungen näher erläutert.
\subsection{Konfiguration des Modells}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}   & \textbf{Wert}              \\
        \midrule
        model\_name          & meta-llama/Llama-2-7b-hf   \\
        cache\_dir           & ./cache                    \\
        use\_fast\_tokenizer & false                      \\
        model\_revision      & main                       \\
        use\_auth\_token     & true                       \\
        hugging\_token       & \textit{Huggingface Token} \\
        torch\_dtype         & auto                       \\
        low\_cpu\_mem\_usage & false                      \\
        \bottomrule
    \end{tabular}
    \caption[Parameter zur Konfiguration des Modells]{Parameter zur Auswahl und Konfiguration des Modells}\label{tab:model-config}
\end{table}
Die zur Auswahl und Konfiguration des Modells verwendeten Parameter sind in \cref{tab:model-config} aufgelistet.\\

Der Parameter \texttt{model\_name} entspricht einer Modellauswahl und kann entweder ein relativer Pfad zu einem lokalen Modell oder ein Modellname sein.
Der Modellname wurde hier auf das Modell Llama 2 7B gesetzt und verweist auf das von Huggingface gehostete Modell in einem mit der Transformers-Bibliothek kompatiblen Format.
Llama 2 wurde im Juli 2023 von Meta AI veröffentlicht und stellt eine generelle Verbesserung der ursprünglichen Llama 1 Modelle dar \citep{llama2}.
Neben der nun auf 4096 Tokens erweiterten Kontextlänge (die ursprüngliche Kontextlänge von Llama 1 lag bei 2048 Tokens) stellt Meta AI Llama 2 auch in einer Chat-Version zur Verfügung.
Die Chat-Version wurde zusätzlich mit Hilfe von Reinforcement Learning aus menschlichem Feedback trainiert und ermöglicht so eine einfachere Nutzung der vortrainierten Modelle im Kontext eines Chatbots.
Diese Chat-Modelle wurden jedoch nicht zum Training verwendet, da davon ausgegangen wird, dass dieses zusätzliche Training durch das hier durchgeführte Continual Pretraining überschrieben wird.
Die Llama 2 Modelle wurden unter anderem im Artikel von \citet{llama2} vorgestellt.\\

Der Parameter \texttt{cache\_dir} beschreibt den Speicherort des heruntergeladenen Modells.
Dies ermöglicht ein wiederholtes Trainieren des Modells ohne erneuten Download.
Die heruntergeladenen Modelle werden durch das Training nicht überschrieben und stellen somit den Grundzustand des Modells dar.\\

\newpage
\texttt{use\_fast\_tokenizer} konfiguriert die Verwendung einer schnelleren Version des Tokenizers zur Konvertierung der Datensätze.
Diese Option ist optional und wurde hier nicht verwendet.\\

\texttt{model\_revision} beschreibt die zu verwendende Version des Modells.
Hier wurde die aktuellste Version verwendet, die durch den Wert \enquote{main} repräsentiert wird.\\

\texttt{use\_auth\_token} und \texttt{hugging\_token} beschreiben die Verwendung eines Authentifizierungs-Tokens, um Modelle von Huggingface herunterzuladen.
Diese Authentifizierung ist notwendig, da auch die Llama 2 Modelle unter der gleichen Lizenz wie Llama 1 stehen und nur auf Anfrage zur Verfügung gestellt werden.
Um die Modelle mit beschränktem Zugang herunterzuladen, wurde ein Huggingface-Account erstellt, die Anfrage an Meta AI zur Nutzung der Llama 2 Modelle gestellt und bestätigt und ein Authentifizierungstoken generiert.\\

Der Parameter \texttt{torch\_dtype} beschreibt den Datentyp, der für die Darstellung der Modellparameter verwendet wird.
Hier stehen Float32, Float16 und BFloat16 zur Verfügung.
Vorgefertigte Modelle wurden ursprünglich mit einem Datentyp erstellt und müssen in einen anderen Datentyp konvertiert werden, wenn der \texttt{torch\_dtype} nicht übereinstimmt.
Diese Konvertierung ist rechenintensiv und kann dazu führen, dass Modelle fehlerhaft trainiert werden oder mehr Rechenleistung während der Inferenz und des Trainings benötigen.
Aus diesem Grund wurde hier der Parameter \enquote{auto} verwendet, der den Datentyp des Modells automatisch erkennt und benutzt.\\

\texttt{low\_cpu\_mem\_usage} beschreibt ein Verfahren der Bibliothek Transformers zum Laden großer Modelle auf Systemen mit wenig Arbeitsspeicher.
Dabei werden die Modelle in mehreren Schritten in den Arbeitsspeicher geladen und anschließend in den \ac{gpu}-Speicher übertragen.
Dieses Verfahren reduziert die Menge des notwendigen Arbeitsspeichers, erhöht aber die Zeit, die zum Laden des Modells benötigt wird.
Da im vorliegenden Fall genügend Arbeitsspeicher zur Verfügung stand, wurde auf dieses Verfahren verzichtet.

\subsection{Konfiguration der Trainingsdaten}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}            & \textbf{Wert} \\
        \midrule
        max\_train\_samples           & None          \\
        overwrite\_cache              & false         \\
        block\_size                   & 1024          \\
        validation\_split\_percentage & 5             \\
        preprocessing\_num\_workers   & 1             \\
        keep\_linebreaks              & true          \\
        \bottomrule
    \end{tabular}
    \caption[Parameter zur Konfiguration der Trainingsdaten]{Parameter zur Auswahl und Konfiguration der Trainingsdaten}\label{tab:data-config}
\end{table}
Für das Training des Modells Llama 2 wurde das Buch \citetitle{bb} von \citet{bb} im epub-Format in das Markdown-Format konvertiert.
Die notwendigen Änderungen am Text sind in \cref{sec:datenkuration} beschrieben.
Um diese Markdown-Datei in eine für das Modell verständliche Form umzuwandeln, wird die Bibliothek datasets \citep{datasets} von Huggingface verwendet.
Sie ermöglicht das Laden der Textdatei, die Umwandlung in Tokens und die Aufteilung in Blöcke.
Die Parameter zur Auswahl und Konfiguration der Trainingsdaten sind in \cref{tab:data-config} aufgelistet.\\

Der Parameter \texttt{train\_file} ist nicht in der Tabelle dargestellt, beschreibt jedoch den Pfad zur genutzten Trainingsdatei.
Diese Trainingsdatei wird einmal eingelesen und je nach Anzahl der unter \cref{subsec:config-training} beschriebenen Epochen mehrfach verwendet.\\

\texttt{max\_train\_samples} beschreibt die maximale Anzahl von Blöcken, die aus der Trainingsdatei gelesen werden sollen.
Zu Testzwecken kann hier eine geringere Anzahl an Blöcken verwendet werden, um die Konfiguration des Modells zu testen.
Im finalen Training wurde dieser Parameter auf \enquote{None} gesetzt, um alle Blöcke zu verwenden.\\

Der Parameter \texttt{overwrite\_cache} beschreibt, ob der Text erneut in Tokens umgewandelt werden soll.
Wenn sich der Text geändert hat, kann das Skript hier die nun tokenisierte Textdatei überschreiben.\\

Wie bereits erwähnt, wird der Text in Blöcke aufgeteilt.
Jeder Block wird vom Modell vollständig und gleichzeitig gelesen.
Die maximale Größe eines Blocks ist durch das Modell begrenzt und liegt bei Llama 2 bei 4096 Tokens.
Standardmäßig verwenden Modelle jedoch eine Blockgröße von 1024 Tokens, weshalb hier diese Größe angenommen wird, wenn keine weiteren Angaben gemacht werden.
Größere Blöcke führen zu einer schnelleren Verarbeitung des Textes und können zu einem besseren Verständnis der Zusammenhänge führen, da ein größerer Kontext betrachtet wird.
Allerdings steigt mit der Größe der Blöcke auch die Fehleranfälligkeit, so dass teilweise auch kleinere Blöcke verwendet werden müssen, um ein Training erfolgreich durchzuführen.
Probleme beim Training sind unter \cref{sec:problem-training} beschrieben.
Die Blockgröße kann mit dem Parameter \texttt{block\_size} angepasst werden.\\

Der Parameter \texttt{validation\_split\_percentage} beschreibt den Anteil der Daten, der für die Validierung genutzt werden soll.
Hier werden \SI{5}{\percent} der Daten für die Validierung verwendet.
Die Validierung liefert während des Trainings Informationen über die tatsächliche Leistung des Modells im Vergleich zum ungesehenen Text und dient dazu, den Fortschritt des Modells zu messen.
Steigt der errechnete Fehlerwert des Validierungsdatensatz über einen größeren Zeitraum, ist davon auszugehen, dass das Modell beginnt den Status Overfitting zu erreichen.
Ein Modell, welches einen sehr hohen Fehlerwert des Validierungsdatensatzes, aber einen sehr niedrigen Fehlerwert des Trainingsdatensatzes hat, kann keine ungesehenen Texte mehr verstehen und lediglich den Trainingsdatensatz zitieren.
Die Analyse der Fehlerwerte über mehrere Epochen ist in \cref{ch:results} aufgeführt.\\

\texttt{preprocessing\_num\_workers} beschreibt die Anzahl der Prozesse, die für die Umwandlung der Textdatei in Tokens verwendet werden sollen.
Bei sehr großen Datenmengen ist die Umwandlung von Textdateien in Tokens eine sehr umfangreiche Aufgabe.
Um diesen Vorgang zu beschleunigen, können mehrere Prozesse die Textdateien parallel übersetzen.
In diesem Fall ist die Datenmenge jedoch klein genug, um die Aufgabe mit nur einem Prozess zu erledigen.\\

Der Parameter \texttt{keep\_linebreaks} beschreibt, ob Zeilenumbrüche in der Textdatei erhalten bleiben sollen.
In einigen Fällen können Textdateien viele Zeilenumbrüche enthalten, die der Formatierung des Textes dienen, aber dem Modell keine Informationen liefern oder es dazu veranlassen, diese Zeilenumbrüche zu imitieren.
Aus diesem Grund können Zeilenumbrüche optional entfernt werden.
Aufgrund der zuvor durchgeführten Datenkuration ist dies jedoch nicht notwendig, weshalb dieser Parameter auf \enquote{true} gesetzt wird.\\

\subsection{Konfiguration des Trainings}\label{subsec:config-training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}              & \textbf{Wert}                      \\
        \midrule
        output\_dir                     & ./trained/7B-3                     \\
        overwrite\_output\_dir          & true                               \\
        do\_train                       & true                               \\
        do\_eval                        & true                               \\
        per\_device\_train\_batch\_size & 1                                  \\
        per\_device\_eval\_batch\_size  & 1                                  \\
        evaluation\_strategy            & steps                              \\
        eval\_steps                     & 50                                 \\
        learning\_rate                  & \num{3e-4}                         \\
        weight\_decay                   & \SI{0.1}{}                         \\
        optim                           & adamw\_torch                       \\
        adam\_beta1                     & \SI{0.9}{}                         \\
        adam\_beta2                     & \SI{0.95}{}                        \\
        adam\_epsilon                   & \num{1e-5}                         \\
        max\_grad\_norm                 & \SI{1.0}{}                         \\
        num\_train\_epochs              & 3                                  \\
        lr\_scheduler\_type             & cosine                             \\
        warmup\_steps                   & 0                                  \\
        save\_strategy                  & steps                              \\
        save\_steps                     & 100                                \\
        save\_total\_limit              & 1                                  \\
        no\_cuda                        & false                              \\
        seed                            & 42                                 \\
        fp16                            & true | false                       \\
        bf16                            & false | true                       \\
        half\_precision\_backend        & auto                               \\
        ddp\_backend                    & nccl                               \\
        deepspeed                       & ./ds\_configs/stage2\_offload.json \\
        \bottomrule
    \end{tabular}
    \caption[Parameter zur Konfiguration des Trainings]{Parameter zur Konfiguration des Trainings.
        Die Parameter \texttt{fp16} und \texttt{bf16} können entweder \texttt{true} und \texttt{false} sein, wenn ein Training im \enquote{FP16}-Datenformat bei der Benutzung von Nvidia Tesla V100 Grafikkarten gewählt wird, oder als \texttt{false} und \texttt{true} gewählt werden für ein Training im \enquote{BF16} Format bei der Benutzung von Nvidia Tesla A30 Grafikkarten.}\label{tab:training-config}
\end{table}
Die Konfiguration des Trainings orientiert sich an den \enquote{TrainingArguments}, die ein Teil der Transformers-Bibliothek sind.
Nicht alle Parameter aus der Bibliothek müssen verwendet werden, weshalb hier nur abweichende Parameter vom Standard beschrieben werden.
Die beschriebenen Parameter sind in \cref{tab:training-config} aufgeführt.\\

Der Parameter \texttt{output\_dir} definiert den Speicherpfad für die Ergebnisse des Trainings.
Diese umfassen die trainierten Gewichte des Modells, eine abschließende Auswertung der Validierung, den Status des Trainers sowie gegebenenfalls während des Trainings erstellte Kontrollpunkte.\\

Der Parameter \texttt{overwrite\_output\_dir} legt fest, ob der Ergebnis-Ordner überschrieben werden soll, wenn er bereits existiert.
Diese Option bestimmt auch, ob das Training von einem zuvor erstellten Kontrollpunkt aus fortgesetzt werden soll.
Wenn der Ergebnis-Ordner überschrieben wird, kann von keinem Kontrollpunkt aus fortgefahren werden.\\

Die Parameter \texttt{do\_train} und \texttt{do\_eval} geben an, ob das Training und die Validierung durchgeführt werden sollen.
Falls keine Validierung durchgeführt wird, wird der Trainingsdatensatz dennoch in Trainings- und Validierungsdatensatz aufgeteilt.\\

Während des Trainings und der Validierung werden die Fehlerfunktionen von mehreren Blöcken berechnet und danach gemittelt.
Daraufhin wird ein Gradient basierend auf dieser kumulativen Fehlerfunktion berechnet, welcher die Gewichte des Modells anpasst.
Eine detailliertere Beschreibung des Ablaufs findet sich in \cref{subsec:backpropagation}.
Die Anzahl der Blöcke pro \enquote{Batch} (siehe \cref{def:batch}) wird durch die Parameter \texttt{per\_device\_train\_batch\_size} und \texttt{per\_device\_eval\_\allowbreak{}batch\_size} festgelegt.
Wenn man eine Batch-Größe von 1 und 3 \ac{gpu}s verwendet, erhält man somit 3 Blöcke pro Gradientenberechnung.
Während höhere Batch-Größen die Berechnungszeit reduzieren können, führen sie auch zu ungenaueren Gradienten und erfordern mehr Speicher auf der \ac{gpu}.
Die Batch-Größe ist in diesem Fall auf 1 gesetzt, da die verwendeten Nvidia V100-\ac{gpu}s als auch Nvidia A30-\ac{gpu}s nur genug Speicher für eine Batch-Größe von 1 hatten.\\

Die Parameter \texttt{evaluation\_strategy} und \texttt{eval\_steps} legen fest, in welchen Intervallen die Validierung durchgeführt werden soll.
In diesem Szenario wird die Validierung alle 50 Iterationen durchgeführt.
Eine Iteration bezieht sich auf die Berechnung eines Gradienten.\\

Die Lernrate des Modells wird durch den Parameter \texttt{learning\_rate} beschrieben.
Dieser Parameter bestimmt, in welchem Ausmaß die Gewichte des Modells angepasst werden.
Eine sehr hohe Lernrate kann dazu führen, dass das Modell nicht konvergiert, während eine sehr niedrige Lernrate zu langen Trainingszeiten führt.
Die Lernrate wurde aus dem Artikel über Llama-Modelle von \citet{llama} übernommen.\\

\enlargethispage{0.5\baselineskip}
In dem Artikel zu den Llama-Modellen von \citet{llama} wird ebenfalls ein Gewichtsabnahme-Wert von $0,1$ verwendet, der auch bei diesem Training mithilfe des Parameters \texttt{weight\_decay} eingestellt wurde.
Die Gewichtsabnahme führt zu einer kontinuierlichen Verringerung der Gewichte des Modells. Dadurch soll verhindert werden, dass es sich zu stark an einzelne Trainingsdaten anpasst.\\

Die Parameter \texttt{adam\_beta1}, \texttt{adam\_beta2} und \texttt{adam\_epsilon} beschreiben die Parameter des verwendeten AdamW-Optimierers, der im Parameter \texttt{optim} eingestellt ist.
Diese Werte entsprechen ebenfalls den Werten, die im Artikel zu den Llama-Modellen von \citet{llama} angegeben sind.
AdamW ist in verschiedenen Implementierungen verfügbar. Hier wurde die neueste Implementierung der PyTorch-Bibliothek verwendet.
Die Funktionsweise des AdamW-Optimierers ist im Artikel von \citet{adamw} genauer beschrieben.\\

Der Parameter \texttt{max\_grad\_norm} beschreibt die maximale Norm des Gradienten (siehe \cref{subsec:backpropagation}), die durch die Gewichte des Modells nicht überschritten werden darf.
Eine weitere Bezeichnung, die in der Bibliothek DeepSpeed oder in Artikeln zu Llama-Modellen genutzt wird, ist \texttt{gradient clipping}.
Er begrenzt die Größe des Gradienten, der in großen neuronalen Netzen explosionsartig ansteigen kann.
Zu große Gradienten führen zu schlechteren Trainingsergebnissen und zu einer zu starken Anpassung der Gewichte, was wiederum zu einer Oszillation um ein Minimum führt.
Um die Gradienten zu begrenzen, wird die L2-Norm des Gradienten berechnet\footnote{Die L2-Norm, auch bezeichnet als euklidische Norm, entspricht der pythagoreischen Länge eines Vektors vom Ursprung. Gegeben eines Vektors $(a,b,c)$ entspräche die L2-Norm $\sqrt{a^2+b^2+c^2}$}.
Wenn diese Norm den angegebenen Maximalwert überschreitet, wird der Gradient herunter skaliert, bis er die Maximalnorm nicht mehr überschreitet.\\

Der Parameter \texttt{num\_train\_epochs} gibt die Anzahl der zu trainierenden Epochen an.
Eine Epoche umfasst einen vollständigen Durchlauf des Trainingsdatensatzes.
Mehr Epochen können insbesondere bei kleineren Datensätzen zu besseren Ergebnissen führen, da sich das Modell besser an die Trainingsdaten anpassen kann.
Zu viele Epochen führen zu einer Überanpassung.
In dieser Arbeit wurde das Modell Llama 2 7B mit einer, drei, fünf und zehn Epochen trainiert.
Bei der Verwendung von Nvidia V100 Grafikkarten traten während des Trainings Probleme auf, die in \cref{sec:problem-training} genauer beschrieben sind.\\

Durch die Anwendung der Parameter \texttt{lr\_scheduler} und \texttt{warmup\_\allowbreak{}steps} kann eine Aufwärmphase des Trainings realisiert werden.
Die Aufwärmphase bezeichnet den Start des Trainings, in dem die Lernrate der \texttt{lr\_scheduler}-Funktion innerhalb der ersten \texttt{warmup\_steps} Iterationen schrittweise von $0$ auf den gewünschten Wert erhöht wird.
Dieser Ansatz verhindert eine zu schnelle Anpassung der Modellgewichte an spezielle Details der Trainingsdaten.
Vor allem bei untrainierten Modellen werden während des Trainings diese erlernten Fehler wieder korrigiert. Dies führt jedoch ohne eine Aufwärmphase zu längeren Trainingszeiten und schlechteren Ergebnissen.
In diesem Fall kann die Aufwärmphase übersprungen werden, da ein bereits vortrainiertes Modell genutzt wird.\\

Die Parameter \texttt{save\_strategy}, \texttt{save\_steps} und \texttt{save\_total\_limit} beschreiben, wie oft und in welchem Abstand Modelle während des Trainings gespeichert werden sollen.
Hier wird alle 100 Iterationen das Modell gespeichert, wobei maximal 1 Kontrollpunkt gleichzeitig existiert.
Kontrollpunkte ermöglichen es, abgebrochene oder fehlgeschlagene Trainingsläufe wieder aufzunehmen.
Bei längerem Training kann die maximale Laufzeit der Skripte erreicht werden.
Eine Wiederaufnahme des Trainings an diesem Punkt ist dann möglich.
Das Rechenzentrum beschränkt die maximale Laufzeit von Skripten auf 2 Tage.
Wenn das Training länger als 2 Tage dauert, kann diese Grenze überschritten werden.
Die Begrenzung ist notwendig aufgrund der gemeinsamen Nutzung des Rechenzentrums.
Durch die regelmäßige Unterbrechung von Skripten können andere Skripte in der Warteschlange zwischengeschoben werden.
Zusammen mit der Begrenzung ergibt sich im Allgemeinen eine Wartezeit von maximal 2 Tagen.
Durch die Einführung von Kontrollpunkten im Training führt eine Unterbrechung nicht zu einem Rückschlag im Fortschritt.\\

Der Begriff \texttt{no\_cuda} beschreibt die Durchführung des Trainings ohne die Verwendung einer \ac{gpu}.
Diese Einstellung wird für Testzwecke verwendet, damit Testskripte auf Systemen ausgeführt werden können, die nicht über die erforderliche Anzahl von \ac{gpu}s verfügen.\\

Die Verwendung des Parameters \texttt{seed} beeinflusst die Zufälligkeit des Trainingsprozesses.
Während des Trainingsprozesses werden einige Zufallsvariablen verwendet, um Werte zu initialisieren.
Jedoch ist die Auswirkung im Kontext des Fortführenden Trainings irrelevant.
Es wird jedoch ein fester Wert gesetzt, da das Trainingsskript auch für untrainierte Modelle genutzt werden kann.
Mit Hilfe eines Seeds erstellen Zufallszahlengeneratoren zufällig verteilte, jedoch reproduzierbare Zahlenfolgen.
Daher sind Modelle mit gleichem Seed und Datensatz identisch.\\

Zur Konfiguration der verwendeten Datentypen stehen neben dem oben genannten \texttt{torch\_dtype} die beiden Parameter \texttt{fp16} und \texttt{bf16} zur Verfügung. Während \texttt{torch\_dtype} die Initialisierung des Modells beschreibt, beeinflussen diese Parameter die Datentypen während des Trainings.
Es kann nur eine der beiden Optionen verwendet werden. Im Falle des Trainings mit Nvidia V100 Grafikkarten und der \ac{zero} Stufe 2 Optimierung wurde FP16 aufgrund von Architekturanforderungen verwendet. Beim Training mit Nvidia A30 Grafikkarten und \ac{zero} Stufe 3 Optimierung wurde BF16 verwendet.
Die Zahl 16 steht hier für die Anzahl der Bits pro Wert, was wiederum einen großen Einfluss auf den Speicherbedarf und die Genauigkeit des Modells hat.
Datentypen sollten nicht geändert werden, wenn ein vortrainiertes Modell weiter trainiert wird, da eine Änderung schnell zu Fehlverhalten führen kann.
Die Llama 2 Modelle wurden mit dem Datentyp \enquote{bf16} (ausgeschrieben BrainFloat 16) trainiert.
Dieser Datentyp ist nur auf bestimmten \ac{gpu}-Architekturen verfügbar.
Ein Datum im BFloat16-Format hat eine kleinere Mantisse (7 Bits) als ein Float16-Datum (10 Bits Mantisse). Dies führt zu einer geringeren Genauigkeit, verkürzt jedoch die Zeit, die für die Konvergenz des Modells benötigt wird.
Die verwendeten V100 Grafikkarten haben nicht die notwendige Architektur, um BFloat16 Werte zu unterstützen, daher wurde das Llama 2 Modell hier in ein Float16 Format konvertiert.
Dies führt zu Problemen beim Training und beschränkt die maximale Anzahl der Epochen auf 3. Eine genauere Erklärung der Probleme ist in \cref{sec:problem-training} beschrieben.
Bei der Verwendung von A30-Grafikkarten ist die Nutzung von BFloat16-Werten möglich. Mit diesem Datentyp wurde das Modell zusätzlich auf 5 und 10 Epochen trainiert.\\

Die Parameter \texttt{half\_precision\_backend} und \texttt{ddp\_backend} beschreiben Architekturen für die Ausführung des Trainings mit Hilfe der Bibliothek Transformers.
Während \texttt{half\_precision\_backend} die Architektur für die Ausführung von Trainingsschritten mit Float16-Werten festlegt, beschreibt \texttt{ddp\_backend} die Kommunikationsarchitektur für den Datenaustausch zwischen \ac{gpu}s.
Die DeepSpeed Bibliothek verwendet die \enquote{NCCL} Architektur, daher wird diese hier eingestellt.\\

Die DeepSpeed Konfiguration wird separat in einer JSON Datei gespeichert.
Durch Setzen des Parameters \texttt{deepspeed} verwendet die Transformers Bibliothek \citep{transformers} die DeepSpeed Bibliothek \citep{deepspeed} zur Ausführung des Trainings.
Wichtig bei der Verwendung von DeepSpeed mit Transformers ist eine identische Konfiguration der Trainingsparameter, wie zum Beispiel \texttt{batch\_size}, \texttt{gradient\_accumulation\_steps} und \texttt{learning\_rate}.\\

\subsection{Konfiguration des DeepSpeed Trainings}\label{sec:deepspeed-config}

\begin{table}
    \centering
    \resizebox{!}{0.35\paperheight}{%
        \begin{tabular}{ll}
            \toprule
            \textbf{Parameter}                  & \textbf{Wert} \\
            \midrule
            \multicolumn{2}{c}{fp16}                            \\
            enabled                             & auto          \\
            loss\_scale                         & $0$           \\
            loss\_scale\_window                 & $1000$        \\
            initial\_scale\_power               & $16$          \\
            hysteresis                          & $2$           \\
            min\_loss\_scale                    & $1$           \\
            \midrule
            \multicolumn{2}{c}{bf16}                            \\
            enabled                             & true|false    \\
            \midrule
            \multicolumn{2}{c}{optimizer}                       \\
            type                                & AdamW         \\
            lr                                  & auto          \\
            betas                               & auto          \\
            eps                                 & auto          \\
            weight\_decay                       & auto          \\
            \midrule
            \multicolumn{2}{c}{scheduler}                       \\
            type                                & WarmupLR      \\
            warmup\_min\_lr                     & auto          \\
            warmup\_max\_lr                     & auto          \\
            warmup\_num\_steps                  & auto          \\
            \midrule
            \multicolumn{2}{c}{zero\_optimizations}             \\
            stage                               & $2$           \\
            contiguous\_gradients               & true          \\
            overlap\_comm                       & true          \\
            reduce\_scatter                     & true          \\
            reduce\_bucket\_size                & \num{2e8}     \\
            allgather\_bucket\_size             & \num{2e8}     \\
            \midrule
            \multicolumn{2}{c}{offload\_optimizer}              \\
            device                              & cpu           \\
            pin\_memory                         & true          \\
            \midrule
            gradient\_clipping                  & $1$           \\
            steps\_per\_print                   & $500$         \\
            wall\_clock\_breakdown              & false         \\
            train\_micro\_batch\_size\_per\_gpu & auto          \\
            \bottomrule
        \end{tabular}}
    \caption[DeepSpeed Konfiguration]{DeepSpeed Konfiguration mit Benutzung der \ac{zero} Stufe 2 Optimierung. Für diese Stufe wurde der Parameter \texttt{enabled} unter der Rubrik \texttt{bf16} auf \enquote{false} gesetzt. Für die Nutzung der \ac{zero} Stufe 3 Optimierung ist dieser Wert auf \enquote{true} gesetzt.}\label{tab:deepspeed-config}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}                    & \textbf{Wert} \\
        \midrule
        \multicolumn{2}{c}{zero\_optimization}                \\
        stage                                 & $3$           \\
        contiguous\_gradients                 & true          \\
        stage3\_max\_live\_parameter          & \num{1e9}     \\
        stage3\_max\_resuse\_distance         & \num{1e9}     \\
        stage3\_prefetch\_bucket\_size        & \num{1e7}     \\
        stage3\_param\_persistence\_threshold & \num{1e5}     \\
        reduce\_bucket\_size                  & \num{1e7}     \\
        sub\_group\_size                      & \num{1e9}     \\
        \midrule
        \multicolumn{2}{c}{offload\_param}                    \\
        device                                & cpu           \\
        pin\_memory                           & true          \\
        \bottomrule
    \end{tabular}
    \caption[DeepSpeed ZeRO 3 Konfiguration]{DeepSpeed \ac{zero} Stufe 3 Konfiguration}\label{tab:deepspeed-config-stage3}
\end{table}

Die DeepSpeed-Konfiguration ist in \cref{tab:deepspeed-config} beschrieben.
Die hier verwendeten Parameter beschreiben ein Training mit \ac{zero} Stufe 2 Optimierung und zusätzlichem CPU Offloading.
Die \ac{zero} Stufe 2 Optimierung beschreibt einen Algorithmus zur Reduzierung des Speicherbedarfs während des Trainings.
Dieser Algorithmus wird in \citet{deepspeed} genauer beschrieben.
Zusätzlich zu dieser Optimierung werden Teile der Berechnungen auf die \ac{cpu} ausgelagert, um den Speicherbedarf pro \ac{gpu} weiter zu reduzieren.
Diese Konfiguration wurde beim Training der Modelle mit Nvidia Tesla V100 Grafikkarten verwendet.
\cref{tab:deepspeed-config-stage3} beschreibt zusätzlich eine Konfiguration des \texttt{zero\_optimization} Abschnitts bei Verwendung der \ac{zero} Stufe 3 Optimierung.
Hier werden zusätzlich Parameter der Modelle auf die CPU ausgelagert.
Diese Konfiguration wurde beim Training der Modelle mit Nvidia Tesla A30 Grafikkarten verwendet.\\\

Der Abschnitt \texttt{fp16} beschreibt den Umgang mit Float16-Werten während des Trainings.
Dabei handelt es sich um eine dynamische Skalierung der Fehlerwerte (engl. \enquote{Dynamic Loss Scaling}).
Die Skalierung der Fehlerwerte ist notwendig, da aufgrund der geringeren Genauigkeit von Float16-Werten kleinere Werte der Fehlerwerte gerundet werden und verloren gehen.
Aus diesem Grund werden die Fehlerwerte bei der Berechnung um mehrere Potenzen skaliert.
Diese Skalierung kann auch zu einem Überlauf über den Wertebereich des Float16 Datentyps führen.
DeepSpeed verwendet hier eine automatische, dynamische Skalierung der Fehlerwerte, ohne einen Überlauf zu verursachen.
Eine genauere Erklärung der Skalierung von Fehlerwerten findet sich im Artikel von \citet{lossscale}.\\

Der Parameter \texttt{loss\_scale} beschreibt die konstante Skalierung der Fehlerwerte.
Ist er auf $0$ gesetzt, wird eine dynamische Skalierung verwendet.
Der Parameter \texttt{loss\_scale\_window} beschreibt das Werteintervall, in dem die dynamische Skalierung erfolgt.
Der Parameter \texttt{initial\_scale\_power} beschreibt die Größe der initialen Skalierung der Fehlerwerte.
Die tatsächliche Skalierung der Fehlerwerte entspricht $2^{initial\_scale\_power}$.
Der Parameter \texttt{hysteresis} beschreibt die minimale Anzahl von Schritten, in denen die Skalierung nicht verändert werden kann.
Der Parameter \texttt{min\_loss\_scale} beschreibt die minimale Skalierung der Fehlerwerte.
Hier entspricht der Wert $1$ keiner Skalierung.\\

Der Abschnitt \texttt{bf16} beschreibt die Verwendung des binären Datentyps BF16 (ausgeschrieben BrainFloat 16) während des Trainings.
Ist er auf \texttt{true} gesetzt, so werden die Berechnungen mit dem Datentyp BFloat16 durchgeführt.
Ist er auf \texttt{false} gesetzt, so werden die Berechnungen mit dem Datentyp Float16 durchgeführt.
Bei einem Training mit V100 Grafikkarten und \ac{zero} Stufe 2 Optimierung ist dieser Parameter auf \enquote{false} gesetzt.
Im Falle des Trainings mit A30 Grafikkarten und einer \ac{zero} Stufe 3 Optimierung ist dieser Parameter auf \enquote{true} gesetzt.\\

Der Abschnitt \texttt{optimizer} beschreibt die Parameter des Optimierungsalgorithmus. Wie bereits bei den Trainingsparametern beschrieben, wird der AdamW-Optimierer verwendet.
Die Parameter \texttt{lr}, \texttt{betas}, \texttt{eps} und \texttt{weight\_decay} müssen mit den in den Trainingsparametern eingestellten Werten übereinstimmen.
Aus diesem Grund werden diese Parameter vor Beginn des Trainings auf \enquote{auto} gesetzt und durch die Transformers-Bibliothek ergänzt.\\

Der Abschnitt \texttt{scheduler} beschreibt die Parameter des Lernratenplaners. Der Lernratenplaner ist für die Anpassung der Lernrate während des Trainings verantwortlich.
Er dient in diesem Fall zur Durchführung der Aufwärmphase. Ist die Aufwärmphase in den Trainingsargumenten deaktiviert, so wird die Bibliothek diese Phase auch in der DeepSpeed-Konfiguration deaktivieren.\\

Der Abschnitt \texttt{zero\_optimizations} beschreibt die Parameter der \ac{zero} Stufe 2 Optimierung \citep{deepspeed}.
Der Parameter \texttt{contiguous\_gradients} beschreibt die Auslagerung der Gradienten in einen zusammenhängenden Speicherbereich während der Berechnung.
Dadurch wird eine Fragmentierung des Speichers während der Backpropagation vermieden.
Mit Hilfe des Parameters \texttt{overlap\_comm} wird versucht, die Gradienten während der Berechnung der Backpropagation zu reduzieren, um eine schnellere Gesamtberechnung zu ermöglichen.
\texttt{reduce\_scatter} beschreibt die Verwendung einer speziellen Reduktionsmethode \enquote{Reduce Scatter} zur Mittelung von Gradienten.
In Kombination mit den Parametern \texttt{reduce\_bucket\_size} und \texttt{allgather\_bucket\_size} wird die maximale Anzahl der in einem Schritt zu reduzierenden Gradienten festgelegt.
Diese Option reduziert den Speicherbedarf während des Trainings erheblich.
Mit Hilfe der Option \texttt{offload\_optimizer} wird der Zustand und die Berechnung des Optimierers auf die CPU ausgelagert.
Optional kann dies für sehr große Modelle auch auf eine NVMe SSD erfolgen.
Mit Hilfe der Einstellung \texttt{pin\_memory} wird der für die Berechnung benötigte Speicher auf der CPU reserviert.
Dies führt zu einer besseren Performance, aber auch zu zusätzlichem Speicherbedarf.\\

Wie bereits bei den Trainingsargumenten durch den Parameter \texttt{max\_grad\_norm} beschrieben, wird die maximale Größe des Gradienten durch den Parameter \texttt{gradient\_clipping} festgelegt.
Eine automatische Übernahme der Konfiguration der Trainingsargumente ist in diesem Fall nicht möglich.
Die Werte müssen dennoch übereinstimmen.\\

Die Parameter \texttt{steps\_per\_print} und \texttt{wall\_clock\_breakdown} beschreiben die Ausgabe von Informationen während des Trainings.
Zusätzlich zur Ausgabe der Transformers-Bibliothek werden alle 500 Iterationen weitere Informationen der DeepSpeed-Bibliothek ausgegeben.
Mit Hilfe des Parameters \texttt{wall\_clock\_breakdown} wird zusätzlich die Messung der verstrichenen Zeit für jede Phase einer Iteration ausgegeben.\\

Der Parameter \texttt{train\_micro\_batch\_size\_per\_gpu} beschreibt die Anzahl der Batches pro GPU.
Dieser Wert wird aus den Trainingsargumenten übernommen.\\

Für die Ausführung des Trainings mit der \ac{zero} Stufe 3 Optimierung auf Nvidia Tesla A30 Grafikkarten wurde der Abschnitt \texttt{zero\_\allowbreak{}optimization} in \cref{tab:deepspeed-config} durch die in \cref{tab:deepspeed-config-stage3} gezeigten Parameter ersetzt.
Weggefallene Parameter sind:
\begin{itemize}
    \item \texttt{overlap\_comm}
    \item \texttt{reduce\_scatter}
    \item \texttt{allgather\_bucket\_size}
\end{itemize}

Zusätzlich hinzugekommene Parameter sind:
\begin{itemize}
    \item \texttt{stage3\_max\_live\_params}
    \item \texttt{stage3\_max\_reuse\_distance}
    \item \texttt{stage3\_prefetch\_bucket\_size}
    \item \texttt{stage3\_param\_persistence\_threshold}
    \item \texttt{sub\_group\_size}
\end{itemize}
sowie der Abschnitt \texttt{offload\_param}.
Der Parameter \texttt{stage3\_max\_live\_\allowbreak{}params} beschreibt die maximale Anzahl von Gewichten, die während des Trainings in den Speicher der \ac{gpu} geladen werden. Kleinere Werte führen zu weniger Speicherverbrauch, aber auch zu mehr Kommunikation.
Der Parameter \texttt{stage3\_max\_reuse\_distance} beschreibt, dass ein Gewicht nicht freigegeben werden kann, wenn es innerhalb dieser Anzahl von Gewichten erneut zur Berechnung verwendet wird. Auch hier gilt, dass kleinere Werte zu einer geringeren Speicherbelegung, aber auch zu einer erhöhten Kommunikation führen.
Der Parameter \texttt{stage3\_prefetch\_bucket\_size} beschreibt die Anzahl der Gewichte, die während des Trainings im Voraus geladen werden.
Der Parameter \texttt{stage3\_param\_persistence\_threshold} verhindert die Partitionierung von Gewichten, die kleiner als dieser Wert sind.
Der Parameter \texttt{sub\_group\_size} beschreibt die maximale Anzahl von Parametern, die gleichzeitig für die Berechnung verwendet werden.

\subsection{Verwendete Bibliotheken zum Training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Bibliothek} & \textbf{Version} \\
        \midrule
        datasets            & 2.13.1           \\
        DeepSpeed           & 0.10.0           \\
        evaluate            & 0.4.0            \\
        PyTorch             & 2.0.1            \\
        sentencepiece       & 0.1.96           \\
        Transformers        & 4.31.0           \\
        \bottomrule
    \end{tabular}
    \caption{Verwendete Bibliotheken zum Training}\label{tab:training-libraries}
\end{table}
Die für das Training verwendeten Bibliotheken sind in \cref{tab:training-libraries} aufgelistet.
Einige der hier aufgeführten Pakete benötigen weitere Abhängigkeiten, die über die verwendete Paketverwaltung \ac{pip}\footnote{\url{https://github.com/pypa/pip} abgerufen am 19.8.2023} installiert werden.\\

Zur Verwaltung der Datensätze wird die Bibliothek \enquote{datasets} verwendet \citep{datasets}.
Mit Hilfe dieser Bibliothek können Textdateien während des Trainings geladen, in Tokens umgewandelt und in Blöcke gruppiert werden.
Über eine Schnittstelle zu den Bibliotheken Transformers und DeepSpeed ermöglicht \enquote{datasets} auch die geordnete Zuordnung von Blöcken zu verschiedenen \ac{gpu}s.\\

Die Bibliothek \enquote{DeepSpeed} wird zur Beschleunigung des Trainings und der Ausführung auf mehreren Grafikkarten verwendet.
Sie ermöglicht die Nutzung der \ac{zero}-Optimierung, die es erlaubt, Teile des Trainings und des Modells auf mehrere GPUs, der CPU und lokalen NVMe SSDs auszulagern.
Der Prozess dieser Optimierung und die Einführung dieser Bibliothek werden in \citet{deepspeed} genauer beschrieben.\\

Die Bibliothek \enquote{evaluate} wird zur Berechnung der Modellgenauigkeit verwendet.
Sie stammt von Huggingface\footnote{\url{https://github.com/huggingface/evaluate} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{PyTorch} bildet die Grundlage für die Berechnung der neuronalen Netze.
Sie ermöglicht neben der eigentlichen Berechnung aller Trainingsphasen auch die Verwendung von \ac{gpu}s \citep{pytorch}.\\

Die Bibliothek \enquote{sentencepiece} wird für die Tokenisierung der Texte genutzt.
Sie wird im Skript nicht explizit verwendet, ist hier aber als zusätzliche Voraussetzung für die Verwendung des Tokenizers des Llama-Modells aufgeführt.
Diese Bibliothek wurde auch im Artikel von \citet{llama} verwendet und wurde von Google veröffentlicht \citep{sentencepiece}.\\

Die Bibliothek \enquote{Transformers} wird für die Verwaltung der Modelle und die Berechnung der neuronalen Netze verwendet.
Sie ermöglicht die Verwendung vortrainierter Modelle und bietet einen abstrakten Trainer, der die verschiedenen Phasen des Trainings steuert.
Außerdem erlaubt sie die Verwendung der Bibliothek \enquote{DeepSpeed} ohne zusätzliche Anpassung.
Die Bibliothek wurde in \citet{transformers} vorgestellt.\\

\subsection{Training auf einem GPU Computing Cluster}\label{sec:training-cluster}
Das Trainieren der Modelle ist wesentlich rechenintensiver als deren Anwendung.
Aus diesem Grund kann ein normaler Computer dafür nicht verwendet werden.
Stattdessen wurde ein GPU Computing Cluster des Rechenzentrums der Universität Leipzig verwendet.\\


\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Cluster} & \textbf{Knoten} & \textbf{Grafikkarten}        & \textbf{GPU-RAM}    \\
        \midrule
        Paula            & 12              & 8 Nvidia Tesla A30           & \SI{24}{\giga\byte} \\
        Clara            & 8               & 4 Nvidia Tesla V100          & \SI{32}{\giga\byte} \\
        Clara            & 23              & 8 Nvidia GeForce RTX 2080 Ti & \SI{11}{\giga\byte} \\
        \bottomrule
    \end{tabular}
    \caption[GPU Werte des Rechenzentrums]{ \ac{gpu} Werte des des Rechenzentrums der Universität Leipzig}\label{tab:gpu-cluster}
\end{table}
\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Cluster} & \textbf{CPU}           & \textbf{RAM}         \\
        \midrule
        Paula            & 2 AMD(R) EPYC(R) 7713  & \SI{1}{\tera\byte}   \\
        Clara            & 1 AMD(R) EPYC(R) 7551P & \SI{512}{\giga\byte} \\
        \bottomrule
    \end{tabular}
    \caption[CPU Werte des Rechenzentrums]{ \ac{cpu} Werte des Rechenzentrums der Universität Leipzig}\label{tab:cpu-cluster}
\end{table}
Das Rechenzentrum der Universität Leipzig stellt zwei GPU Computing Cluster mit den Namen \enquote{Paula} und \enquote{Clara} zur Verfügung.
Die technischen Daten der beiden Cluster sind in \cref{tab:gpu-cluster} und \cref{tab:cpu-cluster} aufgelistet.
Das \enquote{Clara}-Cluster ist hierbei zweigeteilt, da einige Knoten mit Nvidia Tesla V100 Grafikkarten und andere mit Nvidia GeForce RTX 2080 Ti Grafikkarten ausgestattet sind.
Alle Knoten innerhalb eines Clusters sind über ein \SI{100}{\giga\bit\per\second} Infiniband-Netzwerk miteinander verbunden.
Die Verbindung zum Internet erfolgt über eine \SI{25}{\giga\bit\per\second} Ethernet-Verbindung.\\

Die Ausführung der Skripte erfolgt mit Hilfe der Software \enquote{Slurm Workload Manager}\footnote{\url{https://slurm.schedmd.com/documentation.html} abgerufen am  24.8.2023}.
Diese ermöglicht eine umfangreiche Reservierung von Ressourcen und bietet eine Bash-Schnittstelle zur Konfiguration und Ausführung der Skripte.
Mit Hilfe von Slurm-Bash-Dateien konnte das Training der Modelle auf dem Cluster durchgeführt werden.\\

Während des Trainings wurde durchschnittlich alle 12 Sekunden eine Iteration des Modells berechnet.
Eine Iteration entspricht einer vollständigen Berechnung der Ausgaben über einen Batch und einer vollständigen Anpassung aller Gewichte (engl. \enquote{Forwards-} und \enquote{Backwardspass}).
Eingeschlossen sind auch alle Kommunikationen, die zwischen den GPUs durchgeführt werden, um ihren aktuellen Zustand untereinander abzustimmen.
Diese Geschwindigkeit ist unabhängig von der Blockgröße.
Allerdings hängt die Länge des Trainings von der Anzahl der Iterationen ab, daher von der Anzahl der Blöcke und somit von der Blockgröße.
Der kurierte Datensatz, der das Buch \citet{bb} repräsentiert, umfasst ca. 34.500 Tokens.
Dementsprechend wird eine Epoche bei einer Standardblockgröße von 1024 Tokens in 34 Schritten beziehungsweise 6:14 Minuten berechnet.
Eine größere Blockgröße war aufgrund von Speicherfehlern nicht möglich.\\

Bei der Ausführung des Trainings auf Nvidia v100 GPUs traten in unregelmäßigen Abständen Fehler auf, die in \cref{sec:problem-training} genauer beschrieben sind.
Diese konnten durch eine Reduzierung der Blockgröße auf 256 Tokens minimiert werden. Eine Epoche bestand somit aus 134 Schritten und dauerte 24:34 Minuten.\\

Insgesamt wurden 6 Modelle trainiert und 8 Modelle evaluiert.
\cref{tab:trained-models} zeigt die in dieser Arbeit trainierten Modelle.\\
\begin{table}
    \centering
    \begin{tabular}{lll}
        \toprule
        \textbf{Epoche} & \textbf{Grafikkarten} & \textbf{Bezeichnung} \\
        \midrule
        1               & 4 Nvidia Tesla V100   & llama\_1e\_v100      \\
        1               & 4 Nvidia Tesla A30    & llama\_1e\_a30       \\
        3               & 4 Nvidia Tesla V100   & llama\_3e\_v100      \\
        3               & 4 Nvidia Tesla A30    & llama\_3e\_a30       \\
        5               & 4 Nvidia Tesla A30    & llama\_5e\_a30       \\
        10              & 4 Nvidia Tesla A30    & llama\_10e\_a30      \\
        \bottomrule
    \end{tabular}
    \caption[Trainierte Llama 2 7B Modelle]{Trainierte Llama 2 7B Modelle. Das Training wurde von dem vortrainierten Llama 2 7B Huggingface Modell begonnen.}\label{tab:trained-models}
\end{table}

Zusätzlich evaluiert wurden die Modelle:
\begin{itemize}
    \item GPT4 Modell: \enquote{gpt4}
    \item Llama 2 7B untrainiert: \enquote{llama\_$0$e}
\end{itemize}

\subsection{Probleme während des Trainings}\label{sec:problem-training}
Die Durchführung des Trainings verlief nicht ohne Probleme.
Ideal wäre es, mehrere Modelle mit unterschiedlicher Epochenzahl und Größe zu trainieren.
Neben dem Modell Llama 2 7B verspricht ein größeres Modell wie die Modelle 13B oder 70B deutlich bessere Leistungen sowohl in der Textnachahmung als auch im Wissensverständnis und damit in den Ergebnissen der Evaluation.
Das Training wurde auf 4 \ac{gpu}s pro Knoten mit jeweils \SI{32}{\giga\byte} \ac{gpu}-\ac{ram} bzw. \SI{24}{\giga\byte} durchgeführt, wie bereits unter \cref{sec:training-cluster} erwähnt.
Um größere Modelle zu trainieren, muss diese Anzahl erhöht werden.
Die Trainingsparameter erlauben hier eine einfache Erweiterung auf zusätzliche Grafikkarten.
Die Trainingsskripte müssen dafür nicht angepasst werden.\\

Das Training auf mehreren Knoten mit mehr Grafikkarten wurde testweise durchgeführt.
Dazu wurde ein Testskript von Huggingface, das die Kommunikation zwischen Grafikkarten und Knoten sicherstellt, und das Trainingsskript verwendet.
Beide Skripte sind in dieser Konfiguration fehlgeschlagen.
Der Grund dafür ist ein Kommunikationsfehler zwischen den Knoten.
Die Kommunikation während des Trainings wird vollständig von den Bibliotheken Transformers und DeepSpeed übernommen.
Dazu wird ein verwendeter Knoten als Main-Knoten definiert, der das Nachrichten- und Daten-Routing übernimmt.
Andere Knoten übernehmen die Rolle von Client-Knoten.
Auf dem Main-Knoten wird ein Server gestartet, mit dem sich die Client-Knoten über IPv4 oder IPv6 verbinden können.
Diese Verbindung schlug in allen Einstellungen fehl.
Eine Lösung für dieses Problem konnte nicht gefunden werden.
Aus diesem Grund wurde das Training auf einem Knoten und damit 4 \ac{gpu}s beschränkt.\\

Größere Modelle wie das 70B Modell können mit dem aktuellen Setup nicht trainiert werden.
Aus diesem Grund und um Ressourcen zu sparen, wurde das Training mit dem Modell Llama 2 7B begonnen.
Bei der Ausführung auf Nvidia V100 Grafikkarten traten in unregelmäßigen Abständen Überlauffehler auf.
Wie in \cref{sec:deepspeed-config} beschrieben, werden die berechneten Fehlerwerte der Fehlerrückkopplungsfunktion während der Berechnung skaliert, um eine höhere Genauigkeit zu erreichen und einen Unterlauf zu vermeiden.
Diese Skalierung führte zu einem Überlauf über den Maximalwert des Datentyps Float16.
Da in der DeepSpeed-Konfiguration eine dynamische Fehlerskalierung eingestellt war, wurde die Skalierung anschließend um eine Potenz reduziert.
Dieser Vorgang wiederholte sich in unregelmäßigen Abständen und führte schließlich zum Erreichen des minimalen Skalierungsfaktors 1, der keiner Skalierung entspricht.
Ist dieser Wert erreicht und eine Skalierung dennoch erforderlich, wird das Training abgebrochen.\\

Die Gründe für die aufgetretenen Skalierungsfehler sind nicht bekannt.
Eine mögliche Ursache ist die Verwendung des Llama-Modells mit dem Datentyp Float16.
Dieses Modell wurde ursprünglich mit dem Datentyp BFloat16 trainiert.
Eine Transformation in Float16-Werte wird auch von Huggingface nicht empfohlen.
Ebenso wird ein Fehler in der Implementierung von DeepSpeed vermutet, der das Laden von Modellen im Float16-Format nicht korrekt umsetzt.
Im offiziellen GitHub Repository von DeepSpeed wurde bereits eine Lösung vorgeschlagen, die jedoch noch nicht in DeepSpeed übernommen wurde.
Die Verwendung von BFloat16 Werten ist nur auf dem Cluster \enquote{Paula} mit Nvidia Tesla A30 Grafikkarten möglich.
BFloat16 Werte können nur von Grafikkarten umgesetzt werden, die die AMP (Automatic Mixed Precision) Architektur unterstützen.
Diese Architektur wird von den Grafikkarten der Nvidia A100 Serie unterstützt, die für das initiale Training der Llama Modelle verwendet wurden.
Im Rechenzentrum stehen jedoch nur Grafikkarten der Serien Nvidia Tesla V100, Nvidia GeForce RTX 2080 Ti und Nvidia Tesla A30 zur Verfügung.
Sowohl die V100- als auch die RTX2080-Serie unterstützen die Verwendung von BFloat16-Werten nicht.
Die A30-Serie unterstützt zwar BFloat16, bietet aber nur \SI{24}{\giga\byte} \ac{ram}, weshalb hier eine \ac{zero} Stufe 3 Optimierung gewählt wurde.
Bei der Verwendung von A30-Grafikkarten traten bei einem Training von bis zu 10 Epochen keine Skalierungsfehler auf.\\

Anfangs verhinderte der Skalierungsfehler das Training der Modelle auf V100-Grafikkarten vollständig.
Durch Anpassung der Blockgröße in \cref{subsec:config-training} auf kleinere Werte konnte die Häufigkeit der Skalierungsfehler minimiert werden, so dass mit dieser Einstellung ein Training von bis zu 3 Epochen möglich war.\\

Während des Trainings traten immer wieder Speicherfehler auf, die anzeigten, dass der gesamte Speicher der Grafikkarte belegt war, jedoch mehr benötigt wurde.
Diese Fehler waren unabhängig vom Grafikkartentyp und der Konfiguration des Trainings.
Erst die Minimierung der Batchgröße auf 1 pro \ac{gpu}, die Aktivierung des CPU-Offloads und die Nutzung von \SI{256}{\giga\byte} CPU \ac{ram} ermöglichte ein erfolgreiches Training des Modells Llama 2 7B.
Dieser Speicherbedarf ist unerwartet hoch und kann nicht durch die Größe des Modells erklärt werden.

\section{Generierung von Antworten}\label{sec:generierung}
Die Generierung von Antworten auf die erstellten Evaluationsdaten erfolgte auf drei verschiedene Arten.
Für die Textgenerierung durch das GPT4-Modell wurde die offizielle Webseite der OpenAI Foundation verwendet\footnote{\url{https://chat.openai.com/} abgerufen am 19.8.2023}.
Die Verwendung des untrainierten Llama-Modells sowie der trainierten Modelle mit V100-Grafikkarten erfolgte mit Hilfe eines eigens erstellten Skripts.
Dieses Skript wurde größtenteils für die Nutzung der mit A30-Grafikkarten trainierten Modelle verwendet, musste jedoch modifiziert werden, da die Generierung ausschließlich auf einer \ac{gpu} erfolgt, die A30-Grafikkarten jedoch mit \SI{24}{\giga\byte} \ac{ram} das Modell nicht vollständig unterstützen konnten.
Aus diesem Grund wurde das Modell im 8-Bit-Modus geladen. Dieser spezielle Modus reduziert alle Gewichte des Modells auf 8-Bit-Floatwerte, was zu einer geringeren Genauigkeit, aber auch zu einem geringeren Speicherbedarf führt. Eigentlich sollte das 7B Modell mit \SI{14}{\giga\byte} Speicherbedarf ohne Datentyptransformierung mit einer A30-Grafikkarte genutzt werden können, jedoch wurde in diesem Aufbau das Speicherlimit der Grafikkarte erreicht. Ein Grund dafür ist nicht bekannt.\\

\subsection{Erstellung des Evaluierungsdatensatzes}\label{subsec:eval-dataset}
Vor der Generierung der Antworten mussten die in den jeweiligen Klausuren gestellten Fragen aus \cref{sec:approach:questions} transformiert werden.
Dazu wurden diese Fragen um notwendigen Kontext ergänzt, wenn dieser in der Frage angesprochen wurde oder auf den Ergebnissen vorhergehender Fragen aufbaute.
Zusätzlich wurden alle Fragen in die englische Sprache übersetzt, da das Llama-Modell fast ausschließlich auf englischen Texten trainiert wurde und das verwendete Buch \citet{bb} ebenfalls in englischer Sprache verfasst ist.
Jeder Frage wurden 6 Eigenschaften zugeordnet.
\begin{itemize}
    \item \textbf{question}: Die ursprüngliche Frage aus der Quelle
    \item \textbf{transformed}: Die umformulierte und eventuell übersetzte Frage
    \item \textbf{true\_answer}: Die erwartete richtige Antwort
    \item \textbf{num\_answers}: Die Anzahl der in der erwarteten richtigen Antwort enthaltenen Antworten
    \item \textbf{source}: Abkürzung für die Quelle der Frage
    \item \textbf{context}: Ein optionaler Kontext, der zur Beantwortung der Frage benötigt wird
\end{itemize}

Mit Hilfe dieser Struktur wurden die Fragen in eine weitere explizite Frageform umgewandelt.
Wie bereits unter \cref{sec:approach:questions} beschrieben, wird das Llama-Modell nicht mittels Reinforcement Learning auf die Beantwortung von Fragen trainiert.
Aus diesem Grund wurden Anweisung, Frage und Kontext in einem Textblock zusammengefasst.
Die Struktur ist in \cref{fig:llama-input} dargestellt.
Eine Referenz zum Evaluationsdatensatz befindet sich im Anhang.\\

\begin{figure}

    \begin{lstlisting}[linewidth=13cm]
        Instruction: You are given a question and a context. Answer the question to your best knowledge.
        Question: <transformed>
        Context: <context>
        Answer:
    \end{lstlisting}
    \caption{Struktur einer Eingabe für das Llama-Modell}\label{fig:llama-input}
\end{figure}

Die generierten Fragen wurden vor der Verwendung in Dateien in die drei Fragetypen \enquote{single}, \enquote{multi} und \enquote{transfer} unterteilt.
Zusätzlich wurden alle Fragen in einen zweiten Datensatz kopiert, wobei die \enquote{transformed} Fragen mit kleineren Grammatik- und Rechtschreibfehlern versehen wurden.
Dieser zweite Datensatz wird in gleicher Weise im folgenden Schritt unter \cref{sec:answer-rating} zur Bewertung des Kriteriums \enquote{Robustheit} in der Auswertung verwendet.

\section{Bewertung der generierten Antworten}\label{sec:answer-rating}
Die Bewertung der generierten Antworten erfolgte manuell.
Die von den Modellen generierten Ausgaben wurden wie in \cref{subsec:eval-dataset} strukturiert. Drei zusätzliche Eigenschaften wurden hinzugefügt.
\begin{itemize}
    \item \textbf{generated}: Stellt die generierte Antwort des Modells dar
    \item \textbf{type}: Gibt den Typ der Frage an
    \item \textbf{true\_input}: Enthält die tatsächlich verwendete Eingabe
\end{itemize}

Bei der Auswertung wurde zunächst beurteilt, ob die generierte Antwort einen inhaltlichen Bezug zur Frage hat.
War dies nicht der Fall, wurde diese Frage als \enquote{unbeantwortet} gewertet und enthielt auch keine Antwort.
Wenn die Antwort einen inhaltlichen Bezug zur Frage hatte, wurde die Antwort hinsichtlich ihrer Korrektheit bewertet.
Dazu gab die Bewertung eine Anzahl richtiger Antworten in der generierten Antwort an, die zwischen 0 und der erwarteten Anzahl richtiger Antworten liegt.
Schließlich wurde die Anzahl der gegebenen Antworten in der generierten Antwort angegeben.\\

Bei der Evaluation stellte sich schnell heraus, dass die Beurteilung der Llama-Modelle schwieriger ist als die des GPT4-Modells.
Der Grund dafür ist das fehlende Training auf ein Stoppzeichen.
Die Llama-Modelle enthalten kein Finetuning für die Beantwortung der Fragen und erzeugen eine vorgegebene Anzahl von Tokens.
Diese Anzahl wurde auf 512 gesetzt.
In den meisten Fällen wurde die Frage jedoch mit einer kürzeren Länge beantwortet.
Die verbleibenden Tokens enthielten Wiederholungen bereits generierter Tokens, Formatierungsfehler (z. B. nicht vorhandene Überschriften), den Beginn neuer Themen oder die Imitation neuer Fragen.
Da die Ursache in einem fehlenden Finetuning und nicht in der inhaltlichen Wiedergabe des Modells lag, wurde die Antwort nur bis zu diesen Stellen gewertet.
Keine der generierten Antworten benötigte eine Länge von mehr als 512 Tokens, weshalb eine Erweiterung hier nicht weiter verfolgt wurde.
Die Chat-Version des Modells Llama 2 enthält ein solches Finetuning und würde dieses Problem beheben.
Auf den Einsatz der Chat-Version wurde in dieser Arbeit jedoch verzichtet, da das mit Hilfe des Finetunings zusätzlich erlernte Wissen durch das hier durchgeführte Continual Pretraining wieder verlernt werden würde.\\

Sowohl die Antwortdateien der Modelle als auch die Bewertungsdateien folgen dem gleichen Schema wie die Eingabedatei aus \cref{subsec:eval-dataset}. Durch die Einführung der Eigenschaft \enquote{type} ist eine Unterteilung der Dateien in Fragetypen nicht mehr notwendig, daher gibt es 4 Dateien pro Modell.
Zwei Dateien, die die generierten Antworten auf die Fragen enthalten (für jeden Datensatz, wobei der zweite Datensatz die selben Fragen mit Rechtschreibfehlern repräsentiert) und zwei Dateien, die die Bewertungen der generierten Antworten enthalten.
Die Bewertungen wurden mit 3 zusätzlichen Eigenschaften versehen:

\begin{itemize}
    \item \textbf{answered}: Zeigt an, ob die Frage richtig, falsch oder gar nicht beantwortet wurde
    \item \textbf{points}: Gibt die Anzahl der richtigen Antworten in der generierten Antwort an
    \item \textbf{total\_answers}: Zeigt die Anzahl der gegebenen Antworten in der generierten Antwort an
\end{itemize}

\section{Evaluation der Modelle}\label{sec:evaluation}
Die Evaluierung der Modelle wurde für jedes ausgewählte Kriterium aus \cref{sec:approach:comparison} durchgeführt.
Für jedes Kriterium wurde ein Skript geschrieben, welcher die Bewertungsdateien der Modelle einliest und die Ergebnisse grafisch darstellt.
Das Kriterium \enquote{Determinismus} wurde nicht bewertet.
Die Autoren des Artikels \citet{chatgpt_qas} beschreiben ein inhärentes, nicht-deterministisches Verhalten bei der Verwendung von ChatGPT.
Dieses ist unter anderem auf den eingebauten Parameter \enquote{temperature} bei der Textgenerierung zurückzuführen.
Die Temperatur beschreibt die Varianz der Ausgabewerte.
Je höher die Temperatur, desto größer ist die Varianz der Ausgabevektoren.
Eine höhere Varianz führt zu einer größeren Vielfalt des generierten Textes und zu weniger sich wiederholenden Textpassagen.
Dieser Temperaturwert wurde für die Generierung auf $0,9$ gesetzt.
Das beschriebene nicht-deterministische Verhalten konnte bei der Textgenerierung mit den Llama-Modellen nicht beobachtet werden.
Mehrfache identische Eingaben führten bei allen Modellen zu identischen Ausgaben.
Da sich somit ein konstanter Wert von $1,0$ für den Determinismus ergibt, wurde dieser hier nicht weiter ausgewertet.
Alle Kriterien wurden zusätzlich für jeden Fragetyp und jede Fragequelle ausgewertet.
Die Ergebnisse dieser Auswertung sind in \cref{ch:results} beschrieben.\\

\subsection{Kriterium: Korrektheit}
Für die Analyse der Korrektheit wurden alle Bewertungen der Fragen pro Modell, Fragetyp und Fragequelle gezählt.
Mit diesen Werten wurde dann der MakroF1-Wert nach den Formeln aus \cref{sec:approach:comparison} berechnet.
Die Bewertungen ergeben sich aus dem Vergleich der Musterlösung mit den generierten Antworten.
Die Musterlösungen sind im Buch angegeben oder wurden von den Autoren der Klausuren erstellt.
Die Zählung der richtigen Antworten erfolgte somit weitestgehend objektiv.

\subsection{Kriterium: Erklärbarkeit}
Eine Antwort eines Modells auf eine Frage gilt als erklärt, wenn sie inhaltlich mit der Frage zusammenhängt und eine Erklärung oder nähere Beschreibung enthält.
Alle Ausgaben der Modelle wurden erneut manuell auf das Vorhandensein einer Erklärung im Rahmen dieses Kriteriums bewertet.
Ein Beispiel für eine Antwort mit und ohne Erklärung ist in \cref{tab:explanation} dargestellt.
\begin{table}
    \begin{tabularx}{\textwidth}{p{2.8cm}p{3cm}X}
        \toprule
        \textbf{Erklärung} & \textbf{Frage} & \textbf{Generierte Antwort} \\
        \midrule
        enthalten & Is openEHR a standard? & OpenEHR (Sect. 3.7.2.2) is an open standard specification which focuses on the representation of clinical concepts in detailed clinical models.\newline
        It has been maintained by the openEHR Foundation since the early 20s and supports the creation of clinical models in the form of a centralized database. $[\dots]$ \\
        nicht enthalten & What types of integrity exist in the context of health information systems? & - data integrity,\newline
         - semantic integration,\newline 
         - user interface integration,\newline 
         - context integration,\newline
         - feature integration,\newline 
         - process integration.$[\dots]$ \\
        \bottomrule
    \end{tabularx}
    \caption[Beispiele der Erklärbarkeit]{Ein Beispiel für Antworten mit Erklärung und ohne Erklärung}\label{tab:explanation}
\end{table}

\subsection{Kriterium: Fragenverständnis}
Das Kriterium Fragenverständnis bezieht sich auf die Fähigkeit eines Modells, die gestellten Fragen zu verstehen.
Eine Frage gilt als verstanden, wenn sich die generierte Antwort inhaltlich auf die Frage bezieht und darüber hinaus alle Aspekte der Frage adressiert.
Die Bewertung des Fragenverständnisses erfolgte analog zur Bewertung der Erklärbarkeit.

\subsection{Kriterium: Robustheit}
Die Robustheit eines Modells beschreibt die Fähigkeit, auch bei fehlerhaften Eingaben eine korrekte Antwort zu generieren.
Zu diesem Zweck wurde, wie bereits erwähnt, während der Generierung der Ausgaben ein zusätzlicher Datensatz ausgewertet, der die selben Fragen wie der ursprüngliche Datensatz enthielt, jedoch mit kleineren Rechtschreib- und Grammatikfehlern versehen war.
Da sich die Robustheitsbewertung auf richtig beantwortete Fragen bezieht, die auch bei falscher Eingabe richtig beantwortet werden, wurde dieser Datensatz für jedes Modell reduziert und enthielt nur Fragen, die das Modell zuvor mit mindestens einer richtigen Antwort beantwortet hatte.
Die Berechnung der Ergebnisse erfolgt analog zur Berechnung des Kriteriums Korrektheit, wobei hier nicht die Anzahl der im Robustheitsdatensatz enthaltenen Fragen, sondern die Anzahl aller vorhandenen Fragen für die Berechnung des MakroF1-Wertes verwendet wurde.
