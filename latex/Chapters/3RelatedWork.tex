%*****************************************
\chapter{Stand der Forschung}\label{ch:relatedWork}
%*****************************************

Paper Reihenfolge:

Basiswissen - Architektur

Attention is All You Need - Transformer Aufbau
    Basiswissen zu Transformern
    Aufbau

AMMUS
    Einführung der Taxonomie
Deep Residual Learning for Image Recognition
    Referenz zum Aufbau von Residual Connections
Dropout
    Referenz zur Trainings-Herangehensweise

Basiswissen - Training

ChatGPT versus Traditional Question Answering Systems for Knowledge Graphs
    Referenz auf aktuelles Ziel
    Umsetzung nachempfinden, jedoch verbesserung auf basis von Continual Pretraining
Improving Language Understanding by Generative Pre-Training
    Pretraining als Language Understanding Task
Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
    Weiterführung von Pretraining als erfolgsversprechende Methode
Fine-Tuning Language Model from Human Preferences
    Weiterführung von großen Modellen
    um optimale QA Agents zu erstellen
Scaling Laws for Neural Language Models
    Auswahl von Modellen basierend auf besseren Resultaten

Vorstellung - Modelle
Language Models are Few-Shot Learners
    Vorstellung von GPT-3 als Erfolgsmodell
    Hinweis auf FewShot und ZeroShot Performance
GPT4-Technical Report
    Vorstellung von GPT-4 als Zukunftsmusik
GPT-NeoX-20B
    Vorstellung von GPT-NeoX als Open-Source Alternative
LLaMa
    Vorstellung als ALternative zu großen Modellen
    mit selber Performance

Blick in die Forschung - Weiterentwicklungen

AdapterHub  
    Weiterentwicklung von Modellen ohne Scratch Learning
    Potentielle Alternative zu Pretraining
Knowledge Neurons
    Untersuchung des Verständnis von Modellen von Sprache

Probleme mit Modellen

GPT-4 System Card
    Ethische und andere Probleme mit GPT4
Plagiarism in the age of massive generative pre-trained transformers
    Weitere Probleme aufzeigen durch Generative AIs

