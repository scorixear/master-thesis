%*****************************************
\chapter{Diskussion}\label{ch:discussion}
%*****************************************

* LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale https://arxiv.org/abs/2208.07339
* The case for 4-bit precision: k-bit Inference Scaling Laws https://arxiv.org/abs/2212.09720

Inferenz nur möglich, verringert RAM Footprint

- Größere Modelle möglicherweise bessere Leistung
- Chat Modelle nicht möglich - verlernt eher Chat Funktion (Quelle im Stand der Forschung)
- Einsatz Adapter als Alternative zur Kontexterweiterung
    - Lora Adapter Training auf 1 GPU möglich, verspricht gute Leistung aber
    deutlich höherer Trainingsaufwand bei der Programmierung
    - Limittierter Einsatz von Adaptern da Llama nicht für Adapter gemacht --> Modellumbau notwendig
- Bessere Ergebnisse, wenn auf Domäne trainiert wird, nicht auf einzelnes Buch
- Domänspezifische Modelle als Grundlage anstelle General Models (Galatica, BioBert)

- Nutzung von Modellen mit Kontext anstelle von Continual Pretraining
- Nutzung von BERT Based Modellen um Textpassagen aus Kontext zu extrahieren

- Bewertung von Fragen anhand Klausurpunkten


https://news.ycombinator.com/item?id=36832572
Warum Retrieval Augmented Generation nicht gut ist 
https://arxiv.org/abs/2307.03172

