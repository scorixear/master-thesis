%*****************************************
\chapter{Diskussion}\label{ch:discussion}
%*****************************************

* LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale https://arxiv.org/abs/2208.07339
* The case for 4-bit precision: k-bit Inference Scaling Laws https://arxiv.org/abs/2212.09720

Inferenz nur möglich, verringert RAM Footprint

- Größere Modelle möglicherweise bessere Leistung
- Chat Modelle nicht möglich - verlernt eher Chat Funktion (Quelle im Stand der Forschung)
- Einsatz Adapter als Alternative zur Kontexterweiterung
- Bessere Ergebnisse, wenn auf Domäne trainiert wird, nicht auf einzelnes Buch
- Domänspezifische Modelle als Grundlage anstelle General Models (Galatica, BioBert)
- 

- Nutzung von Modellen mit Kontext anstelle von Continual Pretraining
- Nutzung von BERT Based Modellen um Textpassagen aus Kontext zu extrahieren

- Bewertung von Fragen anhand Klausurpunkten