%*****************************************
\chapter{Ergebnisse}\label{ch:results}
%*****************************************
\newcommand{\gpt}{\texttt{gpt$4$}}
\newcommand{\lo}{\texttt{llama$2$\_$0$e}}
\newcommand{\liv}{\texttt{llama$2$\_$1$e\_v$100$}}
\newcommand{\lia}{\texttt{llama$2$\_$1$e\_a$30$}}
\newcommand{\lev}{\texttt{llama$2$\_$3$e\_v$100$}}
\newcommand{\lea}{\texttt{llama$2$\_$3$e\_a$30$}}
\newcommand{\lsa}{\texttt{llama$2$\_$5$e\_a$30$}}
\newcommand{\lioa}{\texttt{llama$2$\_$10$e\_a$30$}}

\newcommand{\pic}[4][1]{
    \begin{figure}
        \makebox[\textwidth][c]{\includegraphics[width=#1\textwidth]{#2}}
        \caption{#3}\label{#4}
    \end{figure}
}
\newcommand{\pich}[4][1]{
    \begin{figure}[H]
        \makebox[\textwidth][c]{\includegraphics[width=#1\textwidth]{#2}}
        \caption{#3}\label{#4}
    \end{figure}
}

In dieser Arbeit wurde die Konzeption und Implementierung eines kontinuierlichen Pre-Trainings von Llama-Modellen beschrieben.
Diese Modelle wurden anschließend nach den in \cref{sec:approach:comparison} genannten Kriterien Korrektheit, Erklärbarkeit, Fragenverständnis und Robustheit verglichen.
Die Durchführung dieses Vergleichs ist in \cref{sec:evaluation} beschrieben.
In diesem Kapitel werden die Ergebnisse dargestellt und analysiert, wobei auch hier eine Unterteilung nach den Kriterien erfolgt.
Die zu vergleichenden Modelle sind in \cref{tab:eval-models} aufgeführt.

\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Modell} & \textbf{Epochen} & \textbf{Grafikkarten} & \textbf{Bezeichnung} \\
        \midrule
        GPT4            & -                & -                     & \gpt{}               \\
        Llama 2 7B      & 0                & Nvidia Tesla A100     & \lo{}                \\
                        & 1                & Nvidia Tesla V100     & \liv{}               \\
                        & 1                & Nvidia Tesla A30      & \lia{}               \\
                        & 3                & Nvidia Tesla V100     & \lev{}               \\
                        & 3                & Nvidia Tesla A30      & \lea{}               \\
                        & 5                & Nvidia Tesla A30      & \lsa{}               \\
                        & 10               & Nvidia Tesla A30      & \lioa{}              \\
        \bottomrule
    \end{tabular}
    \caption[Evaluierte Modelle]{Evaluierte Modelle, deren Anzahl an Epochen, genutzte Grafikkarten und Bezeichnungen in den Grafiken.}\label{tab:eval-models}
\end{table}

\section{Analyse Korrektheit}\label{sec:results:correctness}
\subsection{Vergleich totaler Zahlen}
\pich{results/answers_total.png}{Vergleich evaluierter Modelle und ihren totalen Leistungen bei der Beantwortung von Fragen.}{fig:results:answers_total}

\cref{fig:results:answers_total} zeigt die Verteilung der richtigen, falschen und unbeantworteten Fragen der evaluierten Modelle.
Allen Modellen wurden dieselben $95$ Fragen mit identischem Kontext vorgelegt.
Diese absoluten Zahlen zeigen eine grobe Tendenz der Ergebnisse, spiegeln aber nicht die tatsächlichen Leistungen wider.
Eine Frage gilt als richtig beantwortet, wenn mindestens eine richtige Antwort gegeben wurde.
Dies führt zu irreführend guten Ergebnissen.
Dennoch ist deutlich zu erkennen, dass GPT4 den Llama-Modellen deutlich überlegen ist.
Das untrainierte Llama-Modell beantwortete \SI{28}{\percent} der Fragen mit mindestens einer richtigen Antwort, konnte aber auch einen Großteil der Fragen beantworten und lieferte bei \SI{11}{\percent} keine Antwort. \\

Das \liv-Modell liefert mit \SI{2}{\percent} die wenigsten korrekten Antworten und zeigt ein generelles Verlernen grundlegender sprachlicher Fähigkeiten.
Im Gegensatz dazu zeigt das \lia-Modell eine vergleichbare, wenn auch etwas schlechtere Leistung als das untrainierte Modell.\\

Mit dem \lev-Modell konnte eine signifikante Leistungssteigerung im Vergleich zu einer Epoche erzielt werden, die Gesamtleistung ist jedoch dauerhaft schlechter als beim untrainierten Modell.
Das \lea-Modell hingegen übertraf erstmals die Leistung des untrainierten Modells und beantwortete \SI{43}{\percent} der Fragen mit mindestens einer richtigen Antwort.\\

Modelle mit höherer Epoche wurden ausschließlich auf A30-Grafikkarten trainiert und zeigten keine weitere Leistungssteigerung, beantworteten aber kontinuierlich weniger Fragen.
Hier zeigt sich eine Präferenz für keine Antwort gegenüber einer falschen Antwort.\\

\subsection{Stärken und Schwächen der Modelle}\label{subsec:results:correctness:strengths}
Neben der Gesamtleistung der Modelle wurde auch eine Unterteilung nach Fragetypen und Fragequellen vorgenommen.
Ähnliche Grafiken wie in \cref{fig:results:answers_total} sind in den ergänzenden Materialien zu dieser Arbeit enthalten.

\begin{table}
    \centering
    \begin{tabular}{lll}
        \toprule
        \textbf{Fragetyp} & \textbf{Anzahl der Fragen} & \textbf{Bezeichnung} \\
        \midrule
        Singulär-Fakt     & 34                         & \enquote{single}     \\
        Multi-Fakten      & 38                         & \enquote{multi}      \\
        Transfer          & 23                         & \enquote{transfer}   \\
        \bottomrule
    \end{tabular}
    \caption{Fragetypen die im Evaluierungsdatensatz enthalten sind.}\label{tab:eval-question-types}
\end{table}


Die verschiedenen Fragetypen sind in \cref{tab:eval-question-types} aufgelistet.
In den folgenden Grafiken werden die entsprechenden Bezeichnungen zur Beschriftung der Achsen verwendet.
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{Xll}
        \toprule
        \textbf{Fragequelle}                                                                                                                               & \textbf{Anzahl der Fragen} & \textbf{Bezeichnung}       \\
        \midrule
        \enquote{Health Information Systems} von \citet{bb}                                                                                                & 33                         & \enquote{book}             \\
        \midrule
        Mündliche Klausurfragen aus dem Modul \enquote{Architektur von Informationssystemen im Gesundheitswesen} vom Jahr 2021                             & 22                         & \enquote{A\_2021}          \\
        \midrule
        Schriftliche Klausurfragen aus dem Modul \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022                     & 9                          & \enquote{IS\_2022\_07\_18} \\
        \midrule
        Schriftliche Klausurfragen aus der Nachholklausur des Moduls \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022 & 31                         & \enquote{IS\_2022\_09\_27} \\
        \bottomrule
    \end{tabularx}
    \caption{Fragequellen die im Evaluierungsdatensatz enthalten sind.}\label{tab:eval-question-sources}
\end{table}


Unterschiedliche Fragequellen sind in \cref{tab:eval-question-sources} aufgelistet.
Da die Quelle \enquote{IS\_2022\_07\_18} nur 9 Fragen enthält, ist eine Bewertung der Leistung in dieser Rubrik ungenau und anfällig für kleine Änderungen.
Aus diesem Grund erfolgt der Vergleich der Modellleistungen unabhängig von dieser Fragenquelle.\\


Das GPT4-Modell hat keine Präferenz für bestimmte Fragetypen, beantwortete aber die Fragen aus dem Buch mit \SI{93}{\percent} korrekten Antworten signifikant besser.
Vergleichsweise wenige Antworten wurden bei den beiden schriftlichen Prüfungen gegeben.\\


Das untrainierte Llama-Modell zeigte mit \SI{39}{\percent} richtigen Antworten eine deutliche Stärke bei Multi-Fakten-Fragen und konnte \SI{27}{\percent} der mündlichen Klausurfragen beantworten.
Die schlechtesten Kategorien ergaben sich bei den Ein-Fakten-Fragen mit \SI{20}{\percent} und den Fragen aus dem Buch mit \SI{24}{\percent} richtigen Antworten.\\


Während das \liv-Modell mit nur 2 richtig beantworteten Fragen grundsätzlich schlecht abschnitt,
zeigte das \lia-Modell mit \SI{36}{\percent} eine deutlich bessere Leistung bei Fragen mit mehreren Fakten
und bei der mündlichen Prüfung mit \SI{40}{\percent} richtig beantworteten Fragen.
Dagegen verlor dieses Modell deutlich bei Ein-Fakten-Fragen mit \SI{5}{\percent} und Klausurfragen mit nur \SI{12}{\percent} richtigen Antworten.\\\


Modelle, die auf 3 Epochen trainiert wurden, zeigen einen deutlichen Leistungssprung gegenüber einer Epoche.
Sowohl das \lev-Modell als auch das \lea-Modell verbessern ihre Leistung in den zuvor als schwach identifizierten Fragetypen und Fragequellen.
Im Falle des \lea-Modells bedeutet dies eine Verdoppelung der richtig beantworteten Transferfragen sowie eine Verdoppelung der richtig beantworteten Fragen aus dem Buch.
Diese Leistungssteigerung zeigt sich auch in den MakroF1-Werten.\\

Epoche 5 und Epoche 10 enthalten nur Modelle, die mit Nvidia A30 Grafikkarten trainiert wurden.
Die gezeigten Leistungen unterscheiden sich kaum von denen des \lea-Modells und weisen nur minimale Leistungsschwankungen auf.
Allerdings verschiebt sich die Verteilung der richtig beantworteten Fragen hin zu Fragen aus dem Buch.
Die anderen Fragequellen werden von den Modellen mit zunehmender Epoche immer schlechter beantwortet, während die Fragen aus dem Buch immer besser beantwortet werden.
Diese Verschiebung deutet auf eine Überanpassung hin, da die Fragen aus dem Buch auch in den Trainingsdaten enthalten sind.\\

\subsection{Verbesserungen durch Training}
\pic{results/loss.png}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Trainingsdatensatzes.}{fig:results:loss}
\pic{results/validation_loss.png}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Validierungsdatensatzes.}{fig:results:validation_loss}
Während der Trainingsphase wurden die Modelle mit Hilfe eines Validierungsdatensatzes evaluiert.
Diese zusätzliche Evaluierung berechnete den Fehlerwert des Modells auf einem gegebenen Datensatz.
\cref{fig:results:loss} zeigt die Fehlerwerte der Trainingsdaten und \cref{fig:results:validation_loss} die Fehlerwerte der Validierungsdaten für jedes Modell.
Fehlerwerte beschreiben nicht konkret die Leistung eines Modells, sondern sind ein Indikator für die Verbesserung der Leistung und den Beginn der Überanpassung.
Ein abnehmender Fehlerwert des Trainingsdatensatzes bedeutet, dass das Modell den Trainingsdatensatz besser nachahmen kann, während ein abnehmender Fehlerwert des Validierungsdatensatzes zeigt, dass das Modell diese Nachahmung generalisierend auf ungesehenen Text anwenden kann.

Wenn der Fehlerwert des Validierungsdatensatzes zu steigen beginnt, deutet dies auf eine Überanpassung hin.
Das Modell verliert dann die Fähigkeit, ungesehenen Text zu imitieren und beginnt stattdessen, den Trainingsdatensatz auswendig zu lernen.
Dieses Phänomen ist bei beiden Modelltypen bereits ab Epoche 3 zu beobachten und setzt sich konstant bis Epoche 10 fort.\\

Für die Beantwortung der Fragen ist das Auswendiglernen jedoch in gewissem Maße vorteilhaft, da Definitionen von Begriffen oder Aufzählungen von Fakten vom Modell besser zitiert werden können.
Dies spiegelt sich auch in den Ergebnissen wider.
Modelle der Epoche 3 können Fragen aus allen Fragequellen und Fragetypen besser beantworten, da hier die Zitierfähigkeit des Modells erhöht wird.
Ein zu hoher Grad an Überanpassung führt jedoch zu anderen Problemen.
Treten unbekannte Formulierungen wie z.B. Rechtschreibfehler auf, kann das Modell diese nicht mehr korrekt beantworten.
Diese Problematik nimmt mit steigender Epoche zu, weshalb Modelle ab Epoche 5 in ihrer Leistungsfähigkeit nachlassen.\\

\subsection{Vergleich MakroF1}
\pic{results/makro_total.png}{Vergleich evaluierter Modelle und Makro F1 Werten bei der Beantwortung von Fragen.}{fig:results:makro_total}

\cref{fig:results:makro_total} zeigt die MakroF1 Werte der evaluierten Modelle.
Auch hier sind die Tendenzen aus \cref{subsec:results:correctness:strengths} zu erkennen.
GPT4 erreicht einen MakroF1 Wert von \num{0.7} und ist damit deutlich besser als die Llama Modelle.
Während Modelle trainiert auf 1 Epoche noch unter der Leistung des untrainierten Llama-Modells liegen, verdoppeln sich die MakroF1 Werte der Modell ab 3 Epochen.
Fortführend bleiben die MakroF1 Werte der Modelle mit 5 und 10 Epochen konstant.\\

Der MakroF1 Wert hat einen Wertebereich von \numrange{0}{1} und repräsentiert besser die tatsächliche Leistung des Modells.
Modelle die Fragen zwar richtig beantwortet haben, jedoch auch falsche Antworten geben oder unvollständig antworten, erhalten hier einen geringeren F1-Wert.
Ein Modell mit einem MakroF1 Wert von \num{0} beantwortet keine Fragen richtig, während ein Modell mit einem Wert von \num{1} alle Fragen richtig und vollständig beantwortet.
Somit beantwortet das GPT4-Modell im Durchschnitt eine Frage zu \SI{70}{\percent} korrekt, während das \lea-Modell im Durchschnitt eine Frage zu \SI{30}{\percent} korrekt beantwortet.\\

\pic[0.7]{results/makrof1_total_type_heat.png}{Heatmap der Makro F1 Werte der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:makrof1_total_type_heat}

Eine genauere Aufteilung der MakroF1 Werte nach Fragetyp  ist in \cref{fig:results:makrof1_total_type_heat} zu sehen.
Hier sieht man die Leistungssteigerung in allen Fragetypen ab 3 Epochen.
Nicht intuitiv beantworten die Modelle Transfer-Fragen besser als Singulär-Fakt Fragen, obwohl diese Fragen deutlich einfacher für den Menschen sind.
Grund hierfür könnte eine begünstigte Grundstruktur der Modelle sein.
Durch den inhärenten Aufbau mit Hilfe einer Aufmerksamkeitsmaske sind Transformermodelle darauf spezialisiert, Texte und Token miteinander zu korrelieren und könnten daher besser verschiedene Textstellen aus dem Buch und Fakten miteinander verknüpfen.
Diese Beobachtung lässt sich jedoch nicht auf das GPT4 Modell übertragen.
Da der technische Hintergrund des GPT4 Modells nicht bekannt ist, lässt sich diese Beobachtung nicht weiter begründen.
Das GPT4 Modell wurde nach normalem Training durch viel Finetuning mit menschlichem Feedback auf die Beantwortung von Fragen angepasst, wodurch die Leistung bei der Beantwortung von einfachen Singulär-Fakt Fragen hier durchaus gesteigert werden könnte.\\

\pic{results/makrof1_total_source_heat.png}{Heatmap der Makro F1 Werte der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:makrof1_total_source_heat}

\cref{fig:results:makrof1_total_source_heat} zeigt die MakroF1 Werte der Modelle unterteilt nach Fragequellen.
Hier lässt sich die Entwicklung der Modelle von einem Untrainierten Zustand über einen wissenden Zustand zu einem überangepassten Zustand gut zeigen.
Da die Fragequelle \enquote{IS\_2022\_07\_18} nur aus \num{9} Fragen besteht, ist eine generelle Aussage hier nicht möglich.
Kleinere Fluktuationen wie zum Beispiel das korrekte Beantworten eine Frage führen zu zu hohen Ausschlägen, welche missverständlich als eine sehr gute Leistung interpretiert werden könnten.\\

Bis Epoche 3 ist eine konstante Leistungssteigerung in allen Fragequellen zu erkennen, wobei Fragen aus der Quelle \enquote{A\_2021} am meisten davon profitieren.
Dies ist zu erwarten, da das dahinter stehende Modul inhaltlich sehr nah an dem Buch \enquote{Health Information Systems} ist.
Ab Epoche 5 ist eine Verschiebung in Richtung der Fragequelle \enquote{book} zu erkennen, was auf ein Overfitting hinweist.
Die Modelle können ab diesem Punkt Fragen aus dem Buch durch zitieren der enthaltenen Antworten besser beantworten, während andere Formulierungen des selben Wissens zu Unverständnis führt.
Trotzdem ist auch diese gesteigerte Leistung ein guter Fortschritt und zeigt die Stärken der Informationsextraktion von Transformermodellen aus Text.
Die im Buch gegebenen Fragen werden nicht unmittelbar nach der gestellten Frage beantwortet, sondern erst am Ende des Buches.
Das bedeutet, das die Transformermodelle hier eine klare Verknüpfung zwischen Frage und Antwort herstellen konnten, trotz einem großen Abstand.\\

\subsection{Zusammenfassung}
Die MakroF1 Werte bestätigen zuvor gemachte Aussagen der totalen Anzahl der Fragen über die Leistung der Modelle.
GPT4 erreicht hier mit \num{0.7} den höchsten MakroF1 wert, welches vergleichbar mit Ergebnissen aus \citet{gpt4} zum Thema \enquote{Medical Knowledge} ist.
Das Llama 2 7B startet im untrainierten Zustand deutlich unter normaler Leistung.
Seine Leistung kann durch ein Continual Pretraining ab Epoche 3 um das doppelte gesteigert werden.
Eine Epoche, also das einmalige Lesen des Buches, scheint nicht auszureichen, um eine bessere Leistung zu erreichen.
Dies ist durchaus auch durch die Größe des Trainingsdatensatzes begründet.
Da dieser im Verhältnis klein ist, kann das Modell seine Gewichte nicht schnell genug anpassen, um die enthaltenen Informationen zu replizieren.
Eine größere Lernrate würde ein schnelleres Anpassen der Modell an gesehen Text verbessern, jedoch steigt dadurch die Gefahr
dass das Modell sich zu sehr an Formulierungen und Formatierungen anpasst und somit nicht nutzbar wird.\\

Ab Epoche 5 verliert das Modell an Leistung, obwohl die gezeigten MakroF1 Werte konstant bleiben.
Das Modell beginnt ab diesem Punkt, Generalisierung zu verlernen und kann nur gesehene Fragen aus dem Buch mit zitierten Antworten richtig beantworten.
Aus diesem Grund scheint das Modell seine besten Leistungen ab Epoche 3 erreicht zu haben.
Hier erhält das Modell besonders gute MakroF1-Werte
durch eine übermäßig gute Leistung in Fragen aus der Fragequelle \enquote{A\_2021}.\\

GPT4 ist durch seine Größe und die dahinter stehende Menge an Training unschlagbar.
Das dieses Modell die Leistungen einen 7-Milliarden Parameter Modells übertrifft ist zu erwarten.
Doch die Leistungssteigerung des Llama-Modells zeigt eine Tendenz, die durch größere Modelle durchaus noch weiter gesteigert werden könnte.
Zu diesem Zeitpunkt sind jedoch die trainierten Modelle aus dieser Arbeit nicht nutzbar.
Besonders in Epoche 1 enthalten Antworten grundlegende Fehler in der Ausgabe durch Imitation von Formatierungen (siehe \cref{fig:formatting-errors}).
Weitertrainierte Modelle zeigen eine Präferenz von keiner Antwort gegenüber einer falschen Antwort,
welches durchaus vorteilhaft bei der Benutzung des Modells sein kann.
Jedoch begründet sich diese Tendenz durch die Überanpassung an den Trainingsdatensatz.\\

\begin{figure}
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        Frage              & Definieren Sie den Begriff \enquote{Krankenhausinformationssystem}.                                                                                                                                                                                                    \\
        Übersetzt          & Define the term ``hospital information system''.                                                                                                                                                                                                                       \\
        Erwartete Antwort  & A Hospital Information system is the socio-technical subsystem of hospitals. It comprimises all data, information, and knowledge processing as well as the associated human or technical actors in their respective data, information, and knowledge processing roles. \\
        Generierte Antwort & \#\#\# 2.2.2.2.2.2.1.1.1.1.1.1.2.1.1.2.1.2.1.2.1.2.1.2.1. $[\dots]$                                                                                                                                                                                                    \\
        \bottomrule                                                                                                                                                                                                                                                                                 \\
    \end{tabularx}
    \caption{Beispiel für Formatierungsfehler in der Ausgabe des Llama 2 7B Modells trainiert auf 1 Epoche}\label{fig:formatting-errors}
\end{figure}

Modelle die mit Hilfe von V100 Grafikkarten trainiert wurden zeigen eine sehr große Verschlechterung der Leistung.
Begründet ist diese durch das notwendige Training im FP16-Datenformat gegenüber dem ursprünglich genutzten BF16-Datenformat.
Wie auch von Huggingface beschrieben, sollten vortrainierte Modelle weiter im vorgesehenen Datenformat trainiert werden.
Diese Beobachtung bestätigt sich auch in den Ergebnissen dieser Arbeit.\\


\section{Analyse Erklärbarkeit}\label{sec:results:explainability}
\pic{results/explained.png}{Vergleich evaluierter Modelle und ihrer Anzahl an Antworten, welche Erklärungen enthalten.}{fig:results:explained}
Die hier evaluierten Modelle verfügen über eine hohe Fähigkeit, zu den generierten Antworten auch Erklärungen zu geben.
Insbeondere GPT4 schafft es, richtig beantwortete Fragen zu \SI{97}{\percent} mit einer Erklärung zu versehen.
\cref{fig:results:explained} zeigt hier die totale Anzahl an Antworten, welche Erklärungen enthalten.
Betrachtet wurden dabei nur Fragen, die das jeweilige Modelle zuvor mit mindestens einer richtigen Antwort beantwortet hat.
Das untrainierte Modell als auch Modelle trainiert auf einer Epoche können hierbei nur zur Hälfte der Fragen eine Erklärung liefern.
Ab Epoche 3 steigt diese Leistung ebenso deutlich auf \SI{85}{\percent} an.\\

\pich[0.7]{results/explainability_total_type.png}{Heatmap der Erklärbarkeit der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:explainability_total_type}
\pic{results/explainability_total_source.png}{Heatmap der Erklärbarkeit der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:explainability_total_source}

\cref{fig:results:explainability_total_type} und \cref{fig:results:explainability_total_source} zeigen die Unterteilung dieser Leistung nach Fragetypen und Fragequellen.
Initial ist hier keine klare Entwicklung der Verbesserung in diesen Unterteilungen zu erkennen.
Inbesondere wenig Erklärungen enthalten Fragen aus dem Fragetyp \enquote{Transfer} während Fragen aus dem Fragetyp \enquote{Multi-Fakten} am meisten Erklärungen enthalten.
Die zuvor gut erscheinenden Leistungen bei Fragen aus der Fragequelle \enquote{A\_2021} zeigen hier, dass die generierten Antworten zwar korrekt sind, jedoch größtenteils keine Erklärungen enthalten.
Herausstechend sind die grundlegend guten Leistungen von GPT4, welche durchaus durch das genutzt Finetuning mit Menschlicher Bewertung (engl.
\enquote{Human Reinforcement Learning}) begründet werden können.
Mit Hilfe dieser Trainingsmethode könnte auch das Llama-Modell belohnt werden, wenn es Erklärungen zu seinen Antworten gibt.\\


\section{Analyse Fragenverständnis}\label{sec:results:questionunderstanding}
\pic{results/understood.png}{Vergleich evaluierter Modelle und ihrer Anzahl an verstandenen Fragen.}{fig:results:understood}
Das Kriterium Fragenverständnis ist nicht äquivalent zu der Anzahl beantworteter Fragen aus dem Kriterium Korrektheit.
Eine Frage gilt nicht als verstanden, wenn sie beantwortet wurde, da falsche Antworten zwar falsche Fakten enthalten können, jedoch die Frage nicht vollständig beantwortet wurde und somit als nicht verstanden gezählt wird.
Ebenso kann eine unbeantwortete Frage auch als verstanden gezählt werden.
So zum Beispiel generierte GPT4 in einigen Antwort, in der erklärt wurde, warum das Modell die Frage nicht beantworten konnte, hatte damit jedoch die Frage eindeutig verstanden.\\

\cref{fig:results:understood} zeigt die totalen Zahl der verstandenen Fragen der evaluierten Modelle.
Anders als bei dem Kriterium Erklärbarkeit wurden hier alle vorhanden Fragen zusätzlich auf das Fragenverständnis geprüft.
Auch hier ist GPT4 mit \SI{97}{\percent} verstandenen Fragen deutlich besser als die Llama-Modelle.
Alleinig das \lea-Modell erreichte eine etwas bessere Leistung von \SI{72}{\percent} verstanden Fragen gegenüber dem untrainierten Modell mit \SI{64}{\percent}.
Insbesondere Modelle mit höherer Epoche verlieren die Fähigkeit Fragen zu verstehen.
Diese Beobachtung belegt damit auch die steigende Anzahl unbeantworteter Fragen aus dem Kriterium Korrektheit, da zunehmend mehr Fragen nicht verstanden werden.
Grund hierfür ist die steigende Überanpassung der Modelle, welche es den Modellen erschwert, andere Formulierung gleichen Wissens zu verstehen.\\

\pic[0.7]{results/question_understanding_total_type.png}{Heatmap des Fragenverständnisses der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:question_understanding_total_type}
\pic{results/question_understanding_total_source.png}{Heatmap des Fragenverständnisses der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:question_understanding_total_source}

\cref{fig:results:question_understanding_total_type} und \cref{fig:results:question_understanding_total_source} zeigen die Unterteilung dieser Statistik nach Fragetypen und Fragequellen.
Auch hier ist deutlich eine Verschlechterung der Leistung ab Epoche 5 zu erkennen, insbesondere bei dem Fragetyp \enquote{transfer}. Das \lea-Modell zeigt hier die besten Leistungen von allen Llama-Modellen, wobei es dennoch in der Kategorie \enquote{multi} schlechter als das untrainierte Modell ausgefallen ist.
Bei der Unterteilung in Fragequellen bestätigen sich diese Beobachtungen, wobei die Quelle \enquote{IS\_2022\_09\_27} besonders beeinflusst durch die Überanpassung der Modelle ab Epoche 5 ist.
Fragen dieser Quelle scheinen grundlegend andere Formulierungen gegenüber dem Buch zu benutzen und müssen daher vermehrt durch generalisiertes Wissen beantwortet werden.

\section{Analyse Robustheit}\label{sec:results:robustness}
\pic{results/makro_comparison.png}{Vergleich evaluierter Modelle und ihrere MakroF1 Werte nach der Einführung von Rechtschreibfehlern.}{fig:results:makro_comparison}
\cref{fig:results:makro_comparison} zeigt die MakroF1 Werte der Modelle nach der Einführung von Rechtschreibfehlern.
Hierfür wurden alle richtig beantworteten Fragen der Modelle mit zusätzlichen Rechtschreibfehlern versehen und erneut evaluiert.
Rechtschreibfehler in Fachbegriffen erschweren die Faktenwiedergabe, da Tokenisierung die Wörter anders unterteilt.
Werden Fakten direkt mit einer bestimmten Tokenisierung verknüpft, kann das Modell diese Fakten bei einer anderen Unterteilung nicht mehr wiedergeben.
Aus diesem Grund ist eine schlechtere Leistung der Modelle zu erwarten und zeigt die Fähigkeit zur Generalisierung des gelesenen Wissens.\\

Das GPT4 Modell gibt auch hier die besten Leistungen mit einem MakroF1 Wert von \num{0.59} gegenüber ursprünglichem \num{0.7} ab.
Dies ist äquivalent zu einer Leistungsminimierung von \SI{16}{\percent}. Trainierte Modelle zeigen deutlich höhere Leistungsminimierungen von rund \SI{50}{\percent}.
Diese großen Unterschiede deuten auf eine oberflächliche Wissensaneignung mit Fokus auf korrekt geschriebenen Fachbegriffen hin.
Durch wenig Trainingsdaten können die Modelle das dort enthalte Wissen nur grob erlernen.
Die Leistung kann hier durchaus verbessert werden, in dem der Trainingsdatensatz erweitert wird, um mehr Formulierungsarten des Wissens zu enthalten.
Dies würde die Generalisierung der Modelle verbessern und somit auch die Leistung in diesem Kriterium.\\

Das untrainierte Llama-Modell erreicht leicht bessere Ergebnisse mit eingebauten Rechtschreibfehlern.
Diese Verbesserung wird auf Grund der kleinen Änderung als zufällig angesehen und bedeutet nicht, dass das Modell bessere Ergebnisse liefert, wenn Fehler enthalten sind.
Besser erklärt sind hier ungefähr gleiche Werte.
Die Llama-Modelle wurden initial auf sehr großen Datensätzen trainiert welche auch die Größe der Datensätze für GPT4 übersteigen.
Das erlernte Wissen dahinter ist daher fundierter und kann deutlich besser generalisiert auf verschiedene Formulierungen angewendet werden.
Durch das hier durchgeführte Continual Pretraining werden diese fundierten Wissensknoten überschrieben und durch oberflächliche Wissensaneignung ersetzt.
Dadurch kann das Modell zwar initial mehr Wissen wiedergeben, verliert jedoch die Fähigkeit, dieses Wissen mit unterschiedlichen Formulierungen zu verbinden.\\

\section{Zusammenfassung}\label{sec:results:summary}

\begin{table}
    \centering
    \resizebox{1.2\textwidth}{!}{
        \begin{tabular}{lllll}
            \toprule
            \textbf{Modell} & \textbf{MakroF1}                   & \textbf{Erklärbarkeit}                      & \textbf{Fragenverständnis}                  & \textbf{Robustheit (Leistungsverlust)}       \\
            \midrule
            GPT4            & \textbf{\num[detect-weight]{0.7}}  & \textbf{\SI[detect-weight]{97.3}{\percent}} & \textbf{\SI[detect-weight]{97.9}{\percent}} & \SI{15.7}{\percent}                          \\
            \lo{}           & \num{0.13}                         & \SI{48.1}{\percent}                         & \SI{64.2}{\percent}                         & \textbf{\SI[detect-weight]{-0.23}{\percent}} \\
            \midrule
            \liv{}          & \num{0.01}                         & \SI{0}{\percent}                            & \SI{2.1}{\percent}                          & \SI{100}{\percent}                           \\
            \lia{}          & \num{0.11}                         & \SI{50}{\percent}                           & \SI{54.7}{\percent}                         & \SI{54.5}{\percent}                          \\
            \lev{}          & \num{0.1}                          & \SI{47.4}{\percent}                         & \SI{40}{\percent}                           & \SI{60}{\percent}                            \\
            \lea{}          & \num{0.3}                          & \textbf{\SI[detect-weight]{85.4}{\percent}} & \textbf{\SI[detect-weight]{72.6}{\percent}} & \SI{46.7}{\percent}                          \\
            \lsa{}          & \textbf{\num[detect-weight]{0.33}} & \SI{82.5}{\percent}                         & \SI{0.6}{\percent}                          & \SI{60.6}{\percent}                          \\
            \lioa{}         & \num{0.32}                         & \SI{81}{\percent}                           & \SI{55.8}{\percent}                         & \textbf{\SI[detect-weight]{43.8}{\percent}}  \\
        \end{tabular}}
    \caption[Zusammenfassung der Ergebnisse der Evaluierung]{Zusammenfassung der Ergebnisse der Evaluierung. Fett markierte Werte repräsentieren die beste Leistung in dem Kriterium. Unterteilt sind die Zeilen nach nur evaluierte Modelle und trainiert und evaluierte Modelle. }\label{tab:results:summary}
\end{table}

\cref{tab:results:summary} zeigt eine Zusammenfassung der Ergebnisse der Evaluierung.
Hierbei sind die Modelle in die Gruppe \enquote{nur evaluiert} und \enquote{trainiert und evaluiert} unterteilt.
Generell übertrifft GPT4 in allen Kriterien die Llama-Modelle außer im Leistungsverlust bei dem Kriterium Robustheit.
Von den Llama-Modellen erreicht \lsa{} die besten MakroF1-Leistungen, \lioa{} den kleinsten Leistungsverlust im Kriterium Robustheit und \lea{} die besten Leistungen in den Kriterien Erklärbarkeit und Fragenverständnis.
Unabhängig von dem Robustheitskriterium scheint ein optimales Modell mit 3 bis 5 Epochen trainiert zu sein.
Aus den aktuell trainierten Modellen ist bei genauerer Betrachtung der Kriterien das \lea-Modell am besten geeignet, um Fragen zu beantworten, da ab 5 Epochen die Überanpassung der Modelle zu starken Einfluss hat.\\

Im Bezug auf Erklärbarkeit und Fragenveständnis lässt sich eine deutliche Verbesserung ab Epoche 3 zeigen.
Der Leistungsverlust durch Rechtschreibfehler zeigt jedoch, dass das Wissen nicht fundiert erlernt wurde und die Modelle anfällig für andere Formulierungen sind.\\

Mögliche Leistungssteigerungen können durch größere Modelle erreicht werden.
Die Llama2 Modelle sind zusätzlich mit 13 Milliarden und 70 Milliarden Parametern verfügbar.
Größere Modelle können komplexes Wissen besser erlernen und besser generalisierend anwenden.
Dies könnte dazu führen, dass ein längeres Training ohne eine Überanpassung möglich ist.
Zusätzlich hilft ein größerer Datensatz insbesondere um die Überanpassung zu verhindern, Informationen in verschiedenen Formulierungen darzustellen und somit die Robustheit zu verbessern.
Ebenso ist eine generelle Steigerung der Ergebnisse im Kriterium Korrektheit zu erwarten.
Als letzte Möglichkeit kann ein Training mit Hilfe von Human Reinforcement Learning durchgeführt werden.
Dieses ermöglicht das Belohnen von Erklärungen und dem Verständnis von Fragen und steigert somit auch diese beiden Kriterien.
