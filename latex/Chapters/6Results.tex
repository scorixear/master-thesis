%*****************************************
\chapter{Ergebnisse}\label{ch:results}
%*****************************************
\newcommand{\gpt}{\texttt{gpt$4$}}
\newcommand{\lo}{\texttt{llama$2$\_$0$e}}
\newcommand{\liv}{\texttt{llama$2$\_$1$e\_v$100$}}
\newcommand{\lia}{\texttt{llama$2$\_$1$e\_a$30$}}
\newcommand{\lev}{\texttt{llama$2$\_$3$e\_v$100$}}
\newcommand{\lea}{\texttt{llama$2$\_$3$e\_a$30$}}
\newcommand{\lsa}{\texttt{llama$2$\_$5$e\_a$30$}}
\newcommand{\lioa}{\texttt{llama$2$\_$10$e\_a$30$}}

\newcommand{\pic}[4][1]{
    \begin{figure}
        \makebox[\textwidth][c]{\includegraphics[width=#1\textwidth]{#2}}
        \caption{#3}
        \label{#4}
    \end{figure}
}

In dieser Arbeit wurde die Konzeptionierung und Durchführung eines Continual Pretraining von Llama Modellen beschrieben.
Diese Modelle wurden anschließend nach den in \cref{sec:approach:comparison} genannten Kriterien Korrektheit, Erklärbarkeit, Fragenverständnis und Robustheit verglichen.
Die Durchführung dieses Vergleichs ist in \cref{sec:evaluation} gezeigt.
In diesem Kapitel werden die Ergebnisse vorgestellt und analysiert, wobei auch hier eine Unterteilung nach den Kriterien vorgenommen wird.
Zu vergleichende Modelle sind in \cref{tab:eval-models} gezeigt.

\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Modell} & \textbf{Epochen} & \textbf{Grafikkarten} & \textbf{Bezeichnung} \\
        \midrule
        GPT4            & -                & -                     & \gpt                 \\
        Llama 2 7B      & 0                & Nvidia Tesla A100     & \lo                  \\
                        & 1                & Nvidia Tesla V100     & \liv                 \\
                        & 1                & Nvidia Tesla A30      & \lia                 \\
                        & 3                & Nvidia Tesla V100     & \lev                 \\
                        & 3                & Nvidia Tesla A30      & \lea                 \\
                        & 5                & Nvidia Tesla A30      & \lsa                 \\
                        & 10               & Nvidia Tesla A30      & \lioa                \\
        \bottomrule
    \end{tabular}
    \caption[Evaluierte Modelle]{Evaluierte Modelle, deren Anzahl an Epochen, genutzte Grafikkarten und Bezeichnungen in den Grafiken.}\label{tab:eval-models}
\end{table}

\section{Analyse Korrektheit}\label{sec:results:correctness}
\subsection{Vergleich totaler Zahlen}
\pic{results/answers_total.png}{Vergleich evaluierter Modelle und ihren totalen Leistungen bei der Beantwortung von Fragen.}{fig:results:answers_total}

\cref{fig:results:answers_total} zeigt die Aufteilung der korrekt, falsch und unbeantworteten Fragen der evaluierten Modelle.
Alle Modelle erhielten die selben $95$ Fragen mit identischem Kontext.
Diese absoluten Zahlen zeigen eine grobe Tendenz der Leistungen, widerspiegeln jedoch die wahren Leistungen nicht.
Eine Frage gilt als richtig beantwortet, wenn mindestens eine richtige Antwort gegeben wurde.
Dies führt zu missverständlich guten Ergebnissen.
Dennoch ist klar zu erkennen, dass GPT4 einen deutlichen Vorsprung gegenüber den Llama Modellen hat.
Das untrainierte Llama Modell beantwortete \SI{28}{\percent} der Fragen mit mindestens einer korrekten Antwort, konnte jedoch ebenso einen Großteil der Fragen beanworten und liefert bei \SI{11}{\percent} keine Antwort.\\

Das \liv-Modell liefert mit \SI{2}{\percent} die wenigsten korrekten Antworten und zeigt ein generelles Verlernen grundlegender sprachlicher Fähigkeiten.
Im Gegenzug zeigt das \lia-Modell eine vergleichbare, wenn auch etwas schlechtere Leistung als das untrainierte Modell.\\

Mit dem \lev-Modell konnte eine deutliche Leistungssteigerung gegenüber einer Epoche erreicht werden, jedoch ist die Gesamtleistung bleibend schlechter als das untrainierte Modell.
Das \lea-Modell übertraf dafür erstmalig die Leistungen des untrainierten Modells und beantwortete \SI{43}{\percent} der Fragen mit mindestens einer korrekten Antwort.\\

Modelle mit höherer Epoche wurden ausschließlich auf A30 Grafikkarten trainiert und zeigten keine weitere Leistungssteigerung, beantworteten jedoch stetig weniger Fragen.
Hier ist eine Präferenz von keiner Antwort gegenüber einer falschen Antwort zu erkennen.\\

\subsection{Stärken und Schwächen der Modelle}
Zusätzlich zur Gesamtleistung der Modelle fand auch eine Unterteilung in Fragetypen und Fragequellen statt.
Ähnliche Grafiken wie aus \cref{fig:results:answers_total} sind im zusätzlichen Material zu dieser Arbeit enthalten.
Unterschiedene Fragetypen sind Singulär-Fakt Fragen (bezeichnet als \enquote{single}), Multi-Fakten Fragen (bezeichnet als \enquote{multi}) und Transfer-Fragen (bezeichnet als \enquote{transfer}).
Unterschiedene Fragequellen sind das Buch \enquote{Health Information Systems} von \citet{bb} (bezeichnet als \enquote{book}),
mündliche Klausurfragen aus dem Modul \enquote{Architektur von Informationssystemen im Gesundheitswesen} vom Jahr 2021 (bezeichnet als \enquote{A\_2021}),
schriftliche Klausurfragen aus dem Modul \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022 (bezeichnet als \enquote{IS\_2022\_07\_18})
und schriftliche Klausurfragen aus der Nachholklausur des Moduls \enquote{Informationssysteme in medizinischer Versorgung und Forschung} vom Jahr 2022 (bezeichnet als \enquote{IS\_2022\_09\_27}).\\

Das GPT4 Modell besitzt keine Präferenz gegenüber Fragetypen, beantwortete jedoch Fragen aus dem Buch deutlich besser mit \SI{93}{\percent} korrekten Antworten.
Vergleichsweise wenige Antworten wurden bei beiden schriftliche Klausuren gegeben, wobei die Klausur \enquote{IS\_2022\_07\_18} besonders wenig Fragen enthielt und daher keine genauen Angaben möglich sind.\\

Das untrainierte Llama Modell zeigte eine eindeutige Stärke bei Multi-Fakten Fragen mit \SI{39}{\percent} richtigen Antworten und konnte \SI{27}{\percent} der mündlichen Klausurfragen beantworten.
Die schlechteste Kategorien zeigten sich als Singulär-Fakt Fragen mit \SI{20}{\percent} und Fragen aus dem Buch mit \SI{24}{\percent} korrekten Antworten.\\

Während das \liv-Modell grundlegend schlechte Ergebnisse erzielte mit nur 2 korrekt beantworteten Fragen,
zeigte das \lia-Modell eine deutlich bessere Leistung bei Multi-Fakten Fragen mit \SI{36}{\percent}
und Fragen der mündlichen Klausur mit \SI{40}{\percent} korrekt beantworteter Fragen.
Dafür verlor dieses Modell deutlich bei Singulär-Fakt Fragen mit \SI{5}{\percent} und schriftlichen Klausurfragen mit nur korrekten \SI{12}{\percent} korrekten Antworten.\\

Modelle trainiert auf 3 Epochen zeigen einen deutlichen Leistungssprung gegenüber einer Epoche.
Sowohl das \lev-Modell als auch das \lea-Modell verbessern ihre Leistung in Fragetyp und Fragequelle, die zuvor als schwach erkannt wurden.
Im Falle des \lea-Modells bedeutet dies eine Verdoppelung richtig beantworteter Transfer-Fragen als auch eine Verdoppelung richtig beantworteter Fragen aus dem Buch.
Diese Leistungssteigerung ist auch in den MakroF1 Werten zu erkennen.\\

Epoche 5 und Epoche 10 enthalten nur Modelle, die mit Hilfe von Nvidia A30 Grafikkarten trainiert wurde.
Die gezeigten Leistungen verändern sich kaum gegenüber dem \lea-Modell und beinhalten nur minimale Leistungsschwankungen.
Jedoch verschiebt sich die Aufteilung korrekt beantworteter Fragen in Richtung Fragen aus dem Buch.
Andere Fragequellen können von den Modellen mit steigender Epoche zunehmend schlechter beantwortet werden, während Fragen aus dem Buch zunehmend besser beantwortet werden.
Diese Verschiebung deutet auf ein Overfitting hin, da die Fragen aus dem Buch ebenso in den Trainingsdaten enthalten sind.\\

\subsection{Verbesserungen durch Training}
\pic{results/loss.png}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Trainingsdatensatzes.}{fig:results:loss}
\pic{results/validation_loss.png}{Vergleich evaluierter Modelle und ihrer Fehlerwerte des Validierungsdatensatzes.}{fig:results:validation_loss}
Während der Trainingsphase wurden Modelle mit Hilfe eines Valdierungsdatensatzes evaluiert.
Diese zusätzliche Evaluation berechnete den Fehlerwert des Modells auf einem gegebenen Datensatz.
\cref{fig:results:loss} zeigt die Fehlerwerte der Trainingsdaten und \cref{fig:results:validation_loss} die Fehlerwerte der Validierungsdaten für jedes Modell.
Fehlerwerte beschreiben nicht konkret die Leistung eines Modells, sondern sind ein Indiz über deren Leistungssteigerung und Beginn des Overfittings.
Ein sinkender Fehlerwert des Trainingsdatensatzes bedeutet, dass das Modell besser den Trainingsdatensatz imitieren kann, während ein sinkender Validierungsdatensatz zeigt, dass das Modell diese Imitation generalisierend auf ungesehenen Text anwenden kann.\\

Beginnt der Fehlerwert des Validierungsdatensatzes zu steigen, so ist dies ein Indiz für ein Overfitting.
Das Modell verliert demnach die Fähigkeit, ungesehenen Text zu imitieren und beginnt stattdessen den Trainingsdatensatz auswendig zu lernen.
Dieses Phänomen ist bereits ab Epoche 3 bei beiden Modellarten zu erkennen und setzt sich konstant bis Epoche 10 fort.\\

Zur Beantwortung von Fragen ist das Auswendiglernen des Buches jedoch zu einem gewissen Grad vorteilhaft, da Definitionen von Begriffen oder Aufzählungen von Fakten so besser von dem Modell zitiert werden können.
Dies ist auch in den Ergebnissen zu erkennen.
Modelle der Epoche 3 können Fragen aus allen Fragequellen und Fragetypen besser beantworten, da hier die Zitierfähigkeit des Modells gesteigert wird.
Ein zu hoher Grad des Overfittings führt jedoch zu anderen Problemen.
Kommen ungesehene Formulierungen, wie zum Beispiel Rechtschreibfehler, vor, so kann das Modell diese nicht mehr korrekt beantworten.
Diese Problematik steigert sich mit steigender Epoche, weshalb Modelle ab Epoche 5 in ihrer Leistung sinken.\\

\subsection{Vergleich MakroF1}
\pic{results/makro_total.png}{Vergleich evaluierter Modelle und Makro F1 Werten bei der Beantwortung von Fragen.}{fig:results:makro_total}
\pic{results/makrof1_total_type_heat.png}{Heatmap der Makro F1 Werte der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:makrof1_total_type_heat}
\pic{results/makrof1_total_source_heat.png}{Heatmap der Makro F1 Werte der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:makrof1_total_source_heat}
\subsection{Zusammenfassung}
\section{Analyse Erklärbarkeit}\label{sec:results:explainability}
\pic{results/explained.png}{Vergleich evaluierter Modelle und ihrer Anzahl an Antworten, welche Erklärungen enthalten.}{fig:results:explained}
\pic{results/explainability_total_type.png}{Heatmap der Erklärbarkeit der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:explainability_total_type}
\pic{results/explainability_total_source.png}{Heatmap der Erklärbarkeit der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:explainability_total_source}
\section{Analyse Fragenverständnis}\label{sec:results:questionunderstanding}
\pic{results/understood.png}{Vergleich evaluierter Modelle und ihrer Anzahl an verstandenen Fragen.}{fig:results:understood}
\pic{results/question_understanding_total_type.png}{Heatmap des Fragenverständnisses der evaluierten Modelle unterteilt nach Fragetypen.}{fig:results:question_understanding_total_type}
\pic{results/question_understanding_total_source.png}{Heatmap des Fragenverständnisses der evaluierten Modelle unterteilt nach Fragequellen.}{fig:results:question_understanding_total_source}
\section{Analyse Robustheit}\label{sec:results:robustness}
\pic{results/makro_comparison.png}{Vergleich evaluierter Modelle und ihrere MakroF1 Werte nach der Einführung von Rechtschreibfehlern.}{fig:results:makro_comparison}
\section{Zusammenfassung}\label{sec:results:summary}

% Vergleich LLaMA mit GPT4 durchgängig bessere Ergebnisse bei allen Fragenarten
% Vergleich LLaMA v100 1 Epoche mit anderen Modellen durchgängig schlechtere Ergebnisse bei allen Fragenarten
% Vergleich LLaMA a30 1 mit LLaMA 0:
% - minimal Verbesserung transfer
% - Vergleichbar Multi
% - Schlechter Single
% - generell mehr Fragen unbeantwortet
% - Besser bei A_2021
% - Schlechter bei IS_2022_09_27, IS_2022_07_18 und Book
% - Deutlich mehr unbeantwortet bei IS_2022_09_27 und IS_2022_07_18

% Vergleich LLaMA v100 3 mit LLaMA 0:
% - Minimale Verbesserung bei Transfer Fragen
% - Wesentlich mehr unbeantwortete Fragen bei Singulär
% - Wesentlich mehr unbeantwortete Fragen bei Multi-Fakt
% - Vergleichbar bei IS_2022_09_27 und Book
% - Schlechter bei A_2021 und IS_2022_07_18
% - A2021, IS_2022_09_27 wesentlich mehr unbeantwortet

% Vergleich LLaMA a30 3 mit LLaMA 0:
% - Verbesserung bei Transfer und Single
% - kleine Verbesserung bei Multi
% - Alle Fragen bei transfer verstanden
% - Leicht weniger Fragen beantwotet bei single und Multi
% - Verbesserung Book und A2021
% - Leichte Verbesserung bei IS_2022_09_27
% - Verschlechterung bei IS_2022_07_18
% - Mehr beantwortet bei Book
% - Weniger beantwortet bei A_2021

% Vergleich a30 3 mit a30 1:
% - Verbesserung bei allen Typen
% - Verbesserung bei allen Sources außer IS_2022_07_18
% - mehr Fragen beantwotet bei allen Typen
% - Mehr fragen beantwotet bei allen Sources außer A_2021

% Vergleich a30 3 mit v100 3:
% - Verbesserung bei allen Typen
% - mehr Fragen beantwortet bei allen Typen
% - Verbesserung bei allen Sources
% - Mehr Fragen beantwortet bei allen Sources außer IS_2022_07_18

% Vergleich a30 5 mit Llama 0:
% - Verbesserung bei allen Typen
% - Weniger Fragen Beantwotet bei allen Typen
% - Verbesserung bei A_2021 und Book
% - Verschlechterung bei IS_2022_09_27 und IS_2022_07_18
% - Mehr fragen beantwortet bei Book
% - Weniger Fragen beantwortet bei allen Sources außer Book

% Vergleich a30 5 mit a30 3:
% - Verbesserung bei Multi
% - Verschlechterung bei Single, Transfer
% - Weniger Fragen beantwortet bei allen Typen
% - Verbesserung bei Book
% - Verschlechterung bei allen Sources außer Book
% - Weniger Fragen beantwotet bei allen Sources außer Book

% Vergleich a30 10 mit Llama 0:
% - Verbesserung bei allen Typen
% - Weniger Fragen beantwortet bei allen Typen
% - Verbesserung bei A_2021 und Book
% - Verschlechterung bei IS_2022_07_18
% - Weniger Fragen beantwortet bei allen Sources

% Vergleich a30 10 mit a30 3:
% - Vergleichbar bei allen Typen
% - Mehr Fragen beantwortet bei Multi
% - Weniger Fragen beantwortet bei Transfer
% - Besser bei Book
% - Schlechter bei IS_2022_09_27, IS_2022_07_18
% - Weniger Fragen beantwortet bei allen Sources außer A_2021

% Vergleich a30 10 mit a30 5:
% - Besser bei single
% - Mehr bei Multi
% - Weniger bei Transfer, Single
% - Besser bei IS_2022_09_27
% - Mehr bei A2021
% - Weniger bei allen außer A2021

% Ranking - Richtig Beantwortet:
% - GPT4 immer besser
% - 10e, 5e, 3e a30 besten Ergebnisse
% - v100 1e schlechteste Ergebnisse

% Ranking Beantwortet:
% - GPT4, llama0, llama 3e
% - v100 1e schlechteste Ergebnisse

% Ranking MakroF1:
% - GPT4 besstes
% - A30 3e, 10e und 5e sehr gut
% - 5e und 10e durch Book
% - v100 1e schlechteste Ergebnisse


% Analyse der Korrektheit
% - Betrachtung der MakroF1 Werte bestätigt totale Anzahl der Fragen Ergebnisse
% - GPT4 erreicht 0.7 MakroF1, vergleichbar mit ERgebnissen aus GPT4 Paper (Medical Knowledge)
% - LLaMA auch im untrainierten Zustand deutlich unter normaler Leistung
% - Doppelte Leistung von untrainiert durch 5e, 10e und 3e erreicht
% - 1e vergleichbar mit 0e
% - insbesondere Transfer-Fragen besser
% - synthetische gute Ergebnisse bei 5e, 10e durch overfitting auf Buchfragen
% - 3e erreicht gute Ergebnisse bei A2021 -> kann dadurch mithalten

% Begründung der Ergebnisse
% - GPT4 durch Größe unschlagbar
% - hier nur mit LLaMA 7B verglichen, zu erwarten bei größere Modelle deutlich bessere Leistung
% - Verlauf des Trainings einschätzbar: 1 Epoche deutlich zu wenig, so gut wie keine Leistungssteigerung
% - temporäres Overfitting an Formattierungsarten (Überschriften, Aufzählungen)
%     hier Beispiel von LLaMA 1 einfügen
% - Overfitting durch identische Antwortimitierung bei 5e und 10e
%     hier Beispiel 10e einfügen
% - Maximale Leistung bei 3e/5e erreicht, 10e keine Verbesserung
% - generell sehr wenig Trainigsdaten (600kB), Gefahr des overfittings nach 3 Epochen existent
% - weitertrainieren von v100 nicht möglich, beschrieben in Kapitel 5
% - V100 FP16 training führt zu deutlich schlechteren Leistungen
% - LLaMA erlernt keine falschen Fakten, deutlich mehr Fragen unbeantwortet, während LLaMA 0 mehr Fragen falsch beantwortet
%     - steigert sich mit steigender Epoche -> anzeichen für Overfitting

% - Tatsächliche Steigerung bei Transfer-Fragen
% Ergebnisse enthalten jedoch gravierende Formattierungs- und Wiederholungsfehler

% Modelle im aktuellen Zustand nicht nutzbar, da kein Endtoken für Antworten vorhanden
%     hier Beispiel von LLaMA 5e einfügen


% Analyse Erklärbarkeit
% - GPT 4 auch hier unschlagbar, nahezu jede Antwort enthält Erklärung, 97%
% - LLaMA 0, V100 3, a30 1 unbeeinflusst bei 50% der Fragen
% - LLama 3e, 5e, 10e deutlich besser 85% der Fragen
% - Ergebnisse unabhängig von Source oder Type
% - Verbesserung durch Human Reinforcement Training möglich, um Erklärungen zu belohnen (nicht existent im Trainingsdatensatz)

% Analyse Fragenverständnis
% - Fragenverständnis nicht äquivalent zu "Fragen beantwortet"
% - Fragenverständis bei LLaMA 0 nur durch a30 3 geschlagen (64%, 72%)
% - Ansonsten starker abfall, V100 3 nur noch bei 40%
% - GPT4 versteht so gut wie jede Frage (97%)
% - Multi Fragen bessere Leistung von Llama0 (81%), Andere Modelle bei 63%, begründet durch Leistungsabfall bei A_2021 Fragen
% - IS Fragen zeigen deutliche Leistungssteigerung und abnahme (zu wenig vs zu viel trainiert)


% Analyse Robustheit
% - Rechtschreibfehler führen zu falschen Antworten
% - LLaMA 0 profitiert etwas durch Fehler, erhöhter MakroF1 Wert (vollständigere Antworten)
% - Auch GPT4 schwierigkeiten bei Transfer
% - Andere Modelle Leistungsabfall um ungefähr 50%

% - Rechtschreibfehler in Fachbegriffen erschweren Faktenwiedergabe, da Tokenization Wörter anders
% unterteilt (teilweise auch in einzelne Buchstaben)
% - schlechtere Leistungen zu erwarten gegenüber ohne Fehler
% - zeigt generelles Verständnis von Fakten unabhängig von Textrepräsentation
% - LLaMA 0 enthaltenes Wissen durch größere Datenmengen fundierter
% - LLaMA 3e, 5e, 10e durch einzelnens Buch (wenig Daten) erst oberflächliche Wissensaneignung
% - scheint teile des LLaMA 0 Wissens ersetzt / versteckt zu haben

% Gesamtanalyse
% - LLaMA kommt nicht an Leistungen von GPT4
% - Trainingsart entscheident (v100 schlechter als Llama 0)
% - Beste Leistungen zwischen a30 3 Epochen und a30 5 Epochen
% - Ab 5 Epochen Overfitting Status -> Überaus gut bei Buchfragen, andere Fragen schlechter
% - Präferenz von Keiner Antwort gegenüber einer Antwort wird mit steigender Epoche deutlicher

% - Erklärbarkeit und Fragenverständnis ebenso unter GPT4-
%     - deutliche Verbesserung ab a30 3 Epochen erreicht
%     - zeigt wahre Leistung von 3 Epochen gegenüber 5 und 10 Epochen
% - Robustheit zeigt, dass Wissen oberflächlich erlernt wurde, jedoch nicht fundiert verankert ist
%     - Wird nicht besser mit längerem Training
%     - Overfitting wirkt entgegen Robustheit

% Leistungssteigerung möglich durch:
% - Größere Modelle generelle Verbesserung zu erwarten (LLaMA 13B, 33B)
% - Mehr Trainingsdaten
%     - sinkt Gefahr des overfittings
%     - Wissen wird fundierter erlernt, da in unterschiedlicher Formulierung enthalten
% - Leistungssteigerung durch Human Reinforcement Learning
%     - bessere Erklärbarkeit zu erwarten


