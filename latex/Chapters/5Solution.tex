%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

Die Lösung gliedert sich in sechs Schritte, die für die Durchführung des Trainings notwendig sind.
Im Folgenden werden diese Schritte näher erläutert. Für jeden Schritt sind die verwendeten Techniken und Bibliotheken separat aufgeführt.
Die Schritte sind wie folgt gegliedert:
\begin{enumerate}
    \item Laden des Modells
    \item Konvertierung des Modells in ein kompatibles Format
    \item Modelltraining
    \item Erstellung von Antworten auf dem Evaluierungsdatensatz
    \item Bewertung der erzeugten Antworten
    \item Auswertung basierend auf den Bewertungen
\end{enumerate}

\section{Herunterladen des Modells}
Die LLaMA-Modelle werden unter einer nicht-kommerziellen Lizenz für Forschungszwecke zur Verfügung gestellt. Der Zugang zu den Modellen wird im Einzelfall auf Anfrage gewährt. Diese Anfrage wurde im Rahmen dieser Arbeit gestellt und bestätigt.
Anschließend kann ein von den Autoren vorbereitetes Skript verwendet werden, um die trainierten Parameter des Modells herunterzuladen.
Das Skript benötigt eine explizite \ac{url}, die nach einer vorgegebenen Zeit von einer Woche nach Freigabe der Modelle ungültig wird. Aus diesem Grund ist diese \ac{url} nicht im Skript enthalten.

\section{Umwandlung des Modells in ein kompatibles Format}
Das Training des Modells basiert auf zwei grundlegenden Bibliotheken: der Transformers Bibliothek von Huggingface und der DeepSpeed Bibliothek von Microsoft.
Diese Bibliotheken, insbesondere Transformers, erleichtern den Entwicklungsprozess enorm und bieten ein hohes Maß an Abstraktion. Allerdings können Sie nur mit Modellen arbeiten, die in einer für die Transformers-Bibliothek verständlichen Form vorliegen.
Aus diesem Grund wird das heruntergeladene Modell mit Hilfe eines Huggingface-Skripts in diese kompatible Form umgewandelt.
Während dieser Konvertierung muss das Modell vollständig in den \ac{ram} des ausführenden Rechners geladen werden. Für das Modell LLaMA 7B sind dafür mehr als \SI{14}{\giga\byte} \ac{ram} erforderlich.
Die Konvertierung dauerte auf einem Rechner mit \SI{32}{\giga\byte} \ac{ram} und einem AMD Ryzen 7 5800X Prozessor ca. 30 Minuten.

\section{Training des Modells}
Um das LLaMA-Modell zu trainieren, wird die Transformers Bibliothek von Huggingface verwendet\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 16.8.2023}.
Das in Python geschriebene Trainingsskript basiert auf dem Beispielskript zum Training von kausalen Sprachmodellen aus dem Huggingface Github Repository\footnote{\url{https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py} abgerufen am 16.8.2023} und wurde teilweise an die Anforderungen des hier durchgeführten Trainings angepasst.
Die Konfiguration des Trainings gliedert sich in vier Bereiche: Einstellungen für das Modell, Einstellungen für die Trainingsdaten, Einstellungen für das Training selbst und Einstellungen für DeepSpeed.
Im Folgenden werden diese Einstellungen näher erläutert.
\subsection{Konfiguration des Modells}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        model\_name & meta-llama/Llama-2-7b-hf \\
        cache\_dir & \./cache \\
        use\_fast\_tokenizer & false \\
        model\_revision & main \\
        use\_auth\_token & true \\
        hugging\_token & \textit{Huggingface Token} \\
        torch\_dtype & auto \\
        low\_cpu\_mem\_usage & false \\
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration des Modells}\label{tab:model-config}
\end{table}
Die zur Auswahl und Konfiguration des Modells verwendeten Parameter sind in der Tabelle \ref{tab:model-config} aufgelistet.\\

Der Parameter \enquote{model\_name} entspricht einer Modellauswahl und kann entweder ein relativer Pfad zu einem lokalen Modell oder ein Modellname sein.
Der Modellname wurde hier auf das Modell Llama 2 7B gesetzt und verweist auf das von Huggingface gehostete Modell in einem mit der Transformers-Bibliothek kompatiblen Format.
Llama 2 wurde im Juli 2023 von Meta AI veröffentlicht und stellt eine generelle Verbesserung der ursprünglichen LLaMA 1 Modelle dar.
Neben der nun auf 4096 Tokens erweiterten Kontextlänge stellt Meta AI Llama 2 auch in einer Chat-Version zur Verfügung.
Die Chat-Version wurde zusätzlich mit Hilfe von Reinforcement Learning aus menschlichem Feedback trainiert und ermöglicht so eine einfachere Nutzung der vortrainierten Modelle im Kontext eines Chatbots.
Diese Chat-Modelle wurden jedoch nicht zum Training verwendet, da davon ausgegangen wird, dass dieses zusätzliche Training durch das hier durchgeführte Continual Pretraining überschrieben wird.
Außerdem ermöglicht Huggingface nun die Nutzung der Llama 2 Modelle ohne Download und Konvertierung, so dass die ersten beiden Schritte entfallen.
Die Llama 2 Modelle wurden unter anderem im Artikel von \citet{llama2} vorgestellt.\\

Der Parameter \enquote{cache\_dir} beschreibt den Speicherort des heruntergeladenen Modells.
Dies ermöglicht ein wiederholtes Trainieren des Modells ohne erneuten Download.
Die heruntergeladenen Modelle werden durch das Training nicht überschrieben und stellen somit den Grundzustand des Modells dar (im Folgenden auch als Llama2-0e für \enquote{0 Epochen trainiert} bezeichnet).\\

\enquote{use\_fast\_tokenizer} konfiguriert die Verwendung einer schnelleren Version des Tokenizers zur Konvertierung der Datensätze.
Diese Option ist optional und wurde hier nicht verwendet.\\

\enquote{model\_revision} beschreibt die zu verwendende Version des Modells.
Hier wurde die aktuellste Version verwendet, die durch den Wert \enquote{main} repräsentiert wird.\\

\enquote{use\_auth\_token} und \enquote{hugging\_token} beschreiben die Verwendung eines Authentifizierungs-Tokens, um Modelle von Huggingface herunterzuladen.
Diese Authentifizierung ist notwendig, da auch die Llama 2 Modelle unter der gleichen Lizenz wie LLaMA 1 stehen und nur auf Anfrage zur Verfügung gestellt werden.
Um die Modelle mit beschränktem Zugang herunterzuladen, wurde ein Huggingface-Account erstellt, die Anfrage an Meta AI zur Nutzung der Llama 2 Modelle gestellt und bestätigt und ein Authentifizierungstoken generiert.\\

Der Parameter \enquote{torch\_dtype} beschreibt den Datentyp, der für die Darstellung der Modellparameter verwendet wird.
Hier stehen Float32, Float16 und BFloat16 zur Verfügung.
Vorgefertigte Modelle wurden ursprünglich mit einem Datentyp erstellt und müssen in einen anderen Datentyp konvertiert werden, wenn der \enquote{torch\_dtype} nicht übereinstimmt.
Diese Konvertierung ist rechenintensiv und kann dazu führen, dass Modelle fehlerhaft trainiert werden oder mehr Rechenleistung während der Inferenz und des Trainings benötigen.
Aus diesem Grund wurde hier der Parameter \enquote{auto} verwendet, der den Datentyp des Modells automatisch erkennt und benutzt.\\

\enquote{low\_cpu\_mem\_usage} beschreibt ein Verfahren der Bibliothek Transformers zum Laden großer Modelle auf Systemen mit wenig Arbeitsspeicher.
Dabei werden die Modelle in mehreren Schritten in den Arbeitsspeicher geladen und anschließend in den \ac{gpu}-Speicher übertragen.
Dieses Verfahren reduziert die Menge des notwendigen Arbeitsspeichers, erhöht aber die Zeit, die zum Laden des Modells benötigt wird.
Da im vorliegenden Fall genügend Arbeitsspeicher zur Verfügung stand, wurde auf dieses Verfahren verzichtet.

\subsection{Konfiguration der Trainingsdaten}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        train\_file & ./input/health\_information\_systems\_epub.md \\
        max\_train\_samples & None \\
        overwrite\_cache & false \\
        block\_size & 1024 \\
        validation\_split\_percentage & 5 \\
        preprocessing\_num\_workers & 1 \\
        keep\_linebreaks & true \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration der Trainingsdaten}\label{tab:data-config}
\end{table}
Für das Training des Modells Llama 2 wurde das Buch \enquote{Health Information Systems} von \citet{bb} im epub-Format in das Markdown-Format konvertiert.
Die notwendigen Änderungen am Text sind in \cref{sec:datenkuration} beschrieben.
Um diese Markdown-Datei in eine für das Modell verständliche Form umzuwandeln, wird die Bibliothek datasets\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 16.8.2023} von Huggingface verwendet.
Sie ermöglicht das Laden der Textdatei, die Umwandlung in Tokens und die Aufteilung in Blöcke.
Die Parameter zur Auswahl und Konfiguration der Trainingsdaten sind in der Tabelle \ref{tab:data-config} aufgelistet.\\

Der Parameter \enquote{train\_file} beschreibt den Pfad zur Trainingsdatei.
Diese Trainingsdatei wird einmal eingelesen und je nach Anzahl der unter \cref{subsec:config-training} beschriebenen Epochen mehrfach verwendet.\\

\enquote{max\_train\_samples} beschreibt die maximale Anzahl von Blöcken, die aus der Trainingsdatei gelesen werden sollen.
Zu Testzwecken kann hier eine geringere Anzahl an Blöcken verwendet werden, um die Konfiguration des Modells zu testen.
Im finalen Training wurde dieser Parameter auf \enquote{None} gesetzt, um alle Blöcke zu verwenden.\\

Der Parameter \enquote{overwrite\_cache} beschreibt, ob der Text erneut in Tokens umgewandelt werden soll.
Wenn sich der Text geändert hat, kann das Skript hier die nun tokenisierte Textdatei überschreiben.\\

Wie bereits erwähnt, wird der Text in Blöcke aufgeteilt.
Jeder Block wird vom Modell vollständig und gleichzeitig gelesen.
Die maximale Größe eines Blocks ist durch das Modell begrenzt und liegt bei Llama 2 bei 4096 Tokens.
Standardmäßig verwenden Modelle jedoch eine Blockgröße von 1024 Tokens, weshalb hier diese Größe angenommen wird, wenn keine weiteren Angaben gemacht werden.
Größere Blöcke führen zu einer schnelleren Verarbeitung des Textes und können zu einem besseren Verständnis der Zusammenhänge führen, da ein größerer Kontext betrachtet wird.
Allerdings steigt mit der Größe der Blöcke auch die Fehleranfälligkeit, so dass teilweise auch kleinere Blöcke verwendet werden müssen, um ein Training erfolgreich durchzuführen.
Probleme beim Training sind unter \cref{sec:problem-training} beschrieben.
Die Blockgröße kann mit dem Parameter \enquote{block\_size} angepasst werden.\\

Der Parameter \enquote{validation\_split\_percentage} beschreibt den Anteil der Daten, der für die Validierung genutzt werden soll.
Hier werden \SI{5}{\percent} der Daten für die Validierung verwendet.
Die Validierung liefert während des Trainings Informationen über die tatsächliche Leistung des Modells im Vergleich zum ungesehenen Text und dient dazu, den Fortschritt des Modells zu messen.\\

\enquote{preprocessing\_num\_workers} beschreibt die Anzahl der Prozesse, die für die Umwandlung der Textdatei in Tokens verwendet werden sollen.
Bei sehr großen Datenmengen ist die Umwandlung von Textdateien in Tokens eine sehr umfangreiche Aufgabe.
Um diesen Vorgang zu beschleunigen, können mehrere Prozesse die Textdateien parallel übersetzen.
In diesem Fall ist die Datenmenge jedoch klein genug, um die Aufgabe mit nur einem Prozess zu erledigen.\\

Der Parameter \enquote{keep\_linebreaks} beschreibt, ob Zeilenumbrüche in der Textdatei erhalten bleiben sollen.
In einigen Fällen können Textdateien viele Zeilenumbrüche enthalten, die der Formatierung des Textes dienen, aber dem Modell keine Informationen liefern oder es dazu veranlassen, diese Zeilenumbrüche zu imitieren.
Aus diesem Grund können Zeilenumbrüche optional entfernt werden.
Aufgrund der zuvor durchgeführten Datenkuration ist dies jedoch nicht notwendig, weshalb dieser Parameter auf \enquote{true} gesetzt wird.\\

\subsection{Konfiguration des Trainings}\label{subsec:config-training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        output\_dir & ./trained/7B-3 \\
        overwrite\_output\_dir & true \\
        do\_train & true \\
        do\_eval & true \\
        per\_device\_train\_batch\_size & 1 \\
        per\_device\_eval\_batch\_size & 1 \\
        evaluation\_strategy & steps \\
        eval\_steps & 50 \\
        learning\_rate & 3e-4 \\
        weight\_decay & 0.1 \\
        optim & adamw\_torch \\
        adam\_beta1 & 0.9 \\
        adam\_beta2 & 0.95 \\
        adam\_epsilon & 1e-5 \\
        max\_grad\_norm & 1.0 \\
        num\_train\_epochs & 3 \\
        lr\_scheduler\_type & cosine \\
        warmup\_steps & 0 \\
        save\_strategy & steps \\
        save\_steps & 100 \\
        save\_total\_limit & 1 \\
        no\_cuda & false \\
        seed & 42 \\
        fp16 & true | false \\
        bf16 & false | true \\
        half\_precision\_backend & auto \\
        ddp\_backend & nccl \\
        deepspeed & ./ds\_configs/stage2\_offload.json \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Konfiguration des Trainings}\label{tab:training-config}
\end{table}

Die Konfiguration des Trainings orientiert sich an den \enquote{TrainingArguments}, die ein Teil der Transformers-Bibliothek sind.
Nicht alle Parameter aus der Bibliothek müssen verwendet werden, weshalb hier nur abweichende Parameter vom Standard beschrieben werden.
Eine vollständige Liste der Parameter ist im Anhang unter \cref{app:sec:trainingpaameter} zu finden. Die beschriebenen Parameter sind in \cref{tab:training-config} aufgeführt.\\

Der Parameter \enquote{output\_dir} definiert den Speicherpfad für die Ergebnisse des Trainings.
Diese umfassen die trainierten Gewichte des Modells, eine abschließende Auswertung der Validierung, den Status des Trainers sowie gegebenenfalls während des Trainings erstellte Kontrollpunkte.\\

Der Parameter \enquote{overwrite\_output\_dir} legt fest, ob der Ergebnis-Ordner überschrieben werden soll, wenn er bereits existiert.
Diese Option bestimmt auch, ob das Training von einem zuvor erstellten Kontrollpunkt aus fortgesetzt werden soll.
Wenn der Ergebnis-Ordner überschrieben wird, kann von keinem Kontrollpunkt aus fortgefahren werden.\\

Die Parameter \enquote{do\_train} und \enquote{do\_eval} geben an, ob das Training und die Validierung durchgeführt werden sollen.
Falls keine Validierung durchgeführt wird, wird der Trainingsdatensatz dennoch in Trainings- und Validierungsdatensatz aufgeteilt.\\

Während des Trainings und der Validierung werden die Fehlerfunktionen von mehreren Blöcken berechnet und danach gemittelt.
Daraufhin wird ein Gradient basierend auf dieser kumulativen Fehlerfunktion berechnet, welcher die Gewichte des Modells anpasst.
Eine detailiertere Beschreibung des Ablaufs findet sich in \cref{subsec:backpropagation}.
Die Anzahl der Blöcke pro \enquote{Batch} (siehe \cref{def:batch}) wird durch die Parameter \enquote{per\_device\_train\_batch\_size} und \enquote{per\_device\_eval\_batch\_size} festgelegt.
Wenn man eine Batch-Größe von 1 und 3 \ac{gpu}s verwendet, erhält man somit 3 Blöcke pro Gradientenberechnung.
Während höhere Batch-Größen die Berechnungszeit reduzieren können, führen sie auch zu ungenaueren Gradienten und erfordern mehr Speicher auf der \ac{gpu}.
Die Batch-Größe ist in diesem Fall auf 1 gesetzt, da die verwendeten Nvidia V100-\ac{gpu}s als auch Nvidia A30-\ac{gpu}s nur genug Speicher für eine Batch-Größe von 1 hatten.\\

Die Parameter \enquote{evaluation\_strategy} und \enquote{eval\_steps} legen fest, in welchen Intervallen die Validierung durchgeführt werden soll.
In diesem Szenario wird die Validierung alle 50 Iterationen durchgeführt.
Eine Iteration bezieht sich auf die Berechnung eines Gradienten.\\

Die Lernrate des Modells wird durch den Parameter \enquote{learning\_rate} beschrieben.
Dieser Parameter bestimmt, in welchem Ausmaß die Gewichte des Modells angepasst werden.
Eine sehr hohe Lernrate kann dazu führen, dass das Modell nicht konvergiert, während eine sehr niedrige Lernrate zu langen Trainingszeiten führt.
Die Lernrate wurde aus dem Artikel über LLaMA-Modelle von \citet{llama} übernommen.\\

In dem Artikel zu den LLaMA-Modellen von \citet{llama} wird ebenfalls ein Gewichtsabnahme-Wert von $0,1$ verwendet, der auch bei diesem Training mithilfe des Parameters \enquote{weight\_decay} eingestellt wurde.
Die Gewichtsverminderung führt zu einer kontinuierlichen Verringerung der Gewichte des Modells. Dadurch soll verhindert werden, dass es sich zu stark an einzelne Trainingsdaten anpasst.\\

Die Parameter \enquote{adam\_beta1}, \enquote{adam\_beta2} und \enquote{adam\_epsilon} beschreiben die Parameter des verwendeten AdamW-Optimierers, der im Parameter \enquote{optim} eingestellt ist.
Diese Werte entsprechen ebenfalls den Werten, die im Artikel zu den LLaMA-Modellen von \citet{llama} angegeben sind.
AdamW ist in verschiedenen Implementierungen verfügbar. Hier wurde die neueste Implementierung der PyTorch-Bibliothek verwendet.
Die Funktionsweise des AdamW-Optimierers ist im Artikel von \citet{adamw} genauer beschrieben.\\

Der Parameter \enquote{max\_grad\_norm} beschreibt die maximale Norm des Gradienten, die durch die Gewichte des Modells nicht überschritten werden darf. 
Eine weitere Bezeichnung, die in der Bibliothek DeepSpeed oder in Artikeln zu LLaMA-Modellen genutzt wird, ist \enquote{gradient clipping}.
Er begrenzt die Größe des Gradienten, der in großen neuronalen Netzen explosionsartig ansteigen kann.
Zu große Gradienten führen zu schlechteren Trainingsergebnissen und zu einer zu starken Anpassung der Gewichte, was wiederum zu einer Oszillation um ein Minimum führt.
Um die Gradienten zu begrenzen, wird die L2-Norm des Gradienten berechnet.
Wenn diese Norm den angegebenen Maximalwert überschreitet, wird der Gradient herunterskaliert, bis er die Maximalnorm nicht mehr überschreitet.\\

Der Parameter \enquote{num\_train\_epochs} gibt die Anzahl der zu trainierenden Epochen an.
Eine Epoche umfasst einen vollständigen Durchlauf des Trainingsdatensatzes.
Mehr Epochen können insbesondere bei kleineren Datensätzen zu besseren Ergebnissen führen, da sich das Modell besser an die Trainingsdaten anpassen kann.
Zu viele Epochen führen zu einer Überanpassung.
In dieser Arbeit wurde das Modell Llama 2 7B mit einer, drei, fünf und zehn Epochen trainiert.
Bei der Verwendung von Nvidia V100 Grafikkarten traten während des Trainings Probleme auf, die in \cref{sec:problem-training} genauer beschrieben sind.\\

Durch die Anwendung der Parameter \enquote{lr\_scheduler} und \enquote{warmup\_steps} kann eine Aufwärmphase des Trainings realisiert werden.
Die Aufwärmphase bezeichnet den Start des Trainings, in dem die Lernrate der \enquote{lr\_scheduler}-Funktion innerhalb der ersten \enquote{warmup\_steps} Iterationen schrittweise von 0 auf den gewünschten Wert erhöht wird.
Dieser Ansatz verhindert eine zu schnelle Anpassung der Modellgewichte an spezielle Details der Trainingsdaten.
Vor allem bei untrainierten Modellen werden während des Trainings diese erlernten Fehler wieder korrigiert. Dies führt jedoch ohne eine Aufwärmphase zu längeren Trainingszeiten und schlechteren Ergebnissen.
In diesem Fall kann die Aufwärmphase übersprungen werden, da ein bereits vortrainiertes Modell genutzt wird.\\

Die Parameter \enquote{save\_strategy}, \enquote{save\_steps} und \enquote{save\_total\_limit} beschreiben, wie oft und in welchem Abstand Modelle während des Trainings gespeichert werden sollen.
Hier wird alle 100 Iterationen das Modell gespeichert, wobei maximal 1 Kontrollpunkt gleichzeitig existiert.
Kontrollpunkte ermöglichen es, abgebrochene oder fehlgeschlagene Trainingsläufe wieder aufzunehmen.
Bei längerem Training kann die maximale Laufzeit der Skripte erreicht werden.
Eine Wiederaufnahme des Trainings an diesem Punkt ist dann möglich.
Das Rechenzentrum beschränkt die maximale Laufzeit von Skripten auf 2 Tage.
Wenn das Training länger als 2 Tage dauert, kann diese Grenze überschritten werden.
Die Begrenzung ist notwendig aufgrund der gemeinsamen Nutzung des Rechenzentrums.
Durch die regelmäßige Unterbrechung von Skripten können andere Skripte in der Warteschlange zwischengeschoben werden.
Zusammen mit der Begrenzung ergibt sich im Allgemeinen eine Wartezeit von maximal 2 Tagen.
Durch die Einführung von Kontrollpunkten im Training führt eine Unterbrechung nicht zu einem Rückschlag im Fortschritt.\\

Der Begriff \enquote{no\_cuda} beschreibt die Durchführung des Trainings ohne die Verwendung einer \ac{gpu}.
Diese Einstellung wird für Testzwecke verwendet, damit Testskripte auf Systemen ausgeführt werden können, die nicht über die erforderliche Anzahl von \ac{gpu}s verfügen.\\

Die Verwendung des Parameters \enquote{seed} beeinflusst die Zufälligkeit des Trainingsprozesses.
Während des Trainingsprozesses werden einige Zufallsvariablen verwendet, um Werte zu initialisieren.
Jedoch ist die Auswirkung im Kontext des Fortführenden Trainings irrelevant.
Es wird jedoch ein fester Wert gesetzt, da das Trainingsskript auch für untrainierte Modelle genutzt werden kann.
Mit Hilfe eines Seeds erstellen Zufallszahlengeneratoren zufällig verteilte, jedoch reproduzierbare Zahlenfolgen.
Daher sind Modelle mit gleichem Seed und Datensatz identisch.\\

Zur Konfiguration der verwendeten Datentypen stehen neben dem oben genannten \enquote{torch\_dtype} die beiden Parameter \enquote{fp16} und \enquote{bf16} zur Verfügung. Während \enquote{torch\_dtype} die Initialisierung des Modells beschreibt, beeinflussen diese Parameter die Datentypen während des Trainings.
Es kann nur eine der beiden Optionen verwendet werden. Im Falle des Trainings mit Nvidia V100 Grafikkarten und der \ac{zero} Stufe 2 Optimierung wurde FP16 aufgrund von Architekturanforderungen verwendet. Beim Training mit Nvidia A30 Grafikkarten und \ac{zero} Stufe 3 Optimierung wurde BF16 verwendet.
Die Zahl 16 steht hier für die Anzahl der Bits pro Wert, was wiederum einen großen Einfluss auf den Speicherbedarf und die Genauigkeit des Modells hat.
Datentypen sollten nicht geändert werden, wenn ein vortrainiertes Modell weiter trainiert wird, da eine Änderung schnell zu Fehlverhalten führen kann.
Die Llama 2 Modelle wurden mit dem Datentyp \enquote{bf16} (ausgeschrieben BFloat16) trainiert.
Dieser Datentyp ist nur auf bestimmten \ac{gpu}-Architekturen verfügbar.
Ein Datum im BFloat16-Format hat eine kleinere Mantisse (7 Bits) als ein Float16-Datum (10 Bits Mantisse). Dies führt zu einer geringeren Genauigkeit, verkürzt jedoch die Zeit, die für die Konvergenz des Modells benötigt wird.
Die verwendeten V100 Grafikkarten haben nicht die notwendige Architektur, um BFloat16 Werte zu unterstützen, daher wurde das Llama 2 Modell hier in ein Float16 Format konvertiert.
Dies führt zu Problemen beim Training und beschränkt die maximale Anzahl der Epochen auf 3. Eine genauere Erklärung der Probleme ist in \cref{sec:problem-training} beschrieben.
Bei der Verwendung von A30-Grafikkarten ist die Nutzung von BFloat16-Werten möglich. Mit diesem Datentyp wurden die Modelle zusätzlich auf 5 und 10 Epochen trainiert.\\

Die Parameter \enquote{half\_precision\_backend} und \enquote{ddp\_backend} beschreiben Architekturen für die Ausführung des Trainings mit Hilfe der Bibliothek Transformers.
Während \enquote{half\_precision\_backend} die Architektur für die Ausführung von Trainingsschritten mit Float16-Werten festlegt, beschreibt \enquote{ddp\_backend} die Kommunikationsarchitektur für den Datenaustausch zwischen \ac{gpu}s.
Die DeepSpeed Bibliothek verwendet die \enquote{NCCL} Architektur, daher wird diese hier eingestellt.\\

Die DeepSpeed Konfiguration wird separat in einer JSON Datei gespeichert.
Durch Setzen des Parameters \enquote{deepspeed} verwendet die Transformers Bibliothek die DeepSpeed Bibliothek\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 17.8.2023} zur Ausführung des Trainings.
Wichtig bei der Verwendung von DeepSpeed mit Transformers ist eine identische Konfiguration der Trainingsparameter, wie zum Beispiel \enquote{batch\_size}, \enquote{gradient\_accumulation\_steps} und \enquote{learning\_rate}.\\

\subsection{Konfiguration des DeepSpeed Trainings}\label{sec:deepspeed-config}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{fp16}\\
        enabled&auto\\
        loss\_scale&0\\
        loss\_scale\_window&1000\\
        initial\_scale\_power&16\\
        hysteresis&2\\
        min\_loss\_scale&1\\
        \midrule
        \multicolumn{2}{c}{bf16}\\
        enabled&true|false\\
        \midrule
        \multicolumn{2}{c}{optimizer}\\
        type&AdamW\\
        lr&auto\\
        betas&auto\\
        eps&auto\\
        weight\_decay&auto\\
        \midrule
        \multicolumn{2}{c}{scheduler}\\
        type&WarmupLR\\
        warmup\_min\_lr&auto\\
        warmup\_max\_lr&auto\\
        warmup\_num\_steps&auto\\
        \midrule
        \multicolumn{2}{c}{zero\_optimizations}\\
        stage&2\\
        contiguous\_gradients&true\\
        overlap\_comm&true\\
        reduce\_scatter&true\\
        reduce\_bucket\_size&2e8\\
        allgather\_bucket\_size&2e8\\
        \midrule
        \multicolumn{2}{c}{offload\_optimizer}\\
        device&cpu\\
        pin\_memory&true\\
        \midrule
        gradient\_clipping&1\\
        steps\_per\_print&500\\
        wall\_clock\_breakdown&false\\
        train\_micro\_batch\_size\_per\_gpu&auto\\
    \end{tabular}
    \caption{DeepSpeed Konfiguration}\label{tab:deepspeed-config}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{zero\_optimization}\\
        stage&3\\
        contiguous\_gradients&true\\
        stage3\_max\_live\_parameter&1e9\\
        stage3\_max\_resuse\_distance&1e9\\
        stage3\_prefetch\_bucket\_size&1e7\\
        stage3\_param\_persistence\_threshold&1e5\\
        reduce\_bucket\_size&1e7\\
        sub\_group\_size&1e9\\
        \midrule
        \multicolumn{2}{c}{offload\_param}\\
        device&cpu\\
        pin\_memory&true\\
    \end{tabular}
    \caption{DeepSpeed \ac{zero} Stufe 3 Konfiguration}\label{tab:deepspeed-config-stage3}
\end{table}

Die DeepSpeed-Konfiguration ist in \cref{tab:deepspeed-config} beschrieben.
Die hier verwendeten Parameter beschreiben ein Training mit \ac{zero} Stufe 2 Optimierung und zusätzlichem CPU Offloading.
Die \ac{zero} Stufe 2 Optimierung beschreibt einen Algorithmus zur Reduzierung des Speicherbedarfs während des Trainings.
Dieser Algorithmus wird in \citet{deepspeed} genauer beschrieben.
Zusätzlich zu dieser Optimierung werden Teile der Berechnungen auf die \ac{cpu} ausgelagert, um den Speicherbedarf pro \ac{gpu} weiter zu reduzieren.
Diese Konfiguration wurde beim Training der Modelle mit Nvidia Tesla V100 Grafikkarten verwendet.
\cref{tab:deepspeed-config-stage3} beschreibt zusätzlich eine Konfiguration des \enquote{zero\_optimization} Abschnitts bei Verwendung der \ac{zero} Stufe 3 Optimierung.
Hier werden zusätzlich Parameter der Modelle auf die CPU ausgelagert. 
Diese Konfiguration wurde beim Training der Modelle mit Nvidia Tesla A30 Grafikkarten verwendet.\\\

Der Abschnitt \enquote{fp16} beschreibt den Umgang mit Float16-Werten während des Trainings.
Dabei handelt es sich um eine dynamische Skalierung der Fehlerwerte (engl. \enquote{Dynamic Loss Scaling}).
Die Skalierung der Fehlerwerte ist notwendig, da aufgrund der geringeren Genauigkeit von Float16-Werten kleinere Werte der Fehlerwerte gerundet werden und verloren gehen.
Aus diesem Grund werden die Fehlerwerte bei der Berechnung um mehrere Potenzen skaliert.
Diese Skalierung kann auch zu einem Überlauf über den Wertebereich des Float16 Datentyps führen.
DeepSpeed verwendet hier eine automatische, dynamische Skalierung der Fehlerwerte, ohne einen Überlauf zu verursachen.
Eine genauere Erklärung der Skalierung von Fehlerwerten findet sich im Artikel von \citet{lossscale}.
Der Parameter \enquote{loss\_scale} beschreibt die konstante Skalierung der Fehlerwerte.
Ist er auf $0$ gesetzt, wird eine dynamische Skalierung verwendet.
Der Parameter \enquote{loss\_scale\_window} beschreibt das Werteintervall, in dem die dynamische Skalierung erfolgt.
Der Parameter \enquote{initial\_scale\_power} beschreibt die Größe der initialen Skalierung der Fehlerwerte.
Die tatsächliche Skalierung der Fehlerwerte entspricht $2^{initial\_scale\_power}$.
Der Parameter \enquote{hysteresis} beschreibt die minimale Anzahl von Schritten, in denen die Skalierung nicht verändert werden kann.
Der Parameter \enquote{min\_loss\_scale} beschreibt die minimale Skalierung der Fehlerwerte.
Hier entspricht der Wert $1$ keiner Skalierung.\\

Der Abschnitt \enquote{bf16} beschreibt die Verwendung des binären Datentyps Float16 während des Trainings.
Ist er auf \enquote{true} gesetzt, so werden die Berechnungen mit dem Datentyp BFloat16 durchgeführt.
Ist er auf \enquote{false} gesetzt, so werden die Berechnungen mit dem Datentyp Float16 durchgeführt.
Bei einem Training mit V100 Grafikkarten und \ac{zero} Stufe 2 Optimierung ist dieser Parameter auf \enquote{false} gesetzt.
Im Falle des Trainings mit A30 Grafikkarten und einer \ac{zero} Stufe 3 Optimierung ist dieser Parameter auf \enquote{true} gesetzt.\\

Der Abschnitt \enquote{optimizer} beschreibt die Parameter des Optimierungsalgorithmus. Wie bereits bei den Trainingsparametern beschrieben, wird der AdamW-Optimierer verwendet.
Die Parameter \enquote{lr}, \enquote{betas}, \enquote{eps} und \enquote{weight\_decay} müssen mit den in den Trainingsparametern eingestellten Werten übereinstimmen.
Aus diesem Grund werden diese Parameter vor Beginn des Trainings auf \enquote{auto} gesetzt und durch die Transformer-Bibliothek ergänzt.\\

Der Abschnitt \enquote{scheduler} beschreibt die Parameter des Lernratenplaners. Der Lernratenplaner ist für die Anpassung der Lernrate während des Trainings verantwortlich.
Er dient in diesem Fall zur Durchführung der Aufwärmphase. Ist die Aufwärmphase in den Trainingsargumenten deaktiviert, so wird die Bibliothek diese Phase auch in der DeepSpeed-Konfiguration deaktivieren.\\

Der Abschnitt \enquote{zero\_optimizations} beschreibt die Parameter der \ac{zero} Stufe 2 Optimierung.
Die \ac{zero} Optimierung wird unter \citet{deepspeed} genauer beschrieben.
Der Parameter \enquote{contiguous\_gradients} beschreibt die Auslagerung der Gradienten in einen zusammenhängenden Speicherbereich während der Berechnung.
Dadurch wird eine Fragmentierung des Speichers während der Backpropagation vermieden.
Mit Hilfe des Parameters \enquote{overlap\_comm} wird versucht, die Gradienten während der Berechnung der Backpropagation zu reduzieren, um eine schnellere Gesamtberechnung zu ermöglichen.
\enquote{reduce\_scatter} beschreibt die Verwendung einer speziellen Reduktionsmethode \enquote{Reduce Scatter} zur Mittelung von Gradienten.
In Kombination mit den Parametern \enquote{reduce\_bucket\_size} und \enquote{allgather\_bucket\_size} wird die maximale Anzahl der in einem Schritt zu reduzierenden Gradienten festgelegt.
Diese Option reduziert den Speicherbedarf während des Trainings erheblich.
Mit Hilfe der Option \enquote{offload\_optimizer} wird der Zustand und die Berechnung des Optimierers auf die CPU ausgelagert.
Optional kann dies für sehr große Modelle auch auf eine NVMe SSD erfolgen.
Mit Hilfe der Einstellung \enquote{pin\_memory} wird der für die Berechnung benötigte Speicher auf der CPU reserviert.
Dies führt zu einer besseren Performance, aber auch zu zusätzlichem Speicherbedarf.\\

Wie bereits bei den Trainingsargumenten durch den Parameter \enquote{max\_grad\_norm} beschrieben, wird die maximale Größe des Gradienten durch den Parameter \enquote{gradient\_clipping} festgelegt.
Eine automatische Übernahme der Konfiguration der Trainingsargumente ist in diesem Fall nicht möglich.
Die Werte müssen jedoch übereinstimmen.\\

Die Parameter \enquote{steps\_per\_print} und \enquote{wall\_clock\_breakdown} beschreiben die Ausgabe von Informationen während des Trainings.
Zusätzlich zur Ausgabe der Transformers-Bibliothek werden alle 500 Iterationen weitere Informationen der DeepSpeed-Bibliothek ausgegeben.
Mit Hilfe des Parameters \enquote{wall\_clock\_breakdown} wird zusätzlich die Messung der verstrichenen Zeit für jede Phase einer Iteration ausgegeben.\\

Der Parameter \enquote{train\_micro\_batch\_size\_per\_gpu} beschreibt die Anzahl der Batches pro GPU.
Dieser Wert wird aus den Trainingsargumenten übernommen.\\

Für die Ausführung des Trainings mit der \ac{zero} Stufe 3 Optimierung auf Nvidia Tesla A30 Grafikkarten wurde der Abschnitt \enquote{zero\_optimization} in \cref{tab:deepspeed-config} durch die in \cref{tab:deepspeed-config-stage3} gezeigten Parameter ersetzt. Weggefallen sind hier die Parameter \enquote{overlap\_comm}, \enquote{reduce\_scatter} und \enquote{allgather\_bucket\_size}.
Zusätzliche Parameter sind hier \enquote{stage3\_max\_live\_params}, \enquote{stage3\_max\_reuse\_distance}, \enquote{stage3\_prefetch\_bucket\_size}, \enquote{stage3\_param\_persistence\_threshold} und \enquote{sub\_group\_size} sowie der Abschnitt \enquote{offload\_param}.
Der Parameter \enquote{stage3\_max\_live\_params} beschreibt die maximale Anzahl von Gewichten, die während des Trainings in den Speicher der \ac{gpu} geladen werden. Kleinere Werte führen zu weniger Speicherverbrauch, aber auch zu mehr Kommunikation.
Der Parameter \enquote{stage3\_max\_reuse\_distance} beschreibt, dass ein Gewicht nicht freigegeben werden kann, wenn es innerhalb dieser Anzahl von Gewichten erneut zur Berechnung verwendet wird. Auch hier gilt, dass kleinere Werte zu einer geringeren Speicherbelegung, aber auch zu einer erhöhten Kommunikation führen.
Der Parameter \enquote{stage3\_prefetch\_bucket\_size} beschreibt die Anzahl der Gewichte, die während des Trainings im Voraus geladen werden.
Der Parameter \enquote{stage3\_param\_persistence\_threshold} verhindert die Partitionierung von Gewichten, die kleiner als dieser Wert sind.
Der Parameter \enquote{sub\_group\_size} beschreibt die maximale Anzahl von Parametern, die gleichzeitig für die Berechnung verwendet werden.

\subsection{Verwendete Bibliotheken zum Training}
\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Bibliothek} & \textbf{Version} \\
        \midrule
        datasets & 2.13.1 \\
        DeepSpeed & 0.10.0 \\
        evaluate & 0.4.0 \\
        PyTorch & 2.0.1 \\
        sentencepiece & 0.1.96 \\
        Transformers & 4.31.0 \\
        \bottomrule
    \end{tabular}
    \caption{Verwendete Bibliotheken zum Training}
    \label{tab:training-libraries}
\end{table}
Die für das Training verwendeten Bibliotheken sind in \cref{tab:training-libraries} aufgelistet.
Einige der hier aufgeführten Pakete benötigen weitere Abhängigkeiten, die über die verwendete Paketverwaltung \ac{pip}\footnote{\url{https://pip.pypa.io/en/stable/} abgerufen am 19.8.2023} installiert werden.\\

Zur Verwaltung der Datensätze wird die Bibliothek \enquote{datasets} verwendet.
Mit Hilfe dieser Bibliothek können Textdateien während des Trainings geladen, in Tokens umgewandelt und in Blöcke gruppiert werden.
Über eine Schnittstelle zu den Bibliotheken Transformers und DeepSpeed ermöglicht \enquote{datasets} auch die geordnete Zuordnung von Blöcken zu verschiedenen \ac{gpu}s.
Die Bibliothek stammt von Huggingface\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 19.8.2023}.\\

Die \enquote{DeepSpeed}-Bibliothek wird verwendet, um das Training und die Ausführung auf mehreren \ac{gpu}s zu beschleunigen.
Sie ermöglicht die Nutzung der \ac{zero}-Optimierung, die es erlaubt, Teile des Trainings und des Modells auf mehrere GPUs, die CPU und lokale NVMe SSDs auszulagern.
Der Prozess dieser Optimierung wird in \citet{deepspeed} genauer beschrieben.
Die Bibliothek stammt von Microsoft\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{evaluate} wird zur Berechnung der Modellgenauigkeit verwendet.
Sie stammt von Huggingface\footnote{\url{https://huggingface.co/docs/evaluate/index} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{PyTorch} bildet die Grundlage für die Berechnung der neuronalen Netze.
Sie ermöglicht neben der eigentlichen Berechnung aller Trainingsphasen auch die Verwendung von \ac{gpu}s. Die Bibliothek stammt von PyTorch selbst\footnote{\url{https://pytorch.org/} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{sentencepiece} wird für die Tokenisierung der Texte verwendet.
Sie wird im Skript nicht explizit verwendet, ist hier aber als zusätzliche Voraussetzung für die Verwendung des Tokenizers des Llama-Modells aufgeführt.
Diese Bibliothek wurde auch im Artikel von \citet{llama} verwendet.
Sie wurde von Google veröffentlicht\footnote{\url{https://github.com/google/sentencepiece} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{Transformers} wird für die Verwaltung der Modelle und die Berechnung der neuronalen Netze verwendet.
Sie ermöglicht die Verwendung vortrainierter Modelle und bietet einen abstrakten Trainer, der die verschiedenen Phasen des Trainings steuert.
Außerdem erlaubt es die Verwendung der Bibliothek \enquote{DeepSpeed} ohne zusätzliche Anpassung.
Die Bibliothek stammt von Huggingface\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 19.8.2023}.\\

\subsection{Training auf einem GPU Computing Cluster}\label{sec:training-cluster}
Das Trainieren der Modelle ist wesentlich rechenintensiver als deren Anwendung.
Aus diesem Grund kann ein normaler Computer dafür nicht verwendet werden.
Stattdessen wurde ein GPU Computing Cluster des Rechenzentrums der Universität Leipzig verwendet.\\

Das Rechenzentrum der Universität Leipzig stellt zwei GPU Computing Cluster mit den Namen \enquote{Paula} und \enquote{Clara} zur Verfügung.
Das Cluster \enquote{Paula} verfügt über 12 Knoten, die jeweils mit 8 Nvidia Tesla A30 \ac{gpu}s ausgestattet sind.
Eine A30 \ac{gpu} verfügt über \SI{24}{\giga\byte} \ac{gpu} \ac{ram}.
Ebenso besitzt ein Knoten 2 AMD(R) EPYC(R) 7713 CPU Prozessoren mit jeweils 64 Kernen und \SI{1}{\tera\byte} \ac{ram}.
Das Cluster \enquote{Clara} ist zweigeteilt.
Teil 1 besteht aus 8 Knoten, die jeweils mit 4 Nvidia Tesla V100 \ac{gpu}s ausgestattet sind. Eine V100 \ac{gpu} verfügt über \SI{32}{\giga\byte} \ac{gpu} \ac{ram}.
Teil 2 besteht aus 23 Knoten mit jeweils 8 Nvidia GeForce RTX 2080 Ti \ac{gpu}s. Eine RTX 2080 Ti \ac{gpu} verfügt über \SI{11}{\giga\byte} \ac{gpu} \ac{ram}.
Jeder Knoten ist ausgestattet mit einer AMD(R) EPYC(R) 7551P CPU mit 32 Kernen und \SI{512}{\giga\byte} \ac{ram}.
Alle Knoten innerhalb eines Clusters sind über ein \SI{100}{\giga\bit\per\second} Infiniband-Netzwerk miteinander verbunden.
Die Verbindung zum Internet erfolgt über eine \SI{25}{\giga\bit\per\second} Ethernet-Verbindung.\\

Die Ausführung der Skripte erfolgt mit Hilfe der Software \enquote{Slurm Workload Manager}.
Diese ermöglicht eine umfangreiche Reservierung von Ressourcen und bietet eine Bash-Schnittstelle zur Konfiguration und Ausführung der Skripte.
Mit Hilfe von Slurm-Bash-Dateien konnte das Training der Modelle auf dem Cluster durchgeführt werden.\\

\subsection{Probleme während des Trainings}\label{sec:problem-training}
Die Durchführung des Trainings verlief nicht ohne Probleme.
Ideal wäre es, mehrere Modelle mit unterschiedlicher Epochenzahl und unterschiedlichen Größen zu trainieren.
Neben dem Modell Llama 2 7B verspricht ein größeres Modell wie das Modell 13B, 33B oder 65B deutlich bessere Leistungen sowohl in der Textnachahmung als auch im Wissensverständnis und damit in den Ergebnissen der Evaluation.
Das Training ist auf 4 \ac{gpu}s pro Knoten mit jeweils \SI{32}{\giga\byte} \ac{gpu}-\ac{ram} beziehungsweise \SI{24}{\giga\byte}, wie bereits in \cref{sec:training-cluster} erwähnt.
Um größere Modelle zu trainieren, muss diese Anzahl erhöht werden.
Die Trainingsparameter erlauben hier eine einfache Erweiterung auf zusätzliche Grafikkarten.
Die Trainingsskripte müssen dafür nicht angepasst werden.\\

Das Training auf mehreren Knoten mit mehr Grafikkarten wurde testweise durchgeführt.
Dazu wurde ein Testskript von Huggingface, das die Kommunikation zwischen Grafikkarten und Knoten sicherstellt, sowie das Trainingsskript verwendet.
Beide Skripte schlugen in dieser Konfiguration fehl.
Der Grund dafür ist ein Kommunikationsfehler zwischen den Knoten.
Die Kommunikation während des Trainings wird vollständig von den Bibliotheken Transformers und DeepSpeed übernommen.
Dazu wird ein verwendeter Knoten als Main-Knoten bestimmt, der das Nachrichten-Routing und das Daten-Routing übernimmt.
Andere Knoten übernehmen die Rolle der Client-Knoten.
Auf dem Main-Knoten wird ein Server gestartet, mit dem sich die Client-Knoten über IPv4 oder IPv6 verbinden können.
Diese Verbindung scheiterte in allen Einstellungen.
Es konnte keine Lösung für dieses Problem gefunden werden.
Aus diesem Grund wurde das Training auf einen Knoten und damit 4 \ac{gpu}s beschränkt.\\

Größere Modelle wie das 33B oder das 65B Modell können mit dem aktuellen Setup nicht trainiert werden.
Aus diesem Grund und aus Gründen der Ressourcenschonung wurde das Training mit dem Modell Llama 2 7B begonnen.
Bei der Ausführung auf Nvidia V100 Grafikkarten traten in unregelmäßigen Abständen Überlauffehler auf.
Wie in \cref{sec:deepspeed-config} beschrieben, werden die berechneten Fehlerwerte der Fehlerrückführungsfunktion während der Berechnung skaliert, um eine höhere Genauigkeit zu erreichen und einen Unterlauf zu vermeiden.
Diese Skalierung führte zu einem Überlauf über den Maximalwert des Datentyps Float16.
Da in der DeepSpeed-Konfiguration eine dynamische Fehlerskalierung eingestellt war, wurde die Skalierung anschließend um eine Potenz verringert.
Dieser Vorgang wiederholte sich in unregelmäßigen Abständen und führte schließlich zum Erreichen des minimalen Skalierungsfaktors 1, der keiner Skalierung entspricht.
Ist dieser Wert erreicht und eine Skalierung ist dennoch notwendig, wird das Training abgebrochen.\\

Der Grund für die auftretenden Skalierungsfehler ist nicht bekannt.
Eine mögliche Ursache ist die Verwendung von Llama-Modellen mit dem Datentyp Float16.
Ursprünglich wurden diese Modelle mit dem Datentyp BFloat16 trainiert.
Von einer Transformation in Float16-Werte wird auch seitens Huggingface abgeraten.
Ebenso wird ein Fehler in der Implementierung von DeepSpeed vermutet, der das Laden von Modellen im Float16 Format nicht korrekt umsetzt.
Im offiziellen GitHub Repository von DeepSpeed wurde bereits eine Lösung vorgeschlagen, die jedoch noch nicht in DeepSpeed übernommen wurde.
Die Verwendung von BFloat16 Werten ist nur auf dem Cluster \enquote{Paula} mit Nvidia Tesla A30 Grafikkarten möglich.
BFloat16 Werte können nur von Grafikkarten umgesetzt werden, die die AMP (Automatic Mixed Precision) Architektur unterstützen.
Diese Architektur wird von den Grafikkarten der Nvidia A100 Serie unterstützt, die für das initiale Training der Llama-Modelle verwendet wurden.
Im Rechenzentrum stehen jedoch nur Grafikkarten der Serien Nvidia Tesla V100, Nvidia GeForce RTX 2080 Ti und Nvidia Tesla A30 zur Verfügung.
Sowohl die V100- als auch die RTX2080-Serie unterstützen die Verwendung von BFloat16-Werten nicht.
Die A30 Serie unterstützt BFloat16, bietet aber nur \SI{24}{\giga\byte} \ac{ram}, weshalb hier eine \ac{zero} Stufe 3 Optimierung gewählt wurde.
Bei der Verwendung von A30-Grafikkarten traten bei einem Training bis zu 10 Epochen keine Skalierungsfehler auf.\\

Anfangs verhinderte der Skalierungsfehler das Training der Modelle auf V100 Grafikkarten vollständig.
Durch Anpassung der Blockgröße in \cref{subsec:config-training} auf kleinere Werte konnte die Häufigkeit der Skalierungsfehler minimiert werden, so dass mit dieser Einstellung ein Training bis 3 Epochen möglich war.\\

Während des Trainings traten mehrfach Speicherfehler auf, die anzeigten, dass der gesamte Speicher der Grafikkarte belegt war, jedoch mehr benötigt wurde.
Diese Fehler waren unabhängig vom Grafikkartentyp und der Konfiguration des Trainings.
Erst die Minimierung der Batchgröße auf 1 pro \ac{gpu}, die Aktivierung des CPU-Offloads und die Nutzung von \SI{256}{\giga\byte} CPU \ac{ram} ermöglichten ein erfolgreiches Training des Modells Llama 2 7B.
Dieser Speicherbedarf ist unerwartet hoch und kann nicht durch die Größe des Modells erklärt werden.
Für das Training auf V100 Grafikkarten wurde die \ac{zero} Stufe 2 Optimierung mit zusätzlichem CPU Offloading verwendet.
Für das Training auf A30 Grafikkarten musste die \ac{zero} Stufe 3 Optimierung mit CPU Offloading gewählt werden.\\

% - genertierter Text ohne Stopzeichen: Stopzeichen händisch nach NewLine (double Newline?)
% - Kleinere Block\_size für Loss Scaling
% - BF16 nur auf AMP \ac{gpu} Architektur möglich (hier nicht)
% - Stage2/3 ohne CPU Offloading führt zu OOM
% - Deutlich mehr RAM erforderlich (180GB?)
% - Multi-Node Training versursacht Kommunikationsfehler



%
% - requirements txt erstellen
% - DeepSpeed Config mit https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters+
% - Nutzung von ZeRO Stage2 optimization
% - llama2 7b genutzt
% - dataclasses um Datensatz einzulesen
% - Leistungen bei 11s/it von 3600 Iteration = 11h Training
% - Evaluation aller 50 Iterations, Saves aller 100 Iterations
% - 3 Epochen = 14s/it 1800 = 7h Training (256 Block\_size)
% - Deepspeed Launcher mit 3 \ac{gpu}s (32GB v100, 1 Maschine)
% - Weitere Hyperparameter aus Llama 2 Modell
% - Nutzung von Epub anstelle Word Datei da bessere Konvertierung
% - keine nutung von chat Version (verlernt Human Reinforcment)