%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

Die Lösung gliedert sich in sechs Schritte, die für die Durchführung des Trainings notwendig sind.
Im Folgenden werden diese Schritte näher erläutert. Für jeden Schritt sind die verwendeten Techniken und Bibliotheken separat aufgeführt.
Die Schritte sind wie folgt gegliedert:
\begin{enumerate}
    \item Laden des Modells
    \item Konvertierung des Modells in ein kompatibles Format
    \item Modelltraining
    \item Erstellung von Antworten auf dem Evaluierungsdatensatz
    \item Bewertung der erzeugten Antworten
    \item Auswertung basierend auf den Bewertungen
\end{enumerate}

\section{Herunterladen des Modells}
Die LLaMA-Modelle werden unter einer nicht-kommerziellen Lizenz für Forschungszwecke zur Verfügung gestellt. Der Zugang zu den Modellen wird im Einzelfall auf Anfrage gewährt. Diese Anfrage wurde im Rahmen dieser Arbeit gestellt und bestätigt.
Anschließend kann ein von den Autoren vorbereitetes Skript verwendet werden, um die trainierten Parameter des Modells herunterzuladen.
Das Skript benötigt eine explizite \ac{url}, die nach einer vorgegebenen Zeit von einer Woche nach Freigabe der Modelle ungültig wird. Aus diesem Grund ist diese \ac{url} nicht im Skript enthalten.

\section{Umwandlung des Modells in ein kompatibles Format}
Das Training des Modells basiert auf zwei grundlegenden Bibliotheken: der Transformers Bibliothek von Huggingface und der DeepSpeed Bibliothek von Microsoft.
Diese Bibliotheken, insbesondere Transformers, erleichtern den Entwicklungsprozess enorm und bieten ein hohes Maß an Abstraktion. Allerdings können Sie nur mit Modellen arbeiten, die in einer für die Transformers-Bibliothek verständlichen Form vorliegen.
Aus diesem Grund wird das heruntergeladene Modell mit Hilfe eines Huggingface-Skripts in diese kompatible Form umgewandelt.
Während dieser Konvertierung muss das Modell vollständig in den \ac{ram} des ausführenden Rechners geladen werden. Für das Modell LLaMA 7B sind dafür mehr als 14 GB RAM erforderlich.
Die Konvertierung dauerte auf einem Rechner mit 32 GB RAM und einem AMD Ryzen 7 5800X Prozessor ca. 30 Minuten.

\section{Training des Modells}
Um das LLaMA-Modell zu trainieren, wird die Transformers Bibliothek von Huggingface verwendet\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 16.8.2023}.
Das in Python geschriebene Trainingsskript basiert auf dem Beispielskript zum Training von kausalen Sprachmodellen aus dem Huggingface Github Repository\footnote{\url{https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py} abgerufen am 16.8.2023} und wurde teilweise an die Anforderungen des hier durchgeführten Trainings angepasst.
Die Konfiguration des Trainings gliedert sich in vier Bereiche: Einstellungen für das Modell, Einstellungen für die Trainingsdaten, Einstellungen für das Training selbst und Einstellungen für DeepSpeed.
Im Folgenden werden diese Einstellungen näher erläutert.
\subsection{Konfiguration des Modells}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        model\_name & meta-llama/Llama-2-7b-hf \\
        cache\_dir & ./cache \\
        use\_fast\_tokenizer & false \\
        model\_revision & main \\
        use\_auth\_token & true \\
        hugging\_token & \textit{Huggingface Token} \\
        torch\_dtype & auto \\
        low\_cpu\_mem\_usage & false \\
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration des Modells}\label{tab:model-config}
\end{table}
Die zur Auswahl und Konfiguration des Modells verwendeten Parameter sind in der Tabelle \ref{tab:model-config} aufgelistet.\\

Der Parameter \enquote{model\_name} entspricht einer Modellauswahl und kann entweder ein relativer Pfad zu einem lokalen Modell oder ein Modellname sein.
Der Modellname wurde hier auf das Modell Llama 2 7B gesetzt und verweist auf das von Huggingface gehostete Modell in einem mit der Transformers-Bibliothek kompatiblen Format.
Llama 2 wurde im Juli 2023 von Meta AI veröffentlicht und stellt eine generelle Verbesserung der ursprünglichen LLaMA 1 Modelle dar.
Neben der nun auf 4096 Tokens erweiterten Kontextlänge stellt Meta AI Llama 2 auch in einer Chat-Version zur Verfügung.
Die Chat-Version wurde zusätzlich mit Hilfe von Reinforcement Learning aus menschlichem Feedback trainiert und ermöglicht so eine einfachere Nutzung der vortrainierten Modelle im Kontext eines Chatbots.
Diese Chat-Modelle wurden jedoch nicht zum Training verwendet, da davon ausgegangen wird, dass dieses zusätzliche Training durch das hier durchgeführte Continual Pretraining überschrieben wird.
Außerdem ermöglicht Huggingface nun die Nutzung der Llama 2 Modelle ohne Download und Konvertierung, so dass die ersten beiden Schritte entfallen.
Die Llama 2 Modelle wurden unter anderem im Artikel von \citet{llama2} vorgestellt.\\

Der Parameter \enquote{cache\_dir} beschreibt den Speicherort des heruntergeladenen Modells.
Dies ermöglicht ein wiederholtes Trainieren des Modells ohne erneuten Download.
Die heruntergeladenen Modelle werden durch das Training nicht überschrieben und stellen somit den Grundzustand des Modells dar (im Folgenden auch als Llama2-0e für \enquote{0 Epochen trainiert} bezeichnet).\\

\enquote{use\_fast\_tokenizer} konfiguriert die Verwendung einer schnelleren Version des Tokenizers zur Konvertierung der Datensätze.
Diese Option ist optional und wurde hier nicht verwendet.\\

\enquote{model\_revision} beschreibt die zu verwendende Version des Modells.
Hier wurde die aktuellste Version verwendet, die durch den Wert \enquote{main} repräsentiert wird.\\

\enquote{use\_auth\_token} und \enquote{hugging\_token} beschreiben die Verwendung eines Authentifizierungs-Tokens, um Modelle von Huggingface herunterzuladen.
Diese Authentifizierung ist notwendig, da auch die Llama 2 Modelle unter der gleichen Lizenz wie LLaMA 1 stehen und nur auf Anfrage zur Verfügung gestellt werden.
Um die Modelle mit beschränktem Zugang herunterzuladen, wurde ein Huggingface-Account erstellt, die Anfrage an Meta AI zur Nutzung der Llama 2 Modelle gestellt und bestätigt und ein Authentifizierungstoken generiert.\\

Der Parameter \enquote{torch\_dtype} beschreibt den Datentyp, der für die Darstellung der Modellparameter verwendet wird.
Hier stehen Float32, Float16 und BFloat16 zur Verfügung.
Vorgefertigte Modelle wurden ursprünglich mit einem Datentyp erstellt und müssen in einen anderen Datentyp konvertiert werden, wenn der \enquote{torch\_dtype} nicht übereinstimmt.
Diese Konvertierung ist rechenintensiv und kann dazu führen, dass Modelle fehlerhaft trainiert werden oder mehr Rechenleistung während der Inferenz und des Trainings benötigen.
Aus diesem Grund wurde hier der Parameter \enquote{auto} verwendet, der den Datentyp des Modells automatisch erkennt und benutzt.\\

\enquote{low\_cpu\_mem\_usage} beschreibt ein Verfahren der Bibliothek Transformers zum Laden großer Modelle auf Systemen mit wenig Arbeitsspeicher.
Dabei werden die Modelle in mehreren Schritten in den Arbeitsspeicher geladen und anschließend in den \ac{gpu}-Speicher übertragen.
Dieses Verfahren reduziert die Menge des notwendigen Arbeitsspeichers, erhöht aber die Zeit, die zum Laden des Modells benötigt wird.
Da im vorliegenden Fall genügend Arbeitsspeicher zur Verfügung stand, wurde auf dieses Verfahren verzichtet.

\subsection{Konfiguration der Trainingsdaten}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        train\_file & ./input/health\_information\_systems\_epub.md \\
        max\_train\_samples & None \\
        overwrite\_cache & false \\
        block\_size & 1024 \\
        validation\_split\_percentage & 5 \\
        preprocessing\_num\_workers & 1 \\
        keep\_linebreaks & true \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration der Trainingsdaten}\label{tab:data-config}
\end{table}
Für das Training des Modells Llama 2 wurde das Buch \enquote{Health Information Systems} von \citet{bb} im epub-Format in das Markdown-Format konvertiert.
Die notwendigen Änderungen am Text sind in \cref{sec: datenkuration} beschrieben.
Um diese Markdown-Datei in eine für das Modell verständliche Form umzuwandeln, wird die Bibliothek datasets\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 16.8.2023} von Huggingface verwendet.
Sie ermöglicht das Laden der Textdatei, die Umwandlung in Tokens und die Aufteilung in Blöcke.
Die Parameter zur Auswahl und Konfiguration der Trainingsdaten sind in der Tabelle \ref{tab:data-config} aufgelistet.\\

Der Parameter \enquote{train\_file} beschreibt den Pfad zur Trainingsdatei.
Diese Trainingsdatei wird einmal eingelesen und je nach Anzahl der unter \cref{subsec:config-training} beschriebenen Epochen mehrfach verwendet.\\

\enquote{max\_train\_samples} beschreibt die maximale Anzahl von Blöcken, die aus der Trainingsdatei gelesen werden sollen.
Zu Testzwecken kann hier eine geringere Anzahl an Blöcken verwendet werden, um die Konfiguration des Modells zu testen.
Im finalen Training wurde dieser Parameter auf \enquote{None} gesetzt, um alle Blöcke zu verwenden.\\

Der Parameter \enquote{overwrite\_cache} beschreibt, ob der Text erneut in Tokens umgewandelt werden soll.
Wenn sich der Text geändert hat, kann das Skript hier die nun tokenisierte Textdatei überschreiben.\\

Wie bereits erwähnt, wird der Text in Blöcke aufgeteilt.
Jeder Block wird vom Modell vollständig und gleichzeitig gelesen.
Die maximale Größe eines Blocks ist durch das Modell begrenzt und liegt bei Llama 2 bei 4096 Tokens.
Standardmäßig verwenden Modelle jedoch eine Blockgröße von 1024 Tokens, weshalb hier diese Größe angenommen wird, wenn keine weiteren Angaben gemacht werden.
Größere Blöcke führen zu einer schnelleren Verarbeitung des Textes und können zu einem besseren Verständnis der Zusammenhänge führen, da ein größerer Kontext betrachtet wird.
Allerdings steigt mit der Größe der Blöcke auch die Fehleranfälligkeit, so dass teilweise auch kleinere Blöcke verwendet werden müssen, um ein Training erfolgreich durchzuführen.
Probleme beim Training sind unter \cref{sec: problem-training} beschrieben.
Die Blockgröße kann mit dem Parameter \enquote{block\_size} angepasst werden.\\

Der Parameter \enquote{validation\_split\_percentage} beschreibt den Anteil der Daten, der für die Validierung genutzt werden soll.
Hier werden \SI{5}{\percent} der Daten für die Validierung verwendet.
Die Validierung liefert während des Trainings Informationen über die tatsächliche Leistung des Modells im Vergleich zum ungesehenen Text und dient dazu, den Fortschritt des Modells zu messen.\\

\enquote{preprocessing\_num\_workers} beschreibt die Anzahl der Prozesse, die für die Umwandlung der Textdatei in Tokens verwendet werden sollen.
Bei sehr großen Datenmengen ist die Umwandlung von Textdateien in Tokens eine sehr umfangreiche Aufgabe.
Um diesen Vorgang zu beschleunigen, können mehrere Prozesse die Textdateien parallel übersetzen.
In diesem Fall ist die Datenmenge jedoch klein genug, um die Aufgabe mit nur einem Prozess zu erledigen.\\

Der Parameter \enquote{keep\_linebreaks} beschreibt, ob Zeilenumbrüche in der Textdatei erhalten bleiben sollen.
In einigen Fällen können Textdateien viele Zeilenumbrüche enthalten, die der Formatierung des Textes dienen, aber dem Modell keine Informationen liefern oder es dazu veranlassen, diese Zeilenumbrüche zu imitieren.
Aus diesem Grund können Zeilenumbrüche optional entfernt werden.
Aufgrund der zuvor durchgeführten Datenkuration ist dies jedoch nicht notwendig, weshalb dieser Parameter auf \enquote{true} gesetzt wird.\\

\subsection{Konfiguration des Trainings}\label{subsec:config-training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        output\_dir & ./trained/7B-3 \\
        overwrite\_output\_dir & true \\
        do\_train & true \\
        do\_eval & true \\
        per\_device\_train\_batch\_size & 1 \\
        per\_device\_eval\_batch\_size & 1 \\
        evaluation\_strategy & steps \\
        eval\_steps & 50 \\
        learning\_rate & 3e-4 \\
        weight\_decay & 0.1 \\
        optim & adamw\_torch \\
        adam\_beta1 & 0.9 \\
        adam\_beta2 & 0.95 \\
        adam\_epsilon & 1e-5 \\
        max\_grad\_norm & 1.0 \\
        num\_train\_epochs & 3 \\
        lr\_scheduler\_type & cosine \\
        warmup\_steps & 0 \\
        save\_strategy & steps \\
        save\_steps & 100 \\
        save\_total\_limit & 1 \\
        no\_cuda & false \\
        seed & 42 \\
        fp16 & true \\
        bf16 & false \\
        half\_precision\_backend & auto \\
        ddp\_backend & nccl \\
        deepspeed & ./ds\_configs/stage2\_offload.json \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Konfiguration des Trainings}\label{tab:training-config}
\end{table}

Die Konfiguration des Trainings folgt den \enquote{TrainingArguments}, welche Teil der Transformers Bibliothek sind.
Nicht alle der dort vorhandenen Parameter müssen gesetzt werden, weshalb hier nur die vom Standard abweichenden Parameter beschrieben werden.
Eine vollständige Liste ist im Anhang in \cref{app:sec:trainingpaameter} zu finden. Die hier beschriebenen Parameter sind in \cref{tab:training-config} aufgeführt.\\

Der Parameter \enquote{output\_dir} beschreibt den Pfad, in welchem die Ergebnisse des Trainings gespeichert werden sollen.
Diese beinhalten die trainierten Gewichte des Modells, eine finale Auswertung der Validierung, den Zustand des Trainers und eventuelle während des Trainings erstellten Kontrollpunkte.\\

Mit dem Parameter \enquote{overwrite\_output\_dir} wird festgelegt, ob der Ergebnis-Ordner überschrieben werden soll, falls dieser bereits existiert.
Diese Option bestimmt ebenso ob das Training von einem zuvor erstellen Kontrollpunkt fortgeführt werden soll.
Wird der Ergebnis-Ordner überschrieben, kann von keinem Kontrollpunkt fortgefahren werden.\\

Die Parameter \enquote{do\_train} und \enquote{do\_eval} beschreiben, ob das Training und die Validierung durchgeführt werden sollen.
Wird die Validierung nicht durchgeführt, wird der Trainingsdatensatz trotzdem in Trainings- und Validierungsdatensatz aufgeteilt.\\

Während des Trainings und der Validierung werden die Fehlerfunktionen von mehreren Blöcken berechnet und anschließend gemittelt.
Anschließend wird basierend auf dieser kummulativen Fehlerfunktion ein Gradient berechnet, welcher die Gewichte des Modells anpasst.
Der Ablauf ist genauer unter \cref{subsec:backpropagation} beschrieben.
Die Anzahl der Blöcke pro \enquote{Batch} (siehe \cref{def:batch}) wird durch die Parameter \enquote{per\_device\_train\_batch\_size} und \enquote{per\_device\_eval\_batch\_size} festgelegt.
Mit einer Batch-Größe von 1 und 3 \ac{gpu}s erhält man somit 3 Blöcke pro Gradientenberechnung.
Höhere Batch-Sizes führen zu schnellerer Berechnung, jedoch auch ungenaueren Gradienten und brauchen mehr \ac{gpu}-Speicher.
In diesem Fall ist die Batch-Größe auf 1 gesetzt, da die genutzten V100-\ac{gpu}s nur genug Speicher für eine Batch-Größe von 1 hatten.\\

Die Parameter \enquote{evaluation\_strategy} und \enquote{eval\_steps} beschreiben, wie oft die Validierung durchgeführt werden soll.
In diesem Fall wird die Validierung alle 50 Iterationen durchgeführt.
Eine Iteration beschreibt hierbei die Berechnung eines Gradienten.\\

Der Parameter \enquote{learning\_rate} beschreibt die Lernrate des Modells.
Diese bestimmt, wie stark die Gewichte des Modells angepasst werden.
Eine zu hohe Lernrate kann dazu führen, dass das Modell nicht konvergiert, während eine zu niedrige Lernrate zu langen Trainingszeiten führt.
Die Lernrate wurde aus dem Artikel zu den LLaMA Modellen von \citet{llama} übernommen.\\

In dem Artikel zu den LLaMA Modellen von \citet{llama} wird ebenso eine Gewichtsabnahme von $0,1$ genutzt, welche ebenso in diesem Training mit Hilfe des Parameters \enquote{weight\_decay} gesetzt wurde.
Die Gewichtsreduktion führt zu einer kontinuierlichen Verringerung der Gewichte des Modells, was verhindern soll, dass es sich zu stark auf einzelne Trainingsdaten anpasst.\\

Die Parameter \enquote{adam\_beta1}, \enquote{adam\_beta2} und \enquote{adam\_epsilon} beschreiben die Parameter des genutzten AdamW-Optimierers, eingestellt in dem Parameter \enquote{optim}.
Diese Werte folgen ebenso den Werten aus dem Artikel zu den LLaMA Modellen von \citet{llama}.
AdamW ist in verschiedenen Implementierungen vorhanden, hier wurde die neuste Implementierung der PyTorch Bibliothek genutzt.
Die Funktionsweise des AdamW Optimierers ist genauer im Artikel von \citet{adamw} beschrieben.\\

Der Parameter \enquote{max\_grad\_norm} beschreibt die maximale Norm des Gradienten, welche durch die Gewichte des Modells nicht überschritten werden darf.
Eine andere Bezeichnungen genutzt von der Bibliothek DeepSpeed oder in den Artikeln zu den LLaMA Modellen ist \enquote{gradient clipping}.
Sie limitiert die Größe des Gradienten, welche in großen neuronalen Netzwerken explosionsartig ansteigen kann.
Zu große Gradienten führen zu einem schlechteren Trainingsergebnis und zu starker Anpassung von Gewichten, welches wiederum zur Oszillation um ein Minimum führt.
Zur Limitierung der Gradienten wird die L2 Norm des Gradienten berechnet.
Übersteigt diese den angegebenen Maximalwert, wird der Gradient herunterskaliert, bis er die Maximalnorm nicht mehr überschreitet.\\

Der Parameter \enquote{num\_train\_epochs} beschreibt die Anzahl der Epochen, welche trainiert werden sollen.
Eine Epoche repräsentiert hierbei eine Durchführung des Trainingsdatensatzes.
Mehrere Epochen können zu besseren Ergebnissen führen, insbesondere bei kleineren Datensätzen, damit das Modell sich besser an die Trainingsdaten anpassen kann.
Zu viele Epochen führen zum Overfitting.
Im Rahmen dieser Arbeit wurde das Llama 2 7B Modell auf eine und 3 Epochen trainiert.
Höhere Epochen führten zu Problemen während des Trainings, welche genauer in \cref{sec:problem-training} beschrieben sind.\\

Mit Hilfe der Parameter \enquote{lr\_scheduler} und \enquote{warmup\_steps} ist die Umsetzung einer Aufwärmphase des Trainings möglich.
Die Aufwärmphase beschreibt den Beginn des Trainings, bei dem die Lernrate der Funktion \enquote{lr\_scheduler} folgend innerhalb der ersten \enquote{warmup\_steps} Iterationen von 0 auf den gewünschten Wert ansteigt.
Diese Methodik verhindert ein zu schnelles Anpassen der Gewichte des Modells auf kleine Formatierungsdetails des Trainingsdatensatzes.
Insbesondere bei untrainierten Modellen werden im Laufe des Trainings diese erlernten Fehler wieder korrigiert, führen aber zu einer längeren Trainingszeit und schlechterem Ergebnis ohne Aufwärmphase.
Da in diesem Fall ein bereits vortrainiertes Modell genutzt wird, kann diese Aufwärmphase übersprungen werden.\\

Die Parameter \enquote{save\_strategy}, \enquote{save\_steps} und \enquote{save\_total\_limit} beschreiben, wie oft und wie viele Modelle während des Trainings gespeichert werden sollen.
In diesem Fall wird alle 100 Iterationen ein Modell gespeichert, wobei maximal 1 Kontrollpunkt gleichzeitig existiert.
Durch Kontrollpunkte können Trainingsdurchläufe nach einem Abbruch oder einer Fehlermeldung wieder aufgenommen werden.
Insbesondere bei längerem Training kann potentiell eine maximale Laufzeit der Skripte erreicht werden.
Durch die Kontrollpunkte kann das Training an dieser Stelle wieder aufgenommen werden.
Das Rechenzentrum sieht für die Ausführung von Skripten eine maximale Laufzeit von 2 Tagen vor.
Sollte ein Training länger dauern, könnte diese Grenze überschritten werden.
Der Grund hinter einer Begrenzung ist die gemeinsame Nutzung des Rechenzentrums.
Durch regelmäßige Unterbrechung von Skripten können andere sich in der Warteschlange befindende Skripte zwischengeschoben werden.
Dadurch beträgt die Wartezeit in der Regel maximal 2 Tage.
Durch die Einführung von Kontrollpunkten während des Trainings bedeutet eine Unterbrechung keinen Fortschrittsrückfall.\\

\enquote{no\_cuda} beschreibt die Durchführung des Trainings ohne die Nutzung einer \ac{gpu}.
Diese Einstellung dient zu Testzwecken um Testskripte auf Systemen starten zu können, welche nicht die geforderte Anzahl an \ac{gpu}s besitzen.\\

Die Verwendung des Parameters \enquote{seed} beeinflusst die Zufälligkeit des Trainings.
Während des Trainings werden einige Zufallsvariablen genutzt um Werte zu initialisieren.
Im Kontext des Fortführenden Trainings ist die Auswirkung hier jedoch nicht relevant.
Da der Trainingsskript jedoch auch für untrainierte Modelle genutzt werden kann, wird hier ein fester Wert gesetzt.
Zufallszahlengeneratoren erstellen mit Hilfe eines Seeds zufälligverteilte, jedoch reproduzierbare Zahlenfolgen.
Modelle mit gleichem Seed und Datensatz sind somit auch identisch.\\

Zur Konfiguration der genutzten Datentypen dient neben dem genannten \enquote{torch\_dtype} Parameter die beiden Parameter \enquote{fp16} und \enquote{bf16}. Während \enquote{torch\_dtype} die Initialisierung des Modells beschreibt, beeinflussen diese Parameter die Datentypen während des Trainings.
Die Zahl 16 steht hierbei für die Anzahl an Bits pro Wert, welches wiederum einen großen Einfluss auf die Speicheranforderungen und Genauigkeit des Modells hat.
Datentypen sollten bei der Fortsetzung eines vortrainierten Modells nicht geändert werden, da eine Umstellung schnell zu fehlerhaften Verhalten führen kann.
Die Llama 2 Modelle wurden mit dem Datentyp \enquote{bf16} (ausgeschrieben BFloat16) trainiert.
Dieser Datentyp ist nur auf bestimmten \ac{gpu} Architekturen verfügbar.
Ein Datum im BFloat16 Format verfügt über eine kleinere Mantisse (7 Bits) gegenüber einem Float16 Datum (10 Bits Mantisse). Dies führt zu einer geringeren Genauigkeit, verkürzt jedoch die notwendige Zeit bis zur Konvergenz des Modells.
Die verwendeten Grafikkarten V100 verfügen nicht über die notwendige Architektur um BFloat16 Werte zu unterstützen, weshalb das Llama 2 Modell hier in ein Float16 Format umgewandelt wird.
Dies führt zu Problemen während des Trainings und limitierte die maximale Anzahl an Epochen auf 3. Eine genauere Erklärung der Probleme ist in \cref{sec:problem-training} beschrieben.\\

Die Parameter \enquote{half\_precision\_backend} und \enquote{ddp\_backend} beschreiben Architekturen zur Ausführung des Trainings mit Hilfe der Transformers Bibliothek.
Während \enquote{half\_precision\_backend} die genutzte Architektur zur Ausführung von Trainingsschritten mit Float16 Werten setzt, beschreibt \enquote{ddp\_backend} die genutze Kommunikationsarchitektur um Daten zwischen \ac{gpu}s auszutauschen.
Die Bibliothek DeepSpeed arbeitet mit der \enquote{NCCL} Architektur zur Kommunikation weshalb diese hier gesetzt wird.\\

Die DeepSpeed Konfiguration ist separat in einer JSON Datei gespeichert.
Durch das setzen des Parameters \enquote{deepspeed} nutzt die Transformers Bibliothek die DeepSpeed Bibliothek\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 17.8.2023} zur Ausführung des Trainings.
Wichtig bei der Nutzung von DeepSpeed mit Transformers ist eine identische Konfiguration von Trainingsparametern wie zum Beispiel \enquote{batch\_size}, \enquote{gradient\_accumulation\_steps} und \enquote{learning\_rate}.\\

\subsection{Konfiguration des DeepSpeed Trainings}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{fp16}\\
        enabled&auto\\
        loss\_scale&0\\
        loss\_scale\_window&1000\\
        initial\_scale\_power&16\\
        hysteresis&2\\
        min\_loss\_scale&1\\
        \midrule
        \multicolumn{2}{c}{optimizer}\\
        type&AdamW\\
        lr&auto\\
        betas&auto\\
        eps&auto\\
        weight\_decay&auto\\
        \midrule
        \multicolumn{2}{c}{scheduler}\\
        type&WarmupLR\\
        warmup\_min\_lr&auto\\
        warmup\_max\_lr&auto\\
        warmup\_num\_steps&auto\\
        \midrule
        \multicolumn{2}{c}{zero\_optimizations}\\
        stage&2\\
        contiguous\_gradients&true\\
        overlap\_comm&true\\
        reduce\_scatter&true\\
        reduce\_bucket\_size&2e8\\
        allgather\_bucket\_size&2e8\\
        \midrule
        \multicolumn{2}{c}{offload\_optimizer}\\
        device&cpu\\
        pin\_memory&true\\
        \midrule
        gradient\_clipping&1\\
        steps\_per\_print&500\\
        wall\_clock\_breakdown&false\\
        train\_micro\_batch\_size\_per\_gpu&auto\\
    \end{tabular}
    \caption{DeepSpeed Konfiguration}\label{tab:deepspeed-config}
\end{table}

Die DeepSpeed Konfiguration ist in \cref{tab:deepspeed-config} dargestellt.
Die hier genutzten Parameter beschreiben ein Training mit Hilfe der ZeRO Stage 2 Optimierung und zusätzlichem CPU Offloading.
Die ZeRO Stage 2 Optimierung beschreibt einen Algorithmus zur Reduktion des Speicherbedarfs während des Trainings.
Dieser Algorithmus wird in \citet{deepspeed} genauer beschrieben.
Zusätzlich zu dieser Optimierung werden Teile der Berechnungen auf die \ac{cpu} ausgelagert um den Speicherbedarf pro \ac{gpu} weiter zu reduzieren.\\

Die Rubrik \enquote{fp16} beschreibt den Umgang mit Float16 Werten während des Trainings.
Hierbei handelt es sich um eine dynamische Skalierung der Fehlerwerte (engl. \enquote{Dynamic Loss Scaling}).
Die Skalierung von Fehlerwerten ist notwendig, da durch die geringere Genauigkeit von Float16 Werten kleinere Größen der Fehlerwerte abgerundet werden und verloren gehen.
Aus diesem Grund werden Fehlerwerte um mehrere Potenzen während der Berechnung skaliert.
Diese Skalierung kann ebenso zum Überlauf über den Wertebereich des Float16 Datentyps führen.
Hier verwendet DeepSpeed eine automatische, dynamische Skalierung der Fehlerwerte, ohne einen Überlauf zu verursachen.
Eine genauere Erklärung der Skalierung von Fehlerwerten lässt sich in dem Artikel von \citet{lossscale} finden.
Der Parameter \enquote{loss\_scale} beschreibt die konstante Skalierung der Fehlerwerte.
Ist er auf $0$ gesetzt, wird eine dynamische Skalierung genutzt.
Der \enquote{loss\_scale\_window} Parameter beschreibt das Werteintervall in dem die dynamische Skalierung stattfindet.
Der Parameter \enquote{initial\_scale\_power} beschreibt die Größe der initialen Skalierung der Fehlerwerte.
Die tatsächliche Skalierung der Fehlerwerte entspricht $2^{initial\_scale\_power}$.
Der Parameter \enquote{hysteresis} beschreibt die minimale Anzahl an Schritten, in denen die Skalierung nicht verändert werden kann.
Der Parameter \enquote{min\_loss\_scale} beschreibt die minimale Skalierung der Fehlerwerte.
Hier entspricht der Wert $1$ keiner Skalierung.\\

Die Rubrik \enquote{optimizer} beschreibt die Parameter des Optimierungsalgorithmus. Wie bereits in den Trainingsparametern beschrieben wird der AdamW Optimierer genutzt.
Die Parameter \enquote{lr}, \enquote{betas}, \enquote{eps} und \enquote{weight\_decay} müssen mit den in den Trainingsparametern gesetzten Werten übereinstimmen.
Aus diesem Grund werden diese Parameter auf \enquote{auto} gesetzt und durch die Transformers Bibliothek vor dem Beginn des Trainings ergänzt.\\

Die Rubrik \enquote{scheduler} beschreibt die Parameter des Lernratenplaners. Der Lernratenplaner ist für die Anpassung der Lernrate während des Trainings verantwortlich.
Er dient in diesem Fall zur Umsetzung der Aufwärmphase. Die Aufwärmphase ist in den Trainingsargumenten deaktiviert, die Bibliothek wird dementsprechend auch diese Phase in der DeepSpeed Konfiguration deaktivieren.\\

Die Rubrik \enquote{zero\_optimizations} beschreibt die Parameter der ZeRO Stage 2 Optimierung.
Die ZeRO Optimierung wird in \citet{deepspeed} genauer beschrieben.
Der Parameter \enquote{contiguous\_gradients} beschreibt die Übertragung der Gradienten in einen kontinuierlichen Speicherbereich, während ihrer Berechnung.
Diese Übertragung verhindert Speicherfragmentierung während der Backpropagation.
Mit Hilfe des Parameters \enquote{overlap\_comm} wird versucht, die Reduzierung der Gradienten während der Berechnung der Backpropagation durchzuführen, um eine schneller Gesamtberechnung zu ermöglichen.
\enquote{reduce\_scatter} beschreibt die Benutzung einer speziellen Reduzierungsmethode \enquote{Reduce Scatter} um Gradienten zu mitteln.
In Kombination mit den Parametern \enquote{reduce\_bucket\_size} und \enquote{allgather\_bucket\_size} wird die maximale Anzahl an Gradienten festgelegt, die in einem Schritt reduziert werden.
Diese Option reduziert die notwendige Menge an Speicher während des Trainings deutlich.
Mit Hilfe der Rubrik \enquote{offload\_optimizer} wird der Zustand  und die Berechnung des Optimierers auf die CPU ausgelagert.
Optional kann dies für sehr große Modelle ebenso auf eine NVMe SSD geschehen.
Mit Hilfe der Einstellung \enquote{pin\_memory} wird der notwendige Speicher für Berechnung auf der CPU reserviert.
Dies führt zu besserer Leistung, jedoch zusätzlicher Speicheranforderung.\\

Wie bereits in den Trainingsargumenten durch den Parameter \enquote{max\_grad\_norm} beschrieben, wird die maximale Größe des Gradienten durch den Parameter \enquote{gradient\_clipping} festgelegt.
Eine Automatische Übertragung der Konfiguration der Trainingsargumente ist in diesem Fall nicht möglich.
Die Werte müssen dennoch übereinstimmen.\\

Die Parameter \enquote{steps\_per\_print} und \enquote{wall\_clock\_breakdown} beschreiben die Ausgabe von Informationen während des Trainings.
Zusätzlich zu der Ausgabe der Transformers Bibliothek werden hier weitere Informationen der DeepSpeed Bibliothek alle 500 Iterationen ausgegeben.
Mit Hilfe des Parameters \enquote{wall\_clock\_breakdown} wird zusätzlich die Messung der verlaufenen Zeit für jede Phase einer Iteration ausgegeben.\\

Der Parameter \enquote{train\_micro\_batch\_size\_per\_gpu} beschreibt die Anzahl der Batches pro GPU.
Dieser Wert wird von den Trainingsargumenten übernommen.\\

\subsection{Probleme während des Trainings}\label{sec:problem-training}




%
% - genertierter Text ohne Stopzeichen: Stopzeichen händisch nach NewLine (double Newline?)
% - requirements txt erstellen
% - DeepSpeed Config mit https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters+
% - Nutzung von ZeRO Stage2 optimization
% - llama2 7b genutzt
% - dataclasses um Datensatz einzulesen
% - Kleinere Block\_size für Loss Scaling
% - BF16 nur auf AMP \ac{gpu} Architektur möglich (hier nicht)
% - Deutlich mehr RAM erforderlich (180GB?)
% - Leistungen bei 11s/it von 3600 Iteration = 11h Training
% - Evaluation aller 50 Iterations, Saves aller 100 Iterations
% - Stage2/3 ohne CPU Offloading führt zu OOM
% - 3 Epochen = 14s/it 1800 = 7h Training (256 Block\_size)
% - Deepspeed Launcher mit 3 \ac{gpu}s (32GB v100, 1 Maschine)
% - Weitere Hyperparameter aus Llama 2 Modell
% - Nutzung von Epub anstelle Word Datei da bessere Konvertierung
% - keine nutung von chat Version (verlernt Human Reinforcment)