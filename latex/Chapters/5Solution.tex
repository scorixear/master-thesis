%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

Die Lösung gliedert sich in sechs Schritte, die für die Durchführung des Trainings notwendig sind.
Im Folgenden werden diese Schritte näher erläutert. Für jeden Schritt sind die verwendeten Techniken und Bibliotheken separat aufgeführt.
Die Schritte sind wie folgt gegliedert:
\begin{enumerate}
    \item Laden des Modells
    \item Konvertierung des Modells in ein kompatibles Format
    \item Modelltraining
    \item Erstellung von Antworten auf dem Evaluierungsdatensatz
    \item Bewertung der erzeugten Antworten
    \item Auswertung basierend auf den Bewertungen
\end{enumerate}

\section{Herunterladen des Modells}
Die LLaMA-Modelle werden unter einer nicht-kommerziellen Lizenz für Forschungszwecke zur Verfügung gestellt. Der Zugang zu den Modellen wird im Einzelfall auf Anfrage gewährt. Diese Anfrage wurde im Rahmen dieser Arbeit gestellt und bestätigt.
Anschließend kann ein von den Autoren vorbereitetes Skript verwendet werden, um die trainierten Parameter des Modells herunterzuladen.
Das Skript benötigt eine explizite \ac{url}, die nach einer vorgegebenen Zeit von einer Woche nach Freigabe der Modelle ungültig wird. Aus diesem Grund ist diese \ac{url} nicht im Skript enthalten.

\section{Umwandlung des Modells in ein kompatibles Format}
Das Training des Modells basiert auf zwei grundlegenden Bibliotheken: der Transformers Bibliothek von Huggingface und der DeepSpeed Bibliothek von Microsoft.
Diese Bibliotheken, insbesondere Transformers, erleichtern den Entwicklungsprozess enorm und bieten ein hohes Maß an Abstraktion. Allerdings können Sie nur mit Modellen arbeiten, die in einer für die Transformers-Bibliothek verständlichen Form vorliegen.
Aus diesem Grund wird das heruntergeladene Modell mit Hilfe eines Huggingface-Skripts in diese kompatible Form umgewandelt.
Während dieser Konvertierung muss das Modell vollständig in den \ac{ram} des ausführenden Rechners geladen werden. Für das Modell LLaMA 7B sind dafür mehr als 14 GB RAM erforderlich.
Die Konvertierung dauerte auf einem Rechner mit 32 GB RAM und einem AMD Ryzen 7 5800X Prozessor ca. 30 Minuten.

\section{Training des Modells}
Um das LLaMA-Modell zu trainieren, wird die Transformers Bibliothek von Huggingface verwendet\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 16.8.2023}.
Das in Python geschriebene Trainingsskript basiert auf dem Beispielskript zum Training von kausalen Sprachmodellen aus dem Huggingface Github Repository\footnote{\url{https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py} abgerufen am 16.8.2023} und wurde teilweise an die Anforderungen des hier durchgeführten Trainings angepasst.
Die Konfiguration des Trainings gliedert sich in vier Bereiche: Einstellungen für das Modell, Einstellungen für die Trainingsdaten, Einstellungen für das Training selbst und Einstellungen für DeepSpeed.
Im Folgenden werden diese Einstellungen näher erläutert.
\subsection{Konfiguration des Modells}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        model\_name & meta-llama/Llama-2-7b-hf \\
        cache\_dir & ./cache \\
        use\_fast\_tokenizer & false \\
        model\_revision & main \\
        use\_auth\_token & true \\
        hugging\_token & \textit{Huggingface Token} \\
        torch\_dtype & auto \\
        low\_cpu\_mem\_usage & false \\
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration des Modells}\label{tab:model-config}
\end{table}
Die zur Auswahl und Konfiguration des Modells verwendeten Parameter sind in der Tabelle \ref{tab:model-config} aufgelistet.\\

Der Parameter \enquote{model\_name} entspricht einer Modellauswahl und kann entweder ein relativer Pfad zu einem lokalen Modell oder ein Modellname sein.
Der Modellname wurde hier auf das Modell Llama 2 7B gesetzt und verweist auf das von Huggingface gehostete Modell in einem mit der Transformers-Bibliothek kompatiblen Format.
Llama 2 wurde im Juli 2023 von Meta AI veröffentlicht und stellt eine generelle Verbesserung der ursprünglichen LLaMA 1 Modelle dar.
Neben der nun auf 4096 Tokens erweiterten Kontextlänge stellt Meta AI Llama 2 auch in einer Chat-Version zur Verfügung.
Die Chat-Version wurde zusätzlich mit Hilfe von Reinforcement Learning aus menschlichem Feedback trainiert und ermöglicht so eine einfachere Nutzung der vortrainierten Modelle im Kontext eines Chatbots.
Diese Chat-Modelle wurden jedoch nicht zum Training verwendet, da davon ausgegangen wird, dass dieses zusätzliche Training durch das hier durchgeführte Continual Pretraining überschrieben wird.
Außerdem ermöglicht Huggingface nun die Nutzung der Llama 2 Modelle ohne Download und Konvertierung, so dass die ersten beiden Schritte entfallen.
Die Llama 2 Modelle wurden unter anderem im Artikel von \citet{llama2} vorgestellt.\\

Der Parameter \enquote{cache\_dir} beschreibt den Speicherort des heruntergeladenen Modells.
Dies ermöglicht ein wiederholtes Trainieren des Modells ohne erneuten Download.
Die heruntergeladenen Modelle werden durch das Training nicht überschrieben und stellen somit den Grundzustand des Modells dar (im Folgenden auch als Llama2-0e für \enquote{0 Epochen trainiert} bezeichnet).\\

\enquote{use\_fast\_tokenizer} konfiguriert die Verwendung einer schnelleren Version des Tokenizers zur Konvertierung der Datensätze.
Diese Option ist optional und wurde hier nicht verwendet.\\

\enquote{model\_revision} beschreibt die zu verwendende Version des Modells.
Hier wurde die aktuellste Version verwendet, die durch den Wert \enquote{main} repräsentiert wird.\\

\enquote{use\_auth\_token} und \enquote{hugging\_token} beschreiben die Verwendung eines Authentifizierungs-Tokens, um Modelle von Huggingface herunterzuladen.
Diese Authentifizierung ist notwendig, da auch die Llama 2 Modelle unter der gleichen Lizenz wie LLaMA 1 stehen und nur auf Anfrage zur Verfügung gestellt werden.
Um die Modelle mit beschränktem Zugang herunterzuladen, wurde ein Huggingface-Account erstellt, die Anfrage an Meta AI zur Nutzung der Llama 2 Modelle gestellt und bestätigt und ein Authentifizierungstoken generiert.\\

Der Parameter \enquote{torch\_dtype} beschreibt den Datentyp, der für die Darstellung der Modellparameter verwendet wird.
Hier stehen Float32, Float16 und BFloat16 zur Verfügung.
Vorgefertigte Modelle wurden ursprünglich mit einem Datentyp erstellt und müssen in einen anderen Datentyp konvertiert werden, wenn der \enquote{torch\_dtype} nicht übereinstimmt.
Diese Konvertierung ist rechenintensiv und kann dazu führen, dass Modelle fehlerhaft trainiert werden oder mehr Rechenleistung während der Inferenz und des Trainings benötigen.
Aus diesem Grund wurde hier der Parameter \enquote{auto} verwendet, der den Datentyp des Modells automatisch erkennt und benutzt.\\

\enquote{low\_cpu\_mem\_usage} beschreibt ein Verfahren der Bibliothek Transformers zum Laden großer Modelle auf Systemen mit wenig Arbeitsspeicher.
Dabei werden die Modelle in mehreren Schritten in den Arbeitsspeicher geladen und anschließend in den \ac{gpu}-Speicher übertragen.
Dieses Verfahren reduziert die Menge des notwendigen Arbeitsspeichers, erhöht aber die Zeit, die zum Laden des Modells benötigt wird.
Da im vorliegenden Fall genügend Arbeitsspeicher zur Verfügung stand, wurde auf dieses Verfahren verzichtet.

\subsection{Konfiguration der Trainingsdaten}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        train\_file & ./input/health\_information\_systems\_epub.md \\
        max\_train\_samples & None \\
        overwrite\_cache & false \\
        block\_size & 1024 \\
        validation\_split\_percentage & 5 \\
        preprocessing\_num\_workers & 1 \\
        keep\_linebreaks & true \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration der Trainingsdaten}\label{tab:data-config}
\end{table}
Für das Training des Modells Llama 2 wurde das Buch \enquote{Health Information Systems} von \citet{bb} im epub-Format in das Markdown-Format konvertiert.
Die notwendigen Änderungen am Text sind in \cref{sec: datenkuration} beschrieben.
Um diese Markdown-Datei in eine für das Modell verständliche Form umzuwandeln, wird die Bibliothek datasets\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 16.8.2023} von Huggingface verwendet.
Sie ermöglicht das Laden der Textdatei, die Umwandlung in Tokens und die Aufteilung in Blöcke.
Die Parameter zur Auswahl und Konfiguration der Trainingsdaten sind in der Tabelle \ref{tab:data-config} aufgelistet.\\

Der Parameter \enquote{train\_file} beschreibt den Pfad zur Trainingsdatei.
Diese Trainingsdatei wird einmal eingelesen und je nach Anzahl der unter \cref{subsec:config-training} beschriebenen Epochen mehrfach verwendet.\\

\enquote{max\_train\_samples} beschreibt die maximale Anzahl von Blöcken, die aus der Trainingsdatei gelesen werden sollen.
Zu Testzwecken kann hier eine geringere Anzahl an Blöcken verwendet werden, um die Konfiguration des Modells zu testen.
Im finalen Training wurde dieser Parameter auf \enquote{None} gesetzt, um alle Blöcke zu verwenden.\\

Der Parameter \enquote{overwrite\_cache} beschreibt, ob der Text erneut in Tokens umgewandelt werden soll.
Wenn sich der Text geändert hat, kann das Skript hier die nun tokenisierte Textdatei überschreiben.\\

Wie bereits erwähnt, wird der Text in Blöcke aufgeteilt.
Jeder Block wird vom Modell vollständig und gleichzeitig gelesen.
Die maximale Größe eines Blocks ist durch das Modell begrenzt und liegt bei Llama 2 bei 4096 Tokens.
Standardmäßig verwenden Modelle jedoch eine Blockgröße von 1024 Tokens, weshalb hier diese Größe angenommen wird, wenn keine weiteren Angaben gemacht werden.
Größere Blöcke führen zu einer schnelleren Verarbeitung des Textes und können zu einem besseren Verständnis der Zusammenhänge führen, da ein größerer Kontext betrachtet wird.
Allerdings steigt mit der Größe der Blöcke auch die Fehleranfälligkeit, so dass teilweise auch kleinere Blöcke verwendet werden müssen, um ein Training erfolgreich durchzuführen.
Probleme beim Training sind unter \cref{sec: problem-training} beschrieben.
Die Blockgröße kann mit dem Parameter \enquote{block\_size} angepasst werden.\\

Der Parameter \enquote{validation\_split\_percentage} beschreibt den Anteil der Daten, der für die Validierung genutzt werden soll.
Hier werden \SI{5}{\percent} der Daten für die Validierung verwendet.
Die Validierung liefert während des Trainings Informationen über die tatsächliche Leistung des Modells im Vergleich zum ungesehenen Text und dient dazu, den Fortschritt des Modells zu messen.\\

\enquote{preprocessing\_num\_workers} beschreibt die Anzahl der Prozesse, die für die Umwandlung der Textdatei in Tokens verwendet werden sollen.
Bei sehr großen Datenmengen ist die Umwandlung von Textdateien in Tokens eine sehr umfangreiche Aufgabe.
Um diesen Vorgang zu beschleunigen, können mehrere Prozesse die Textdateien parallel übersetzen.
In diesem Fall ist die Datenmenge jedoch klein genug, um die Aufgabe mit nur einem Prozess zu erledigen.\\

Der Parameter \enquote{keep\_linebreaks} beschreibt, ob Zeilenumbrüche in der Textdatei erhalten bleiben sollen.
In einigen Fällen können Textdateien viele Zeilenumbrüche enthalten, die der Formatierung des Textes dienen, aber dem Modell keine Informationen liefern oder es dazu veranlassen, diese Zeilenumbrüche zu imitieren.
Aus diesem Grund können Zeilenumbrüche optional entfernt werden.
Aufgrund der zuvor durchgeführten Datenkuration ist dies jedoch nicht notwendig, weshalb dieser Parameter auf \enquote{true} gesetzt wird.\\

\subsection{Konfiguration des Trainings}\label{subsec:config-training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        output\_dir & ./trained/7B-3 \\
        overwrite\_output\_dir & true \\
        do\_train & true \\
        do\_eval & true \\
        per\_device\_train\_batch\_size & 1 \\
        per\_device\_eval\_batch\_size & 1 \\
        evaluation\_strategy & steps \\
        eval\_steps & 50 \\
        learning\_rate & 3e-4 \\
        weight\_decay & 0.1 \\
        optim & adamw\_torch \\
        adam\_beta1 & 0.9 \\
        adam\_beta2 & 0.95 \\
        adam\_epsilon & 1e-5 \\
        max\_grad\_norm & 1.0 \\
        num\_train\_epochs & 3 \\
        lr\_scheduler\_type & cosine \\
        warmup\_steps & 0 \\
        save\_strategy & steps \\
        save\_steps & 100 \\
        save\_total\_limit & 1 \\
        no\_cuda & false \\
        seed & 42 \\
        fp16 & true \\
        bf16 & false \\
        half\_precision\_backend & auto \\
        ddp\_backend & nccl \\
        deepspeed & ./ds\_configs/stage2\_offload.json \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Konfiguration des Trainings}\label{tab:training-config}
\end{table}

Die Konfiguration des Trainings orientiert sich an den \enquote{TrainingArguments}, die ein Teil der Transformers-Bibliothek sind.
Nicht alle Parameter aus der Bibliothek müssen verwendet werden, weshalb hier nur abweichende Parameter vom Standard beschrieben werden.
Eine vollständige Liste der Parameter ist im Anhang unter \cref{app:sec:trainingpaameter} zu finden. Die beschriebenen Parameter sind in \cref{tab:training-config} aufgeführt.

Der Parameter \enquote{output\_dir} definiert den Speicherpfad für die Ergebnisse des Trainings.
Diese umfassen die trainierten Gewichte des Modells, eine abschließende Auswertung der Validierung, den Status des Trainers sowie gegebenenfalls während des Trainings erstellte Kontrollpunkte.

Der Parameter \enquote{overwrite\_output\_dir} legt fest, ob der Ergebnis-Ordner überschrieben werden soll, wenn er bereits existiert.
Diese Option bestimmt auch, ob das Training von einem zuvor erstellten Kontrollpunkt aus fortgesetzt werden soll.
Wenn der Ergebnis-Ordner überschrieben wird, kann von keinem Kontrollpunkt aus fortgefahren werden.

Die Parameter \enquote{do\_train} und \enquote{do\_eval} geben an, ob das Training und die Validierung durchgeführt werden sollen.
Falls keine Validierung durchgeführt wird, wird der Trainingsdatensatz dennoch in Trainings- und Validierungsdatensatz aufgeteilt.

Während des Trainings und der Validierung werden die Fehlerfunktionen von mehreren Blöcken berechnet und danach gemittelt.
Daraufhin wird ein Gradient basierend auf dieser kumulativen Fehlerfunktion berechnet, welcher die Gewichte des Modells anpasst.
Eine detailiertere Beschreibung des Ablaufs findet sich in \cref{subsec:backpropagation}.
Die Anzahl der Blöcke pro \enquote{Batch} (siehe \cref{def:batch}) wird durch die Parameter \enquote{per\_device\_train\_batch\_size} und \enquote{per\_device\_eval\_batch\_size} festgelegt.
Wenn man eine Batch-Größe von 1 und 3 \ac{gpu}s verwendet, erhält man somit 3 Blöcke pro Gradientenberechnung.
Während höhere Batch-Größen die Berechnungszeit reduzieren können, führen sie auch zu ungenaueren Gradienten und erfordern mehr Speicher auf der \ac{gpu}.
Die Batch-Größe ist in diesem Fall auf 1 gesetzt, da die verwendeten Nvidia V100-\ac{gpu}s nur genug Speicher für eine Batch-Größe von 1 hatten.\\

Die Parameter \enquote{evaluation\_strategy} und \enquote{eval\_steps} legen fest, in welchen Intervallen die Validierung durchgeführt werden soll.
In diesem Szenario wird die Validierung alle 50 Iterationen durchgeführt.
Eine Iteration bezieht sich auf die Berechnung eines Gradienten.\\

Die Lernrate des Modells wird durch den Parameter \enquote{learning\_rate} beschrieben.
Dieser Parameter bestimmt, in welchem Ausmaß die Gewichte des Modells angepasst werden.
Eine sehr hohe Lernrate kann dazu führen, dass das Modell nicht konvergiert, während eine sehr niedrige Lernrate zu langen Trainingszeiten führt.
Die Lernrate wurde aus dem Artikel über LLaMA-Modelle von \citet{llama} übernommen.\\

In dem Artikel zu den LLaMA-Modellen von \citet{llama} wird ebenfalls ein Gewichtsabnahme-Wert von $0,1$ verwendet, der auch bei diesem Training mithilfe des Parameters \enquote{weight\_decay} eingestellt wurde.
Die Gewichtsverminderung führt zu einer kontinuierlichen Verringerung der Gewichte des Modells. Dadurch soll verhindert werden, dass es sich zu stark an einzelne Trainingsdaten anpasst.\\

Die Parameter \enquote{adam\_beta1}, \enquote{adam\_beta2} und \enquote{adam\_epsilon} beschreiben die Parameter des verwendeten AdamW-Optimierers, der im Parameter \enquote{optim} eingestellt ist.
Diese Werte entsprechen ebenfalls den Werten, die im Artikel zu den LLaMA-Modellen von \citet{llama} angegeben sind.
AdamW ist in verschiedenen Implementierungen verfügbar. Hier wurde die neueste Implementierung der PyTorch-Bibliothek verwendet.
Die Funktionsweise des AdamW-Optimierers ist im Artikel von \citet{adamw} genauer beschrieben.\\

Der Parameter \enquote{max\_grad\_norm} beschreibt die maximale Norm des Gradienten, die durch die Gewichte des Modells nicht überschritten werden darf. 
Eine weitere Bezeichnung, die in der Bibliothek DeepSpeed oder in Artikeln zu LLaMA-Modellen genutzt wird, ist \enquote{gradient clipping}.
Er begrenzt die Größe des Gradienten, der in großen neuronalen Netzen explosionsartig ansteigen kann.
Zu große Gradienten führen zu schlechteren Trainingsergebnissen und zu einer zu starken Anpassung der Gewichte, was wiederum zu einer Oszillation um ein Minimum führt.
Um die Gradienten zu begrenzen, wird die L2-Norm des Gradienten berechnet.
Wenn diese Norm den angegebenen Maximalwert überschreitet, wird der Gradient herunterskaliert, bis er die Maximalnorm nicht mehr überschreitet.\\

Der Parameter \enquote{num\_train\_epochs} gibt an, wie viele Epochen trainiert werden sollen.
Eine Epoche umfasst eine vollständige Durchführung des Trainingsdatensatzes.
Mehrere Epochen können zu besseren Ergebnissen führen, insbesondere bei kleineren Datensätzen, weil das Modell sich besser an die Trainingsdaten anpassen kann.
Zu viele Epochen führen zu Overfitting.
In dieser Arbeit wurde das Llama 2 7B Modell für eine und drei Epochen trainiert.
Bei höheren Epochen kam es während des Trainings zu Problemen, die in \cref{sec:problem-training} genauer beschrieben sind.

Durch die Anwendung der Parameter \enquote{lr\_scheduler} und \enquote{warmup\_steps} kann eine Aufwärmphase des Trainings realisiert werden.
Die Aufwärmphase bezeichnet den Start des Trainings, in dem die Lernrate der \enquote{lr\_scheduler}-Funktion innerhalb der ersten \enquote{warmup\_steps} Iterationen schrittweise von 0 auf den gewünschten Wert erhöht wird.
Dieser Ansatz verhindert eine zu schnelle Anpassung der Modellgewichte an spezielle Details der Trainingsdaten.
Vor allem bei untrainierten Modellen werden während des Trainings diese erlernten Fehler wieder korrigiert. Dies führt jedoch ohne eine Aufwärmphase zu längeren Trainingszeiten und schlechteren Ergebnissen.
In diesem Fall kann die Aufwärmphase übersprungen werden, da ein bereits vortrainiertes Modell genutzt wird.

Die Parameter \enquote{save\_strategy}, \enquote{save\_steps} und \enquote{save\_total\_limit} beschreiben, wie oft und in welchem Abstand Modelle während des Trainings gespeichert werden sollen.
Hier wird alle 100 Iterationen das Modell gespeichert, wobei maximal 1 Kontrollpunkt gleichzeitig existiert.
Kontrollpunkte ermöglichen es, abgebrochene oder fehlgeschlagene Trainingsläufe wieder aufzunehmen.
Bei längerem Training kann die maximale Laufzeit der Skripte erreicht werden.
Eine Wiederaufnahme des Trainings an diesem Punkt ist dann möglich.
Das Rechenzentrum beschränkt die maximale Laufzeit von Skripten auf 2 Tage.
Wenn das Training länger als 2 Tage dauert, kann diese Grenze überschritten werden.
Die Begrenzung ist notwendig aufgrund der gemeinsamen Nutzung des Rechenzentrums.
Durch die regelmäßige Unterbrechung von Skripten können andere Skripte in der Warteschlange zwischengeschoben werden.
Zusammen mit der Begrenzung ergibt sich im Allgemeinen eine Wartezeit von maximal 2 Tagen.
Durch die Einführung von Kontrollpunkten im Training führt eine Unterbrechung nicht zu einem Rückschlag im Fortschritt.

Der Begriff \enquote{no\_cuda} beschreibt die Durchführung des Trainings ohne die Verwendung einer \ac{gpu}.
Diese Einstellung wird für Testzwecke verwendet, damit Testskripte auf Systemen ausgeführt werden können, die nicht über die erforderliche Anzahl von \ac{gpu}s verfügen.

Die Verwendung des Parameters \enquote{seed} beeinflusst die Zufälligkeit des Trainingsprozesses.
Während des Trainingsprozesses werden einige Zufallsvariablen verwendet, um Werte zu initialisieren.
Jedoch ist die Auswirkung im Kontext des Fortführenden Trainings irrelevant.
Es wird jedoch ein fester Wert gesetzt, da das Trainingsskript auch für untrainierte Modelle genutzt werden kann.
Mit Hilfe eines Seeds erstellen Zufallszahlengeneratoren zufällig verteilte, jedoch reproduzierbare Zahlenfolgen.
Daher sind Modelle mit gleichem Seed und Datensatz identisch.\\

Zur Konfiguration der verwendeten Datentypen stehen neben dem oben genannten \enquote{torch\_dtype} die beiden Parameter \enquote{fp16} und \enquote{bf16}. Während \enquote{torch\_dtype} die Initialisierung des Modells beschreibt, beeinflussen diese Parameter die Datentypen während des Trainings.
Die Zahl 16 steht hier für die Anzahl der Bits pro Wert, was wiederum einen großen Einfluss auf den Speicherbedarf und die Genauigkeit des Modells hat.
Datentypen sollten nicht geändert werden, wenn ein vortrainiertes Modell weiter trainiert wird, da eine Änderung schnell zu Fehlverhalten führen kann.
Die Llama 2 Modelle wurden mit dem Datentyp \enquote{bf16} (ausgeschrieben BFloat16) trainiert.
Dieser Datentyp steht nur auf bestimmten \ac{gpu}-Architekturen zur Verfügung.
Ein Datum im BFloat16-Format hat eine kleinere Mantisse (7 Bits) als ein Float16-Datum (10 Bits Mantisse). Dies führt zu einer geringeren Genauigkeit, verkürzt jedoch die Zeit, die für die Konvergenz des Modells benötigt wird.
Die verwendeten V100 Grafikkarten haben nicht die notwendige Architektur um BFloat16 Werte zu unterstützen, daher wird das Llama 2 Modell hier in ein Float16 Format konvertiert.
Dies führt zu Problemen während des Trainings und beschränkt die maximale Anzahl der Epochen auf 3. Eine genauere Erklärung der Probleme ist in \cref{sec:problem-training} beschrieben.\\

Die Parameter \enquote{half\_precision\_backend} und \enquote{ddp\_backend} beschreiben Architekturen für die Ausführung des Trainings mit Hilfe der Bibliothek Transformers.
Während \enquote{half\_precision\_backend} die Architektur für die Ausführung von Trainingsschritten mit Float16-Werten festlegt, beschreibt \enquote{ddp\_backend} die Kommunikationsarchitektur für den Datenaustausch zwischen \ac{gpu}s.
Die DeepSpeed Bibliothek verwendet die \enquote{NCCL} Architektur, daher wird diese hier eingestellt.\\

Die DeepSpeed Konfiguration wird separat in einer JSON Datei gespeichert.
Durch Setzen des Parameters \enquote{deepspeed} verwendet die Transformers Bibliothek die DeepSpeed Bibliothek\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 17.8.2023} zur Ausführung des Trainings.
Wichtig bei der Verwendung von DeepSpeed mit Transformers ist eine identische Konfiguration der Trainingsparameter, wie zum Beispiel \enquote{batch\_size}, \enquote{gradient\_accumulation\_steps} und \enquote{learning\_rate}.\\

\subsection{Konfiguration des DeepSpeed Trainings}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{fp16}\\
        enabled&auto\\
        loss\_scale&0\\
        loss\_scale\_window&1000\\
        initial\_scale\_power&16\\
        hysteresis&2\\
        min\_loss\_scale&1\\
        \midrule
        \multicolumn{2}{c}{optimizer}\\
        type&AdamW\\
        lr&auto\\
        betas&auto\\
        eps&auto\\
        weight\_decay&auto\\
        \midrule
        \multicolumn{2}{c}{scheduler}\\
        type&WarmupLR\\
        warmup\_min\_lr&auto\\
        warmup\_max\_lr&auto\\
        warmup\_num\_steps&auto\\
        \midrule
        \multicolumn{2}{c}{zero\_optimizations}\\
        stage&2\\
        contiguous\_gradients&true\\
        overlap\_comm&true\\
        reduce\_scatter&true\\
        reduce\_bucket\_size&2e8\\
        allgather\_bucket\_size&2e8\\
        \midrule
        \multicolumn{2}{c}{offload\_optimizer}\\
        device&cpu\\
        pin\_memory&true\\
        \midrule
        gradient\_clipping&1\\
        steps\_per\_print&500\\
        wall\_clock\_breakdown&false\\
        train\_micro\_batch\_size\_per\_gpu&auto\\
    \end{tabular}
    \caption{DeepSpeed Konfiguration}\label{tab:deepspeed-config}
\end{table}

Die DeepSpeed Konfiguration ist in \cref{tab:deepspeed-config} beschrieben.
Die hier verwendeten Parameter beschreiben ein Training mit Hilfe der ZeRO Stage 2 Optimierung und zusätzlichem CPU Offloading.
Die ZeRO Stage 2 Optimierung beschreibt einen Algorithmus zur Reduzierung des Speicherbedarfs während des Trainings.
Dieser Algorithmus ist in \citet{deepspeed} genauer beschrieben.
Zusätzlich zu dieser Optimierung werden Teile der Berechnungen auf die \ac{cpu} ausgelagert, um den Speicherbedarf pro \ac{gpu} weiter zu reduzieren.\\

Der Abschnitt \enquote{fp16} beschreibt den Umgang mit Float16-Werten während des Trainings.
Dabei handelt es sich um eine dynamische Skalierung der Fehlerwerte (engl. \enquote{Dynamic Loss Scaling}).
Die Skalierung der Fehlerwerte ist notwendig, da aufgrund der geringeren Genauigkeit von Float16-Werten kleinere Werte der Fehlerwerte gerundet werden und verloren gehen.
Aus diesem Grund werden die Fehlerwerte bei der Berechnung um mehrere Potenzen skaliert.
Diese Skalierung kann auch zu einem Überlauf über den Wertebereich des Float16 Datentyps führen.
DeepSpeed verwendet hier eine automatische, dynamische Skalierung der Fehlerwerte, ohne einen Überlauf zu verursachen.
Eine genauere Erklärung der Skalierung von Fehlerwerten findet sich im Artikel von \citet{lossscale}.
Der Parameter \enquote{loss\_scale} beschreibt die konstante Skalierung der Fehlerwerte.
Ist er auf $0$ gesetzt, wird eine dynamische Skalierung verwendet.
Der Parameter \enquote{loss\_scale\_window} beschreibt das Werteintervall, in dem die dynamische Skalierung erfolgt.
Der Parameter \enquote{initial\_scale\_power} beschreibt die Größe der initialen Skalierung der Fehlerwerte.
Die tatsächliche Skalierung der Fehlerwerte entspricht $2^{initial\_scale\_power}$.
Der Parameter \enquote{hysteresis} beschreibt die minimale Anzahl von Schritten, in denen die Skalierung nicht verändert werden kann.
Der Parameter \enquote{min\_loss\_scale} beschreibt die minimale Skalierung der Fehlerwerte.
Hier entspricht der Wert $1$ keiner Skalierung.\\

Der Abschnitt \enquote{optimizer} beschreibt die Parameter des Optimierungsalgorithmus. Wie bereits bei den Trainingsparametern beschrieben, wird der AdamW-Optimierer verwendet.
Die Parameter \enquote{lr}, \enquote{betas}, \enquote{eps} und \enquote{weight\_decay} müssen mit den in den Trainingsparametern eingestellten Werten übereinstimmen.
Aus diesem Grund werden diese Parameter vor Beginn des Trainings auf \enquote{auto} gesetzt und durch die Transformer-Bibliothek ergänzt.\\

Der Abschnitt \enquote{scheduler} beschreibt die Parameter des Lernratenplaners. Der Lernratenplaner ist für die Anpassung der Lernrate während des Trainings verantwortlich.
Er dient in diesem Fall zur Durchführung der Aufwärmphase. Ist die Aufwärmphase in den Trainingsargumenten deaktiviert, so wird die Bibliothek diese Phase auch in der DeepSpeed-Konfiguration deaktivieren.\\

Der Abschnitt \enquote{zero\_optimizations} beschreibt die Parameter der ZeRO Stage 2 Optimierung.
Die ZeRO Optimierung wird unter \citet{deepspeed} genauer beschrieben.
Der Parameter \enquote{contiguous\_gradients} beschreibt die Auslagerung der Gradienten in einen zusammenhängenden Speicherbereich während der Berechnung.
Dadurch wird eine Fragmentierung des Speichers während der Backpropagation vermieden.
Mit Hilfe des Parameters \enquote{overlap\_comm} wird versucht, die Gradienten während der Berechnung der Backpropagation zu reduzieren, um eine schnellere Gesamtberechnung zu ermöglichen.
\enquote{reduce\_scatter} beschreibt die Verwendung einer speziellen Reduktionsmethode \enquote{Reduce Scatter} zur Mittelung von Gradienten.
In Kombination mit den Parametern \enquote{reduce\_bucket\_size} und \enquote{allgather\_bucket\_size} wird die maximale Anzahl der in einem Schritt zu reduzierenden Gradienten festgelegt.
Diese Option reduziert den Speicherbedarf während des Trainings erheblich.
Mit Hilfe der Option \enquote{offload\_optimizer} wird der Zustand und die Berechnung des Optimierers auf die CPU ausgelagert.
Optional kann dies für sehr große Modelle auch auf eine NVMe SSD erfolgen.
Mit Hilfe der Einstellung \enquote{pin\_memory} wird der für die Berechnung benötigte Speicher auf der CPU reserviert.
Dies führt zu einer besseren Performance, aber auch zu zusätzlichem Speicherbedarf.\\

Wie bereits bei den Trainingsargumenten durch den Parameter \enquote{max\_grad\_norm} beschrieben, wird die maximale Größe des Gradienten durch den Parameter \enquote{gradient\_clipping} festgelegt.
Eine automatische Übernahme der Konfiguration der Trainingsargumente ist in diesem Fall nicht möglich.
Die Werte müssen jedoch übereinstimmen.\\

Die Parameter \enquote{steps\_per\_print} und \enquote{wall\_clock\_breakdown} beschreiben die Ausgabe von Informationen während des Trainings.
Zusätzlich zur Ausgabe der Transformers-Bibliothek werden alle 500 Iterationen weitere Informationen der DeepSpeed-Bibliothek ausgegeben.
Mit Hilfe des Parameters \enquote{wall\_clock\_breakdown} wird zusätzlich die Messung der verstrichenen Zeit für jede Phase einer Iteration ausgegeben.\\

Der Parameter \enquote{train\_micro\_batch\_size\_per\_gpu} beschreibt die Anzahl der Batches pro GPU.
Dieser Wert wird aus den Trainingsargumenten übernommen.\\

\subsection{Probleme während des Trainings}\label{sec:problem-training}




%
% - genertierter Text ohne Stopzeichen: Stopzeichen händisch nach NewLine (double Newline?)
% - requirements txt erstellen
% - DeepSpeed Config mit https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters+
% - Nutzung von ZeRO Stage2 optimization
% - llama2 7b genutzt
% - dataclasses um Datensatz einzulesen
% - Kleinere Block\_size für Loss Scaling
% - BF16 nur auf AMP \ac{gpu} Architektur möglich (hier nicht)
% - Deutlich mehr RAM erforderlich (180GB?)
% - Leistungen bei 11s/it von 3600 Iteration = 11h Training
% - Evaluation aller 50 Iterations, Saves aller 100 Iterations
% - Stage2/3 ohne CPU Offloading führt zu OOM
% - 3 Epochen = 14s/it 1800 = 7h Training (256 Block\_size)
% - Deepspeed Launcher mit 3 \ac{gpu}s (32GB v100, 1 Maschine)
% - Weitere Hyperparameter aus Llama 2 Modell
% - Nutzung von Epub anstelle Word Datei da bessere Konvertierung
% - keine nutung von chat Version (verlernt Human Reinforcment)