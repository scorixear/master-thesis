%*****************************************
\chapter{Ausführung der Lösung}\label{ch:solution}
%*****************************************

Die Lösung gliedert sich in sechs Schritte, die für die Durchführung des Trainings notwendig sind.
Im Folgenden werden diese Schritte näher erläutert. Für jeden Schritt sind die verwendeten Techniken und Bibliotheken separat aufgeführt.
Die Schritte sind wie folgt gegliedert:
\begin{enumerate}
    \item Laden des Modells
    \item Konvertierung des Modells in ein kompatibles Format
    \item Modelltraining
    \item Erstellung von Antworten auf dem Evaluierungsdatensatz
    \item Bewertung der erzeugten Antworten
    \item Auswertung basierend auf den Bewertungen
\end{enumerate}

\section{Herunterladen des Modells}
Die LLaMA-Modelle werden unter einer nicht-kommerziellen Lizenz für Forschungszwecke zur Verfügung gestellt. Der Zugang zu den Modellen wird im Einzelfall auf Anfrage gewährt. Diese Anfrage wurde im Rahmen dieser Arbeit gestellt und bestätigt.
Anschließend kann ein von den Autoren vorbereitetes Skript verwendet werden, um die trainierten Parameter des Modells herunterzuladen.
Das Skript benötigt eine explizite \ac{url}, die nach einer vorgegebenen Zeit von einer Woche nach Freigabe der Modelle ungültig wird. Aus diesem Grund ist diese \ac{url} nicht im Skript enthalten.

\section{Umwandlung des Modells in ein kompatibles Format}
Das Training des Modells basiert auf zwei grundlegenden Bibliotheken: der Transformers Bibliothek von Huggingface und der DeepSpeed Bibliothek von Microsoft.
Diese Bibliotheken, insbesondere Transformers, erleichtern den Entwicklungsprozess enorm und bieten ein hohes Maß an Abstraktion. Allerdings können Sie nur mit Modellen arbeiten, die in einer für die Transformers-Bibliothek verständlichen Form vorliegen.
Aus diesem Grund wird das heruntergeladene Modell mit Hilfe eines Huggingface-Skripts in diese kompatible Form umgewandelt.
Während dieser Konvertierung muss das Modell vollständig in den \ac{ram} des ausführenden Rechners geladen werden. Für das Modell LLaMA 7B sind dafür mehr als \SI{14}{\giga\byte} RAM erforderlich.
Die Konvertierung dauerte auf einem Rechner mit \SI{32}{\giga\byte} RAM und einem AMD Ryzen 7 5800X Prozessor ca. 30 Minuten.

\section{Training des Modells}
Um das LLaMA-Modell zu trainieren, wird die Transformers Bibliothek von Huggingface verwendet\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 16.8.2023}.
Das in Python geschriebene Trainingsskript basiert auf dem Beispielskript zum Training von kausalen Sprachmodellen aus dem Huggingface Github Repository\footnote{\url{https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py} abgerufen am 16.8.2023} und wurde teilweise an die Anforderungen des hier durchgeführten Trainings angepasst.
Die Konfiguration des Trainings gliedert sich in vier Bereiche: Einstellungen für das Modell, Einstellungen für die Trainingsdaten, Einstellungen für das Training selbst und Einstellungen für DeepSpeed.
Im Folgenden werden diese Einstellungen näher erläutert.
\subsection{Konfiguration des Modells}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        model\_name & meta-llama/Llama-2-7b-hf \\
        cache\_dir & \./cache \\
        use\_fast\_tokenizer & false \\
        model\_revision & main \\
        use\_auth\_token & true \\
        hugging\_token & \textit{Huggingface Token} \\
        torch\_dtype & auto \\
        low\_cpu\_mem\_usage & false \\
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration des Modells}\label{tab:model-config}
\end{table}
Die zur Auswahl und Konfiguration des Modells verwendeten Parameter sind in der Tabelle \ref{tab:model-config} aufgelistet.\\

Der Parameter \enquote{model\_name} entspricht einer Modellauswahl und kann entweder ein relativer Pfad zu einem lokalen Modell oder ein Modellname sein.
Der Modellname wurde hier auf das Modell Llama 2 7B gesetzt und verweist auf das von Huggingface gehostete Modell in einem mit der Transformers-Bibliothek kompatiblen Format.
Llama 2 wurde im Juli 2023 von Meta AI veröffentlicht und stellt eine generelle Verbesserung der ursprünglichen LLaMA 1 Modelle dar.
Neben der nun auf 4096 Tokens erweiterten Kontextlänge stellt Meta AI Llama 2 auch in einer Chat-Version zur Verfügung.
Die Chat-Version wurde zusätzlich mit Hilfe von Reinforcement Learning aus menschlichem Feedback trainiert und ermöglicht so eine einfachere Nutzung der vortrainierten Modelle im Kontext eines Chatbots.
Diese Chat-Modelle wurden jedoch nicht zum Training verwendet, da davon ausgegangen wird, dass dieses zusätzliche Training durch das hier durchgeführte Continual Pretraining überschrieben wird.
Außerdem ermöglicht Huggingface nun die Nutzung der Llama 2 Modelle ohne Download und Konvertierung, so dass die ersten beiden Schritte entfallen.
Die Llama 2 Modelle wurden unter anderem im Artikel von \citet{llama2} vorgestellt.\\

Der Parameter \enquote{cache\_dir} beschreibt den Speicherort des heruntergeladenen Modells.
Dies ermöglicht ein wiederholtes Trainieren des Modells ohne erneuten Download.
Die heruntergeladenen Modelle werden durch das Training nicht überschrieben und stellen somit den Grundzustand des Modells dar (im Folgenden auch als Llama2-0e für \enquote{0 Epochen trainiert} bezeichnet).\\

\enquote{use\_fast\_tokenizer} konfiguriert die Verwendung einer schnelleren Version des Tokenizers zur Konvertierung der Datensätze.
Diese Option ist optional und wurde hier nicht verwendet.\\

\enquote{model\_revision} beschreibt die zu verwendende Version des Modells.
Hier wurde die aktuellste Version verwendet, die durch den Wert \enquote{main} repräsentiert wird.\\

\enquote{use\_auth\_token} und \enquote{hugging\_token} beschreiben die Verwendung eines Authentifizierungs-Tokens, um Modelle von Huggingface herunterzuladen.
Diese Authentifizierung ist notwendig, da auch die Llama 2 Modelle unter der gleichen Lizenz wie LLaMA 1 stehen und nur auf Anfrage zur Verfügung gestellt werden.
Um die Modelle mit beschränktem Zugang herunterzuladen, wurde ein Huggingface-Account erstellt, die Anfrage an Meta AI zur Nutzung der Llama 2 Modelle gestellt und bestätigt und ein Authentifizierungstoken generiert.\\

Der Parameter \enquote{torch\_dtype} beschreibt den Datentyp, der für die Darstellung der Modellparameter verwendet wird.
Hier stehen Float32, Float16 und BFloat16 zur Verfügung.
Vorgefertigte Modelle wurden ursprünglich mit einem Datentyp erstellt und müssen in einen anderen Datentyp konvertiert werden, wenn der \enquote{torch\_dtype} nicht übereinstimmt.
Diese Konvertierung ist rechenintensiv und kann dazu führen, dass Modelle fehlerhaft trainiert werden oder mehr Rechenleistung während der Inferenz und des Trainings benötigen.
Aus diesem Grund wurde hier der Parameter \enquote{auto} verwendet, der den Datentyp des Modells automatisch erkennt und benutzt.\\

\enquote{low\_cpu\_mem\_usage} beschreibt ein Verfahren der Bibliothek Transformers zum Laden großer Modelle auf Systemen mit wenig Arbeitsspeicher.
Dabei werden die Modelle in mehreren Schritten in den Arbeitsspeicher geladen und anschließend in den \ac{gpu}-Speicher übertragen.
Dieses Verfahren reduziert die Menge des notwendigen Arbeitsspeichers, erhöht aber die Zeit, die zum Laden des Modells benötigt wird.
Da im vorliegenden Fall genügend Arbeitsspeicher zur Verfügung stand, wurde auf dieses Verfahren verzichtet.

\subsection{Konfiguration der Trainingsdaten}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        train\_file & ./input/health\_information\_systems\_epub.md \\
        max\_train\_samples & None \\
        overwrite\_cache & false \\
        block\_size & 1024 \\
        validation\_split\_percentage & 5 \\
        preprocessing\_num\_workers & 1 \\
        keep\_linebreaks & true \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Auswahl und Konfiguration der Trainingsdaten}\label{tab:data-config}
\end{table}
Für das Training des Modells Llama 2 wurde das Buch \enquote{Health Information Systems} von \citet{bb} im epub-Format in das Markdown-Format konvertiert.
Die notwendigen Änderungen am Text sind in \cref{sec:datenkuration} beschrieben.
Um diese Markdown-Datei in eine für das Modell verständliche Form umzuwandeln, wird die Bibliothek datasets\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 16.8.2023} von Huggingface verwendet.
Sie ermöglicht das Laden der Textdatei, die Umwandlung in Tokens und die Aufteilung in Blöcke.
Die Parameter zur Auswahl und Konfiguration der Trainingsdaten sind in der Tabelle \ref{tab:data-config} aufgelistet.\\

Der Parameter \enquote{train\_file} beschreibt den Pfad zur Trainingsdatei.
Diese Trainingsdatei wird einmal eingelesen und je nach Anzahl der unter \cref{subsec:config-training} beschriebenen Epochen mehrfach verwendet.\\

\enquote{max\_train\_samples} beschreibt die maximale Anzahl von Blöcken, die aus der Trainingsdatei gelesen werden sollen.
Zu Testzwecken kann hier eine geringere Anzahl an Blöcken verwendet werden, um die Konfiguration des Modells zu testen.
Im finalen Training wurde dieser Parameter auf \enquote{None} gesetzt, um alle Blöcke zu verwenden.\\

Der Parameter \enquote{overwrite\_cache} beschreibt, ob der Text erneut in Tokens umgewandelt werden soll.
Wenn sich der Text geändert hat, kann das Skript hier die nun tokenisierte Textdatei überschreiben.\\

Wie bereits erwähnt, wird der Text in Blöcke aufgeteilt.
Jeder Block wird vom Modell vollständig und gleichzeitig gelesen.
Die maximale Größe eines Blocks ist durch das Modell begrenzt und liegt bei Llama 2 bei 4096 Tokens.
Standardmäßig verwenden Modelle jedoch eine Blockgröße von 1024 Tokens, weshalb hier diese Größe angenommen wird, wenn keine weiteren Angaben gemacht werden.
Größere Blöcke führen zu einer schnelleren Verarbeitung des Textes und können zu einem besseren Verständnis der Zusammenhänge führen, da ein größerer Kontext betrachtet wird.
Allerdings steigt mit der Größe der Blöcke auch die Fehleranfälligkeit, so dass teilweise auch kleinere Blöcke verwendet werden müssen, um ein Training erfolgreich durchzuführen.
Probleme beim Training sind unter \cref{sec:problem-training} beschrieben.
Die Blockgröße kann mit dem Parameter \enquote{block\_size} angepasst werden.\\

Der Parameter \enquote{validation\_split\_percentage} beschreibt den Anteil der Daten, der für die Validierung genutzt werden soll.
Hier werden \SI{5}{\percent} der Daten für die Validierung verwendet.
Die Validierung liefert während des Trainings Informationen über die tatsächliche Leistung des Modells im Vergleich zum ungesehenen Text und dient dazu, den Fortschritt des Modells zu messen.\\

\enquote{preprocessing\_num\_workers} beschreibt die Anzahl der Prozesse, die für die Umwandlung der Textdatei in Tokens verwendet werden sollen.
Bei sehr großen Datenmengen ist die Umwandlung von Textdateien in Tokens eine sehr umfangreiche Aufgabe.
Um diesen Vorgang zu beschleunigen, können mehrere Prozesse die Textdateien parallel übersetzen.
In diesem Fall ist die Datenmenge jedoch klein genug, um die Aufgabe mit nur einem Prozess zu erledigen.\\

Der Parameter \enquote{keep\_linebreaks} beschreibt, ob Zeilenumbrüche in der Textdatei erhalten bleiben sollen.
In einigen Fällen können Textdateien viele Zeilenumbrüche enthalten, die der Formatierung des Textes dienen, aber dem Modell keine Informationen liefern oder es dazu veranlassen, diese Zeilenumbrüche zu imitieren.
Aus diesem Grund können Zeilenumbrüche optional entfernt werden.
Aufgrund der zuvor durchgeführten Datenkuration ist dies jedoch nicht notwendig, weshalb dieser Parameter auf \enquote{true} gesetzt wird.\\

\subsection{Konfiguration des Trainings}\label{subsec:config-training}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        output\_dir & ./trained/7B-3 \\
        overwrite\_output\_dir & true \\
        do\_train & true \\
        do\_eval & true \\
        per\_device\_train\_batch\_size & 1 \\
        per\_device\_eval\_batch\_size & 1 \\
        evaluation\_strategy & steps \\
        eval\_steps & 50 \\
        learning\_rate & 3e-4 \\
        weight\_decay & 0.1 \\
        optim & adamw\_torch \\
        adam\_beta1 & 0.9 \\
        adam\_beta2 & 0.95 \\
        adam\_epsilon & 1e-5 \\
        max\_grad\_norm & 1.0 \\
        num\_train\_epochs & 3 \\
        lr\_scheduler\_type & cosine \\
        warmup\_steps & 0 \\
        save\_strategy & steps \\
        save\_steps & 100 \\
        save\_total\_limit & 1 \\
        no\_cuda & false \\
        seed & 42 \\
        fp16 & true \\
        bf16 & false \\
        half\_precision\_backend & auto \\
        ddp\_backend & nccl \\
        deepspeed & ./ds\_configs/stage2\_offload.json \\
        \bottomrule
    \end{tabular}
    \caption{Parameter zur Konfiguration des Trainings}\label{tab:training-config}
\end{table}

Die Konfiguration des Trainings orientiert sich an den \enquote{TrainingArguments}, die ein Teil der Transformers-Bibliothek sind.
Nicht alle Parameter aus der Bibliothek müssen verwendet werden, weshalb hier nur abweichende Parameter vom Standard beschrieben werden.
Eine vollständige Liste der Parameter ist im Anhang unter \cref{app:sec:trainingpaameter} zu finden. Die beschriebenen Parameter sind in \cref{tab:training-config} aufgeführt.\\

Der Parameter \enquote{output\_dir} definiert den Speicherpfad für die Ergebnisse des Trainings.
Diese umfassen die trainierten Gewichte des Modells, eine abschließende Auswertung der Validierung, den Status des Trainers sowie gegebenenfalls während des Trainings erstellte Kontrollpunkte.\\

Der Parameter \enquote{overwrite\_output\_dir} legt fest, ob der Ergebnis-Ordner überschrieben werden soll, wenn er bereits existiert.
Diese Option bestimmt auch, ob das Training von einem zuvor erstellten Kontrollpunkt aus fortgesetzt werden soll.
Wenn der Ergebnis-Ordner überschrieben wird, kann von keinem Kontrollpunkt aus fortgefahren werden.\\

Die Parameter \enquote{do\_train} und \enquote{do\_eval} geben an, ob das Training und die Validierung durchgeführt werden sollen.
Falls keine Validierung durchgeführt wird, wird der Trainingsdatensatz dennoch in Trainings- und Validierungsdatensatz aufgeteilt.\\

Während des Trainings und der Validierung werden die Fehlerfunktionen von mehreren Blöcken berechnet und danach gemittelt.
Daraufhin wird ein Gradient basierend auf dieser kumulativen Fehlerfunktion berechnet, welcher die Gewichte des Modells anpasst.
Eine detailiertere Beschreibung des Ablaufs findet sich in \cref{subsec:backpropagation}.
Die Anzahl der Blöcke pro \enquote{Batch} (siehe \cref{def:batch}) wird durch die Parameter \enquote{per\_device\_train\_batch\_size} und \enquote{per\_device\_eval\_batch\_size} festgelegt.
Wenn man eine Batch-Größe von 1 und 3 \ac{gpu}s verwendet, erhält man somit 3 Blöcke pro Gradientenberechnung.
Während höhere Batch-Größen die Berechnungszeit reduzieren können, führen sie auch zu ungenaueren Gradienten und erfordern mehr Speicher auf der \ac{gpu}.
Die Batch-Größe ist in diesem Fall auf 1 gesetzt, da die verwendeten Nvidia V100-\ac{gpu}s als auch Nvidia A30-\ac{gpu}s nur genug Speicher für eine Batch-Größe von 1 hatten.\\

Die Parameter \enquote{evaluation\_strategy} und \enquote{eval\_steps} legen fest, in welchen Intervallen die Validierung durchgeführt werden soll.
In diesem Szenario wird die Validierung alle 50 Iterationen durchgeführt.
Eine Iteration bezieht sich auf die Berechnung eines Gradienten.\\

Die Lernrate des Modells wird durch den Parameter \enquote{learning\_rate} beschrieben.
Dieser Parameter bestimmt, in welchem Ausmaß die Gewichte des Modells angepasst werden.
Eine sehr hohe Lernrate kann dazu führen, dass das Modell nicht konvergiert, während eine sehr niedrige Lernrate zu langen Trainingszeiten führt.
Die Lernrate wurde aus dem Artikel über LLaMA-Modelle von \citet{llama} übernommen.\\

In dem Artikel zu den LLaMA-Modellen von \citet{llama} wird ebenfalls ein Gewichtsabnahme-Wert von $0,1$ verwendet, der auch bei diesem Training mithilfe des Parameters \enquote{weight\_decay} eingestellt wurde.
Die Gewichtsverminderung führt zu einer kontinuierlichen Verringerung der Gewichte des Modells. Dadurch soll verhindert werden, dass es sich zu stark an einzelne Trainingsdaten anpasst.\\

Die Parameter \enquote{adam\_beta1}, \enquote{adam\_beta2} und \enquote{adam\_epsilon} beschreiben die Parameter des verwendeten AdamW-Optimierers, der im Parameter \enquote{optim} eingestellt ist.
Diese Werte entsprechen ebenfalls den Werten, die im Artikel zu den LLaMA-Modellen von \citet{llama} angegeben sind.
AdamW ist in verschiedenen Implementierungen verfügbar. Hier wurde die neueste Implementierung der PyTorch-Bibliothek verwendet.
Die Funktionsweise des AdamW-Optimierers ist im Artikel von \citet{adamw} genauer beschrieben.\\

Der Parameter \enquote{max\_grad\_norm} beschreibt die maximale Norm des Gradienten, die durch die Gewichte des Modells nicht überschritten werden darf. 
Eine weitere Bezeichnung, die in der Bibliothek DeepSpeed oder in Artikeln zu LLaMA-Modellen genutzt wird, ist \enquote{gradient clipping}.
Er begrenzt die Größe des Gradienten, der in großen neuronalen Netzen explosionsartig ansteigen kann.
Zu große Gradienten führen zu schlechteren Trainingsergebnissen und zu einer zu starken Anpassung der Gewichte, was wiederum zu einer Oszillation um ein Minimum führt.
Um die Gradienten zu begrenzen, wird die L2-Norm des Gradienten berechnet.
Wenn diese Norm den angegebenen Maximalwert überschreitet, wird der Gradient herunterskaliert, bis er die Maximalnorm nicht mehr überschreitet.\\

Der Parameter \enquote{num\_train\_epochs} gibt an, wie viele Epochen trainiert werden sollen.
Eine Epoche umfasst eine vollständige Durchführung des Trainingsdatensatzes.
Mehrere Epochen können zu besseren Ergebnissen führen, insbesondere bei kleineren Datensätzen, weil das Modell sich besser an die Trainingsdaten anpassen kann.
Zu viele Epochen führen zu Overfitting.
In dieser Arbeit wurde das Llama 2 7B Modell für eine, drei, fünf und zehn Epochen trainiert.
Bei der Verwendung von Nvidia V100 Grafikkarten kam es während des Trainings zu Problemen, die in \cref{sec:problem-training} genauer beschrieben sind.\\

Durch die Anwendung der Parameter \enquote{lr\_scheduler} und \enquote{warmup\_steps} kann eine Aufwärmphase des Trainings realisiert werden.
Die Aufwärmphase bezeichnet den Start des Trainings, in dem die Lernrate der \enquote{lr\_scheduler}-Funktion innerhalb der ersten \enquote{warmup\_steps} Iterationen schrittweise von 0 auf den gewünschten Wert erhöht wird.
Dieser Ansatz verhindert eine zu schnelle Anpassung der Modellgewichte an spezielle Details der Trainingsdaten.
Vor allem bei untrainierten Modellen werden während des Trainings diese erlernten Fehler wieder korrigiert. Dies führt jedoch ohne eine Aufwärmphase zu längeren Trainingszeiten und schlechteren Ergebnissen.
In diesem Fall kann die Aufwärmphase übersprungen werden, da ein bereits vortrainiertes Modell genutzt wird.\\

Die Parameter \enquote{save\_strategy}, \enquote{save\_steps} und \enquote{save\_total\_limit} beschreiben, wie oft und in welchem Abstand Modelle während des Trainings gespeichert werden sollen.
Hier wird alle 100 Iterationen das Modell gespeichert, wobei maximal 1 Kontrollpunkt gleichzeitig existiert.
Kontrollpunkte ermöglichen es, abgebrochene oder fehlgeschlagene Trainingsläufe wieder aufzunehmen.
Bei längerem Training kann die maximale Laufzeit der Skripte erreicht werden.
Eine Wiederaufnahme des Trainings an diesem Punkt ist dann möglich.
Das Rechenzentrum beschränkt die maximale Laufzeit von Skripten auf 2 Tage.
Wenn das Training länger als 2 Tage dauert, kann diese Grenze überschritten werden.
Die Begrenzung ist notwendig aufgrund der gemeinsamen Nutzung des Rechenzentrums.
Durch die regelmäßige Unterbrechung von Skripten können andere Skripte in der Warteschlange zwischengeschoben werden.
Zusammen mit der Begrenzung ergibt sich im Allgemeinen eine Wartezeit von maximal 2 Tagen.
Durch die Einführung von Kontrollpunkten im Training führt eine Unterbrechung nicht zu einem Rückschlag im Fortschritt.\\

Der Begriff \enquote{no\_cuda} beschreibt die Durchführung des Trainings ohne die Verwendung einer \ac{gpu}.
Diese Einstellung wird für Testzwecke verwendet, damit Testskripte auf Systemen ausgeführt werden können, die nicht über die erforderliche Anzahl von \ac{gpu}s verfügen.\\

Die Verwendung des Parameters \enquote{seed} beeinflusst die Zufälligkeit des Trainingsprozesses.
Während des Trainingsprozesses werden einige Zufallsvariablen verwendet, um Werte zu initialisieren.
Jedoch ist die Auswirkung im Kontext des Fortführenden Trainings irrelevant.
Es wird jedoch ein fester Wert gesetzt, da das Trainingsskript auch für untrainierte Modelle genutzt werden kann.
Mit Hilfe eines Seeds erstellen Zufallszahlengeneratoren zufällig verteilte, jedoch reproduzierbare Zahlenfolgen.
Daher sind Modelle mit gleichem Seed und Datensatz identisch.\\

Zur Konfiguration der verwendeten Datentypen stehen neben dem oben genannten \enquote{torch\_dtype} die beiden Parameter \enquote{fp16} und \enquote{bf16}. Während \enquote{torch\_dtype} die Initialisierung des Modells beschreibt, beeinflussen diese Parameter die Datentypen während des Trainings.
Die Zahl 16 steht hier für die Anzahl der Bits pro Wert, was wiederum einen großen Einfluss auf den Speicherbedarf und die Genauigkeit des Modells hat.
Datentypen sollten nicht geändert werden, wenn ein vortrainiertes Modell weiter trainiert wird, da eine Änderung schnell zu Fehlverhalten führen kann.
Die Llama 2 Modelle wurden mit dem Datentyp \enquote{bf16} (ausgeschrieben BFloat16) trainiert.
Dieser Datentyp steht nur auf bestimmten \ac{gpu}-Architekturen zur Verfügung.
Ein Datum im BFloat16-Format hat eine kleinere Mantisse (7 Bits) als ein Float16-Datum (10 Bits Mantisse). Dies führt zu einer geringeren Genauigkeit, verkürzt jedoch die Zeit, die für die Konvergenz des Modells benötigt wird.
Die verwendeten V100 Grafikkarten haben nicht die notwendige Architektur um BFloat16 Werte zu unterstützen, daher wure das Llama 2 Modell hier in ein Float16 Format konvertiert.
Dies führt zu Problemen während des Trainings und beschränkt die maximale Anzahl der Epochen auf 3. Eine genauere Erklärung der Probleme ist in \cref{sec:problem-training} beschrieben.
Bei der Verwendung von A30 Grafikkarten steht eine Nutzung von BFloat16 Werten zur Verfügung. Mit Hilfe dieses Datentypes wurden die Modelle zusätzlich auf fünf und zehn Epochen trainiert.\\

Die Parameter \enquote{half\_precision\_backend} und \enquote{ddp\_backend} beschreiben Architekturen für die Ausführung des Trainings mit Hilfe der Bibliothek Transformers.
Während \enquote{half\_precision\_backend} die Architektur für die Ausführung von Trainingsschritten mit Float16-Werten festlegt, beschreibt \enquote{ddp\_backend} die Kommunikationsarchitektur für den Datenaustausch zwischen \ac{gpu}s.
Die DeepSpeed Bibliothek verwendet die \enquote{NCCL} Architektur, daher wird diese hier eingestellt.\\

Die DeepSpeed Konfiguration wird separat in einer JSON Datei gespeichert.
Durch Setzen des Parameters \enquote{deepspeed} verwendet die Transformers Bibliothek die DeepSpeed Bibliothek\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 17.8.2023} zur Ausführung des Trainings.
Wichtig bei der Verwendung von DeepSpeed mit Transformers ist eine identische Konfiguration der Trainingsparameter, wie zum Beispiel \enquote{batch\_size}, \enquote{gradient\_accumulation\_steps} und \enquote{learning\_rate}.\\

\subsection{Konfiguration des DeepSpeed Trainings}\label{sec:deepspeed-config}
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{fp16}\\
        enabled&auto\\
        loss\_scale&0\\
        loss\_scale\_window&1000\\
        initial\_scale\_power&16\\
        hysteresis&2\\
        min\_loss\_scale&1\\
        \midrule
        \multicolumn{2}{c}{bf16}\\
        enabled&true|false\\
        \midrule
        \multicolumn{2}{c}{optimizer}\\
        type&AdamW\\
        lr&auto\\
        betas&auto\\
        eps&auto\\
        weight\_decay&auto\\
        \midrule
        \multicolumn{2}{c}{scheduler}\\
        type&WarmupLR\\
        warmup\_min\_lr&auto\\
        warmup\_max\_lr&auto\\
        warmup\_num\_steps&auto\\
        \midrule
        \multicolumn{2}{c}{zero\_optimizations}\\
        stage&2\\
        contiguous\_gradients&true\\
        overlap\_comm&true\\
        reduce\_scatter&true\\
        reduce\_bucket\_size&2e8\\
        allgather\_bucket\_size&2e8\\
        \midrule
        \multicolumn{2}{c}{offload\_optimizer}\\
        device&cpu\\
        pin\_memory&true\\
        \midrule
        gradient\_clipping&1\\
        steps\_per\_print&500\\
        wall\_clock\_breakdown&false\\
        train\_micro\_batch\_size\_per\_gpu&auto\\
    \end{tabular}
    \caption{DeepSpeed Konfiguration}\label{tab:deepspeed-config}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Parameter}&\textbf{Wert}\\
        \midrule
        \multicolumn{2}{c}{zero\_optimization}\\
        stage&3\\
        contiguous\_gradients&true\\
        stage3\_max\_live\_parameter&1e9\\
        stage3\_max\_resuse\_distance&1e9\\
        stage3\_prefetch\_bucket\_size&1e7\\
        stage3\_param\_persistence\_threshold&1e5\\
        reduce\_bucket\_size&1e7\\
        sub\_group\_size&1e9\\
        \midrule
        \multicolumn{2}{c}{offload\_param}\\
        device&cpu\\
        pin\_memory&true\\
    \end{tabular}
    \caption{DeepSpeed \ac{zero} Stufe 3 Konfiguration}\label{tab:deepspeed-config-stage3}
\end{table}

Die DeepSpeed Konfiguration ist in \cref{tab:deepspeed-config} beschrieben.
Die hier verwendeten Parameter beschreiben ein Training mit Hilfe der \ac{zero} Stufe 2 Optimierung und zusätzlichem CPU Offloading.
Die \ac{zero} Stufe 2 Optimierung beschreibt einen Algorithmus zur Reduzierung des Speicherbedarfs während des Trainings.
Dieser Algorithmus ist in \citet{deepspeed} genauer beschrieben.
Zusätzlich zu dieser Optimierung werden Teile der Berechnungen auf die \ac{cpu} ausgelagert, um den Speicherbedarf pro \ac{gpu} weiter zu reduzieren.
Diese Konfiguration kam zum Einsatz bei dem Training der Modelle mit Hilfe von Nvidia Tesla V100 Grafikkarten.
\cref{tab:deepspeed-config-stage3} beschreibt zusätzlich eine Konfiguration des \enquote{zero\_optimization} Abschnitts, während der Nutzung der \ac{zero} Stufe 3 Optimierung.
Hier wird zusätzlich Parameter der Modelle auf die CPU ausgelagert. 
Diese Konfiguration kam zum Einsatz bei der dem Training der Modelle mit Hilfe von Nvidia Tesla A30 Grafikkarten.\\

Der Abschnitt \enquote{fp16} beschreibt den Umgang mit Float16-Werten während des Trainings.
Dabei handelt es sich um eine dynamische Skalierung der Fehlerwerte (engl. \enquote{Dynamic Loss Scaling}).
Die Skalierung der Fehlerwerte ist notwendig, da aufgrund der geringeren Genauigkeit von Float16-Werten kleinere Werte der Fehlerwerte gerundet werden und verloren gehen.
Aus diesem Grund werden die Fehlerwerte bei der Berechnung um mehrere Potenzen skaliert.
Diese Skalierung kann auch zu einem Überlauf über den Wertebereich des Float16 Datentyps führen.
DeepSpeed verwendet hier eine automatische, dynamische Skalierung der Fehlerwerte, ohne einen Überlauf zu verursachen.
Eine genauere Erklärung der Skalierung von Fehlerwerten findet sich im Artikel von \citet{lossscale}.
Der Parameter \enquote{loss\_scale} beschreibt die konstante Skalierung der Fehlerwerte.
Ist er auf $0$ gesetzt, wird eine dynamische Skalierung verwendet.
Der Parameter \enquote{loss\_scale\_window} beschreibt das Werteintervall, in dem die dynamische Skalierung erfolgt.
Der Parameter \enquote{initial\_scale\_power} beschreibt die Größe der initialen Skalierung der Fehlerwerte.
Die tatsächliche Skalierung der Fehlerwerte entspricht $2^{initial\_scale\_power}$.
Der Parameter \enquote{hysteresis} beschreibt die minimale Anzahl von Schritten, in denen die Skalierung nicht verändert werden kann.
Der Parameter \enquote{min\_loss\_scale} beschreibt die minimale Skalierung der Fehlerwerte.
Hier entspricht der Wert $1$ keiner Skalierung.\\

Der Abschnitt \enquote{bf16} beschreibt die Verwendung des binären FLoat16 Datentyps während des Trainings.
Ist sie auf \enquote{true} gesetzt, so werden die Berechnungen mit Hilfe des binären Float16 Datentyps durchgeführt.
Ist sie auf \enquote{false} gesetzt, so werden die Berechnungen mit Hilfe des Float16 Datentyps durchgeführt.
Im Falle des Trainings mit V100 Grafikkarten und einer \ac{zero} Stufe 2 Optimierung ist dieser Parameter auf \enquote{false} gesetzt.
Im Falle des Trainings mit A30 Grafikkarten und einer \ac{zero} Stufe 3 Optimierung ist dieser Parameter auf \enquote{true} gesetzt.\\

Der Abschnitt \enquote{optimizer} beschreibt die Parameter des Optimierungsalgorithmus. Wie bereits bei den Trainingsparametern beschrieben, wird der AdamW-Optimierer verwendet.
Die Parameter \enquote{lr}, \enquote{betas}, \enquote{eps} und \enquote{weight\_decay} müssen mit den in den Trainingsparametern eingestellten Werten übereinstimmen.
Aus diesem Grund werden diese Parameter vor Beginn des Trainings auf \enquote{auto} gesetzt und durch die Transformer-Bibliothek ergänzt.\\

Der Abschnitt \enquote{scheduler} beschreibt die Parameter des Lernratenplaners. Der Lernratenplaner ist für die Anpassung der Lernrate während des Trainings verantwortlich.
Er dient in diesem Fall zur Durchführung der Aufwärmphase. Ist die Aufwärmphase in den Trainingsargumenten deaktiviert, so wird die Bibliothek diese Phase auch in der DeepSpeed-Konfiguration deaktivieren.\\

Der Abschnitt \enquote{zero\_optimizations} beschreibt die Parameter der \ac{zero} Stufe 2 Optimierung.
Die \ac{zero} Optimierung wird unter \citet{deepspeed} genauer beschrieben.
Der Parameter \enquote{contiguous\_gradients} beschreibt die Auslagerung der Gradienten in einen zusammenhängenden Speicherbereich während der Berechnung.
Dadurch wird eine Fragmentierung des Speichers während der Backpropagation vermieden.
Mit Hilfe des Parameters \enquote{overlap\_comm} wird versucht, die Gradienten während der Berechnung der Backpropagation zu reduzieren, um eine schnellere Gesamtberechnung zu ermöglichen.
\enquote{reduce\_scatter} beschreibt die Verwendung einer speziellen Reduktionsmethode \enquote{Reduce Scatter} zur Mittelung von Gradienten.
In Kombination mit den Parametern \enquote{reduce\_bucket\_size} und \enquote{allgather\_bucket\_size} wird die maximale Anzahl der in einem Schritt zu reduzierenden Gradienten festgelegt.
Diese Option reduziert den Speicherbedarf während des Trainings erheblich.
Mit Hilfe der Option \enquote{offload\_optimizer} wird der Zustand und die Berechnung des Optimierers auf die CPU ausgelagert.
Optional kann dies für sehr große Modelle auch auf eine NVMe SSD erfolgen.
Mit Hilfe der Einstellung \enquote{pin\_memory} wird der für die Berechnung benötigte Speicher auf der CPU reserviert.
Dies führt zu einer besseren Performance, aber auch zu zusätzlichem Speicherbedarf.\\

Wie bereits bei den Trainingsargumenten durch den Parameter \enquote{max\_grad\_norm} beschrieben, wird die maximale Größe des Gradienten durch den Parameter \enquote{gradient\_clipping} festgelegt.
Eine automatische Übernahme der Konfiguration der Trainingsargumente ist in diesem Fall nicht möglich.
Die Werte müssen jedoch übereinstimmen.\\

Die Parameter \enquote{steps\_per\_print} und \enquote{wall\_clock\_breakdown} beschreiben die Ausgabe von Informationen während des Trainings.
Zusätzlich zur Ausgabe der Transformers-Bibliothek werden alle 500 Iterationen weitere Informationen der DeepSpeed-Bibliothek ausgegeben.
Mit Hilfe des Parameters \enquote{wall\_clock\_breakdown} wird zusätzlich die Messung der verstrichenen Zeit für jede Phase einer Iteration ausgegeben.\\

Der Parameter \enquote{train\_micro\_batch\_size\_per\_gpu} beschreibt die Anzahl der Batches pro GPU.
Dieser Wert wird aus den Trainingsargumenten übernommen.\\

Zur Ausführung des Trainings mit Hilfe der \ac{zero} Stufe 3 Optimierung auf Nvidia Tesla A30 Grafikkarten wurde der Abschnitt \enquote{zero\_optimization} in \cref{tab:deepspeed-config} durch die gezeigten Parameter in \cref{tab:deepspeed-config-stage3} ersetzt. Weggefallene Parameter sind hier \enquote{overlap\_comm}, \enquote{reduce\_scatter} und \enquote{allgather\_bucket\_size}.
Zusätzliche Parameter sind hier \enquote{stage3\_max\_live\_params}, \enquote{stage3\_max\_reuse\_distance}, \enquote{stage3\_prefetch\_bucket\_size}, \enquote{stage3\_param\_persistence\_threshold} und \enquote{sub\_group\_size} sowie der Abschnitt \enquote{offload\_param}.
Der Parameter \enquote{stage3\_max\_live\_params} beschreibt die maximale Anzahl der Gewichte, die während des Trainings in den Speicher der GPU geladen werden. Kleinere Werte führen zu einer geringeren Speicherbelegung, jedoch auch zu erhöhter Kommunikation.
Der Parameter \enquote{stage3\_max\_reuse\_distance} beschreibt, dass ein Gewicht nicht freigegeben werden kann, wenn er innerhalb dieser Anzahl an Gewichten erneut zur Berechnung genutzt wird. Auch hiier führen kleinere Werte zu einer geringeren Speicherbelegung, jedoch auch zu erhöhter Kommunikation.
Der Parameter \enquote{stage3\_prefetch\_bucket\_size} beschreibt die Anzahl der Gewichte, die während des Trainings vorausgeladen werden.
Der Parameter \enquote{stage3\_param\_persistence\_threshold} verhindert das Partitionieren von Gewichten, die kleiner als dieser Wert sind.
Der Parameter \enquote{sub\_group\_size} beschreibt die maximale Anzahl an Parametern, die gleichzeitig zur Berechnung genutzt werden.

\subsection{Verwendete Bibliotheken zum Training}
\begin{table}
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Bibliothek} & \textbf{Version} \\
        \midrule
        datasets & 2.13.1 \\
        DeepSpeed & 0.10.0 \\
        evaluate & 0.4.0 \\
        PyTorch & 2.0.1 \\
        sentencepiece & 0.1.96 \\
        Transformers & 4.31.0 \\
        \bottomrule
    \end{tabular}
    \caption{Verwendete Bibliotheken zum Training}
    \label{tab:training-libraries}
\end{table}
Die für das Training verwendeten Bibliotheken sind in \cref{tab:training-libraries} aufgelistet.
Einige der hier aufgeführten Pakete benötigen weitere Abhängigkeiten, die über die verwendete Paketverwaltung \ac{pip}\footnote{\url{https://pip.pypa.io/en/stable/} abgerufen am 19.8.2023} installiert werden.\\

Zur Verwaltung der Datensätze wird die Bibliothek \enquote{datasets} verwendet.
Mit Hilfe dieser Bibliothek können Textdateien während des Trainings geladen, in Tokens umgewandelt und in Blöcke gruppiert werden.
\enquote{datasets} ermöglicht auch eine geordnete Zuweisung von Blöcken zu verschiedenen \ac{gpu}s durch eine Schnittstelle mit den Bibliotheken Transformers und DeepSpeed.
Die Bibliothek stammt von Huggingface\footnote{\url{https://huggingface.co/docs/datasets/index} abgerufen am 19.8.2023}.\\

Die \enquote{DeepSpeed}-Bibliothek wird verwendet, um das Training und die Ausführung auf mehreren \ac{gpu}s zu beschleunigen.
Sie ermöglicht die Nutzung der \ac{zero}-Optimierung, die es erlaubt, Teile des Trainings und des Modells auf mehrere GPUs, der CPU und lokalen NVMe SSDs auszulagern.
Der Prozess dieser Optimierung wird in \citet{deepspeed} genauer beschrieben.
Die Bibliothek stammt von Microsoft\footnote{\url{https://www.microsoft.com/en-us/research/project/deepspeed/} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{evaluate} wird verwendet, um die Modellgenauigkeit zu berechnen.
Sie stammt von Huggingface\footnote{\url{https://huggingface.co/docs/evaluate/index} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{PyTorch} bildet die Grundlage für die Berechnung der neuronalen Netze.
Sie ermöglicht neben der eigentlichen Berechnung aller Trainingsphasen auch die Verwendung von \ac{gpu}s. Die Bibliothek stammt von PyTorch selbst\footnote{\url{https://pytorch.org/} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{sentencepiece} wird für die Tokenisierung der Texte verwendet.
Sie wird im Skript nicht explizit verwendet, ist hier aber als zusätzliche Voraussetzung für die Verwendung des Tokenizers des Llama-Modells aufgeführt.
Diese Bibliothek wurde auch im Artikel von \citet{llama} verwendet.
Sie wurde von Google veröffentlicht\footnote{\url{https://github.com/google/sentencepiece} abgerufen am 19.8.2023}.\\

Die Bibliothek \enquote{Transformers} wird für die Verwaltung der Modelle und die Berechnung der neuronalen Netze verwendet.
Sie ermöglicht die Verwendung vortrainierter Modelle und bietet einen abstrakten Trainer, der die verschiedenen Phasen des Trainings steuert.
Außerdem ermöglicht sie die Verwendung der Bibliothek \enquote{DeepSpeed} ohne zusätzliche Anpassung.
Die Bibliothek stammt von Huggingface\footnote{\url{https://huggingface.co/docs/transformers/index} abgerufen am 19.8.2023}.\\

\subsection{Training auf einem GPU Computing Cluster}\label{sec:training-cluster}

Das Training der Modelle erfordert weit aus mehr Leistung als die Nutzung jener.
Aus diesem Grund kann ein normaler Computer hierfür nicht verwendet werden.
Stattdessen wurde ein GPU Computing Cluster des Rechenzentrums der Universität Leipzig verwendet.\\

Das Rechenzentrum der Universität Leipzig stellt zwei GPU Computing Cluster zur Verfügung, welche namentlich als \enquote{Paula} und \enquote{Clara} bezeichnet sind.
Das Cluster \enquote{Paula} verfügt über 12 Knoten, welche jeweils über 8 Nvidia Tesla A30 \ac{gpu}s besitzen.
Eine A30 \ac{gpu} besitzt \SI{24}{\giga\byte} \ac{gpu}-RAM.
Ebenso besitzt ein Knoten 2 AMD(R) EPYC(R) 7713 CPU Prozessoren mit je 64 Kernen und \SI{1}{\tera\byte} \ac{ram}.
Das Cluster \enquote{Clara} ist zweigeteilt.
Teil 1 besteht aus 8 Knoten, ausgestattet mit jeweils 4 Nvidia Tesla V100 \ac{gpu}s. Eine V100 \ac{gpu} besitzt \SI{32}{\giga\byte} \ac{gpu}-RAM.
Teil 2 besteht auf 23 Knoten mit jeweils 8 Nvidia GeForce RTX 2080 Ti \ac{gpu}s. Eine RTX 2080 Ti \ac{gpu} besitzt \SI{11}{\giga\byte} \ac{gpu}-RAM.
Jeder Knoten beistzt eine AMD(R) EPYC(R) 7551P CPU mit 32 Kernen und \SI{512}{\giga\byte} \ac{ram}.
Alle Knoten innerhalb eines Cluster sind über ein \SI{100}{\giga\bit\per\second} Infiniband Netzwerk miteinander verbunden.
Zur Verbindung zum Internet wird ein \SI{25}{\giga\bit\per\second} Ethernet Verbindung verwendet.\\

Die Ausführung der Skripte erfolgt mit Hilfe der \enquote{Slurm Workload Manager} Software.
Diese ermöglicht eine umfangreiche Reservierung von Ressourcen und bietet eine Bash-Schnittstelle zur Konfiguration und Ausführung von Skripten.
Mit Hilfe von Slurm-Bash Dateien konnte das Training der Modelle auf dem Cluster erfolgen.\\

\subsection{Probleme während des Trainings}\label{sec:problem-training}

Die Ausführung des Trainings verlief nicht ohne Probleme.
Geplant war ein Training mehrerer Modelle mit unterschiedlicher Anzahl an Epochen als auch unterschiedlicher Größe.
Neben dem Llama 2 7B Modell verspricht ein größeres Modell wie das 13B, 33B oder 65B Modell weitaus höhere Leistungen sowohl bei der Imitierung des Textes als auch dem Verständnis des Wissen und damit einhergehend den Ergebnissen der Evaluation.
Limitiert ist das Training auf 4 \ac{gpu}s pro Knoten mit jeweils \SI{32}{\giga\byte} \ac{gpu}-RAM beziehungsweise 8 \ac{gpu}s pro Knoten mit jeweils \SI{24}{\giga\byte}, wie bereits in \cref{sec:training-cluster} aufgeführt.
Um größere Modelle zu trainieren muss diese Anzahl erhöht werden.
Die Trainingsparameter ermöglichen hier eine einfache Erweiterung auf zusätzliche Grafikkarten.
Die Trainingsskripte müssen hierfür nicht angepasst werden.\\

Das Training auf mehreren Knoten mit mehr Grafikkarten wurde testweise durchgeführt.
Hierzu wurde ein Testskript von Huggingface benutzt, der die Kommunikation zwischen Grafikkarten und Knoten sicherstellt, als auch den Trainingsskript.
Beide Skript schlugen in dieser Einstellung fehl.
Grund hierfür ist ein Kommunikationsfehler zwischen Knoten.
Die Kommunikation während des Trainings wird vollständig von den Bibliotheken Transformers und DeepSpeed übernommen.
Hierfür wird ein genutzter Knoten als Main-Knoten ernannt, welcher die Nachrichtenweiterleitung und Datenweiterleitung übernimmt.
Andere Knoten übernehmen die Rolle als Client-Knoten.
Auf dem Main-Knoten wird ein Server gestartet, mit dem sich die Client-Knoten per IPv4 oder IPv6 verbinden können.
Diese Verbindung schlug in allen Einstellungen fehl.
Eine Lösung für dieses Problem konnte nicht gefunden werden.
Aus diesem Grund wurde das Training auf einen Knoten und damit 4 \ac{gpu}s beschränkt.\\

Mit dem aktuellen Aufbau können größere Modelle wie das 33B oder das 65B Modell nicht trainiert werden.
Aus diesem Grund und einer ressourcenschonenderer erhöhten Nutzbarkeit wurde das Training mit dem Llama 2 7B Modell begonnen.
Während der Durchführung auf Nvidia V100 Grafikkarten traten in unregelmäßigen Abständen Überlauffehler auf.
Wie in \cref{sec:deepspeed-config} beschrieben werden berechnete Fehlerwerte der Fehlerrückführungsfunktion während der Berechnung skaliert, um eine höhere Genauigkeit zu erreichen und einen Unterlauf zu vermeiden.
Diese Skalierung führte zu einem Überlauf über den maximalen Wert des Float16 Datentyps.
Da eine dynamische Skalierung der Fehler in der DeepSpeed Konfiguration eingestellt wurde, wurde daraufhin die Skalierung um eine Potenz geringer angesetzt.
Dieser Prozess wiederholte sich in unregelmäßigen Abständen und führte ultimativ zum Erreichen des minimalen Skalierungsfaktor 1, welches keiner Skalierung entspricht.
Wird dieser Wert erreicht und eine Skalierung ist dennoch notwendig wird das Training abgebrochen.\\

Grund für diese auftretenden Skalierungsfehler ist nicht bekannt.
Vermutet ist jedoch die Nutzung der Llama-Modelle mit den Datentypen Float16.
Ursprünglich wurden diese Modelle mit dem Datentypen BFloat16 trainiert.
Von einer Transformierung in Float16 Werten wird auch auf der Huggingface Seite abgeraten.
Ebenso ist eine Fehler in der Implementierung von DeepSpeed vermutet, der das Laden von Modellen im Float16 Format nicht korrekt umsetzt.
Eine Lösung hierzu ist bereits im offiziellen GitHub Repository von DeepSpeed vorgeschlagen, wurde allerdings noch nicht in DeepSpeed übernommen.
Die Nutzung von BFloat16 Werten ist nur auf dem Cluster \enquote{Paula} mit Nvidia Tesla A30 Grafikkarten möglich.
BFloat16 Werte können nur von Grafikkarten umgesetzt werden, welche die AMP (Automatic Mixed Precision) Architektur unterstützen.
Diese Architektur wird von den Grafikkarten der Nvidia A100 Serie unterstützt, welche zum initialen Training der Llama-Modelle genutzt wurden.
Zur Verfügung stehen im Rechenzentrum jedoch nur Grafikkarten der Reihe Nvidia Tesla V100, Nvidia GeForce RTX 2080 Ti und Nvidia Tesla A30.
Sowohl die V100 als auch RTX2080 Reihe unterstützen die Nutzung von BFloat16 Werten nicht.
Die A30 Reihe unterstützt BFloat16, bietet allerdings nur \SI{24}{\giga\byte} RAM, weshalb hier eine \ac{zero} Stufe 3 Optimierung gewählt wurde.
Mit der Nutzung von A30 Grafikkarten traten keine Skalierungsfehler bei einem Training bis 10 Epochen auf.\\

Initial verhinderte der Skalierungsfehler ein Training der Modelle auf V100 Grafikkarten vollständig.
Durch Anpassung der Block-Größe aus \cref{subsec:config-training} auf geringere Werte konnte die Frequenz der Skalierungsfehler minimiert werden, weshalb in dieser Einstellung ein Training auf 3 Epochen möglich war.\\

Während des Trainings traten mehrfach Speicher-Fehler auf, die angaben, dass sämtlicher Speicher der Grafikkarte belegt sei, jedoch mehr gebraucht wurde.
Diese Fehler waren unabhängig von Grafikkarten-Art und Konfiguration des Trainings.
Ausschließlich eine Minimierung der Batch-Größe auf 1 pro \ac{gpu}, der Aktivierung des CPU Offloading und der Nutzung von \SI{256}{\giga\byte} CPU RAM ermöglichte ein erfolgreiches Training des Llama 2 7B Modells.
Diese Speicheranforderung ist unerwartet hoch und kann nicht durch die Größe des Modells erklärt werden.
Für das Training auf V100 Grafikkarten wurde die \ac{zero} Stufe 2 Optimierung mit zusätzlichem CPU Offloading genutzt.
Um das Training auf A30 Grafikkarten durchzuführen, musste die \ac{zero} Stufe 3 Optimierung mit CPU Offloading gewählt werden.\\

% - genertierter Text ohne Stopzeichen: Stopzeichen händisch nach NewLine (double Newline?)
% - Kleinere Block\_size für Loss Scaling
% - BF16 nur auf AMP \ac{gpu} Architektur möglich (hier nicht)
% - Stage2/3 ohne CPU Offloading führt zu OOM
% - Deutlich mehr RAM erforderlich (180GB?)
% - Multi-Node Training versursacht Kommunikationsfehler



%
% - requirements txt erstellen
% - DeepSpeed Config mit https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters+
% - Nutzung von ZeRO Stage2 optimization
% - llama2 7b genutzt
% - dataclasses um Datensatz einzulesen
% - Leistungen bei 11s/it von 3600 Iteration = 11h Training
% - Evaluation aller 50 Iterations, Saves aller 100 Iterations
% - 3 Epochen = 14s/it 1800 = 7h Training (256 Block\_size)
% - Deepspeed Launcher mit 3 \ac{gpu}s (32GB v100, 1 Maschine)
% - Weitere Hyperparameter aus Llama 2 Modell
% - Nutzung von Epub anstelle Word Datei da bessere Konvertierung
% - keine nutung von chat Version (verlernt Human Reinforcment)