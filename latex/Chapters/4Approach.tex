%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************
\section*{Vorgehen}
- Rerchnernetz Aufbau und Konfiguration (warum reicht nicht eigener Computer)

- Auswahl Autoregressives Modell
- Datenkuration Blaues Buch
- Unüberwacht Weitertrainiert = Continual Pretraining
    - Nutzung von HuggingFace Trainer
    - Parallelisierung auf Rechennetz

- Auswahl Beispielklausuren
- Beantwortung von Fragen

- Modellvergleich
    - Omar folgende Kriterien
    - Precision, Recall, F-Score als Benchmark
- Modellvergleich GPT4
    - Omar folgende Kriterien

\section{Auswahl von Sprachmodellen}

Für die Auswahl eines Sprachmodells müssen verschiedene Kriterien erfüllt sein.
Ausgehend von der Aufgabenstellung in \ref{sec:zielsetzung} stehen verschiedene Modelle der Transformer-Architektur zur Auswahl.
Die Auswahl beschränkt sich auf Decoder-basierte Modelle, da diese die Eigenschaften eines autoregressiven Modells besitzen.
Wie bereits in \ref{sec:grundlagen_transformer} beschrieben, ist das Ziel die Generierung von Text auf Basis einer Fragestellung.
Diese Fragestellung wird zu Beginn als Eingabe zur Verfügung gestellt, woraufhin das Modell kontinuierlich neue Tokens generiert.
Neu generierte Tokens werden in Verbindung mit der ursprünglichen Fragestellung als Eingabe verwendet, um das nächste Token zu generieren.
Dieses Verhalten entspricht autoregressiven Modellen und erfordert Decoder-basierte Modelle.
Ein Decoder-basiertes Modell verwendet alle vorhergehenden Tokens zur Generierung von Tokens (auch als Vorhersage von Tokens bezeichnet), während ein Encoder-Modell sowohl vorhergehende als auch nachfolgende Tokens verwendet.
Encoder-basierte Modelle sind für die Textvervollständigung oder Textklassifikation gedacht, aber nicht für die Textgenerierung.\\

Darüber hinaus sind folgende Eigenschaften erwünscht. Das Modell sollte eine gute ZeroShot-Performance haben und nicht nur eine gute FewShot- oder OneShot-Performance.
Diese Arbeit untersucht die Fähigkeit eines Modells, Wissen aus domänenspezifischer Literatur (hier aus \citet{bb}) zu reproduzieren.
Durch die Verwendung früherer Fragen und Antworten als Kontext werden die Ergebnisse synthetisch durch zusätzliches Wissen im Kontext verbessert.
Einige Fragen könnten nicht durch das Modell mit Hilfe des gelernten Wissens beantwortet werden, sondern durch das Wissen im Kontext.
Das Modell sollte auch ohne Fine-Tuning eine gute Leistung erbringen.
Da ein Fine-Tuning in dieser Arbeit aufgrund der fehlenden Datenmenge nicht möglich ist, sollte ein Modell gewählt werden, das auch ohne diess Fine-Tuning eine gute Performance erreicht.\\

Die hier zur Auswahl stehenden Modelle sind GPT-2, GPT-3, GPT-4, GPT-J, GPT-NeoX und LLaMa.
Modelle, die sehr populär geworden sind, aber nicht zur Auswahl stehen, sind BERT, RoBERTa, DistillBERT, BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5.
Wenn die Aufgabe des Question Answering gelöst werden soll, ist ein BERT-basiertes Modell die erste Wahl. 
Modelle wie BERT, RoBERTa und DistillBERT besitzen aufgrund ihrer Encoder-Architektur eine optimale Eigenschaft zur Informationsextraktion aus Text.
Allerdings ist dieser Ansatz in zwei Punkten eingeschränkt. Die Modelle lernen nicht Fakten und können auf Basis einer Fragestellung neue Antworten formulieren, sondern finden Textstellen in einem Kontext, die die Fragestellung beantworten.
Es findet keine Textgenerierung statt, sondern es werden Ausschnitte aus einem gegebenen Textkontext als Antwort gefunden.
Dieser Kontext ist wiederum durch die Größe des Modells begrenzt und beschränkt sich in den meisten Fällen auf 512 Tokens.
Ist die Antwort auf die Frage nicht im Kontext enthalten, kann das Modell keine Antwort finden. Da die Fragen unabhängig vom Kontext beantwortet werden sollen, können diese Modelle nicht verwendet werden.
Andere Modelle wie BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5 sind für andere Aufgabenstellungen konzipiert und können nicht auf die gegebene Aufgabenstellung angewendet werden. Sie sind für Textklassifikation, Textübersetzung, Textzusammenfassung, Dialoge oder Text-2-Text-Aufgaben ausgelegt.\\

GPT-2 ist das kleinste Modell der OpenAI GPT-Serie und wurde erstmals in \citet{gpt2} vorgestellt.
Aufgrund seiner geringen Größe kann das Modell auf einzelnen GPUs trainiert und verwendet (inferiert) werden, was die Nutzung des Modells erheblich vereinfacht.
Außerdem ist es frei verfügbar und kann ohne weitere Maßnahmen verwendet werden. Allerdings ist die Performance in der Einstellung ZeroShot nicht ausreichend.
Das Modell erreicht eine Imitation von Texten, aber kein Verständnis und keine Wiedergabe von Fakten.\\

GPT-3 und das darauf aufbauende GPT-3.5 (ChatGPT), vorgestellt in \citet{gpt3}, kann zum Zeitpunkt der Arbeit kostenlos genutzt werden, hat eine sehr gute ZeroShot-Performance und besitzt die notwendige Größe, um als QA-Modell zu fungieren. Allerdings ist das Modell nicht frei verfügbar.
Es existiert zwar eine \ac{api}, diese dient jedoch nur zur Interferenz des Modells, ein Training ist nicht möglich.
Damit fällt das Modell bereits aus der Auswahl. Ein weiteres Problem stellt die Größe des Modells dar.
Mit ca. 175 Milliarden Parametern ist eine Nutzung des Modells nur mit 8 A100 GPUs möglich. Das Training benötigt zusätzlich etwa die 4-fache Leistung.
Diese Leistung kann zwar vom Rechenzentrum der Universität Leipzig bereitgestellt werden, verhindert aber den Einsatz des Modells in einer längerfristigen Umgebung.\\

GPT-4 ist das mächtigste Modell von OpenAI und wurde in \citet{gpt4} vorgestellt.
Auch dieses Modell ist nicht frei verfügbar und kann nur über eine \ac{api} genutzt werden.
Die Leistungsfähigkeit von GPT-4 ist jedoch deutlich höher als die von GPT-3, weshalb die Untersuchung dieses Modells als Ersatz für ein eigens trainiertes Modell geplant ist.
Die Größe des Modells wurde von OpenAI nicht veröffentlicht, ist aber definitiv größer als GPT-3, weshalb eine Verwendung hier allein aufgrund der Größe ausscheidet.\\

GPT-J erreicht mit 6,7 Milliarden Parametern die Größe des kleinsten Modells GPT-3, zeigt aber eine deutlich bessere Performance als GPT-2.
Das Modell ist unter \citet{gptj} veröffentlicht und frei verfügbar.
Die Leistungsfähigkeit des Modells ist im Vergleich zu den anderen Modellen in der Auswahl geringer und wird daher ausgeschlossen.\\

GPT-NeoX wurde in \citet{gpt_neox} vorgestellt und erreicht mit einer Größe von 20 Milliarden Parametern die Leistungsfähigkeit von GPT-3 mit 175 Milliarden Parametern.
Das Modell ist frei verfügbar und kann ohne weitere Maßnahmen verwendet werden.
Die Leistung des Modells ist vergleichbar mit GPT-3, während die Größe des Modells ein Training mit 8 A100 GPUs erlaubt.
Diese Leistung wird jedoch von LLaMa übertroffen und daher nicht ausgewählt.\\

Die in \citet{llama} vorgestellten LLaMa-Modelle haben einen besonderen Ansatz.
Durch die Ausrichtung auf längeres Training und größere Datensätze bieten sie parametereffiziente Modelle, die sehr gute Leistungen erzielen können.
Um die bestmögliche Performance mit den kleinstmöglichen Modellen zu erreichen, werden die Modelle von LLaMa ausgewählt.
Hier stellt sich die Frage, welches Modell aus der LLaMa-Reihe verwendet werden soll.
Die Modelle unterscheiden sich sowohl in der Größe als auch proportional in der Leistung.
Abbildung \ref{llama_naturalquestion} zeigt die Leistung der Modelle im Vergleich zu anderen Modellen und in verschiedenen Einstellungen (ZeroShot bis 64Shot).
Selbst das kleinste Modell mit 7 Milliarden Parametern übertrifft die Leistung von GPT-3 und kann während der Interferenz auf einer einzelnen V100 GPU verwendet werden.
Nach Angaben der Autoren kann auch das 13 Milliarden Parameter Modell auf einer einzelnen V100 GPU genutzt werden, was für die Wahl dieses Modells spricht.
Das 33 Milliarden Parameter Modell erzielt sogar etwas bessere Ergebnisse im ZeroShot als das 65 Milliarden Parameter Modell, weshalb hier die Wahl des 33 Milliarden Parameter Modells sinnvoll ist.
Mit 33 Milliarden Parametern und der Verwendung von Float16 Werten (auch Half-Precision genannt) benötigt das Modell 66 GB GPU RAM, um für die Interferenz verwendet werden zu können.
Für das Training werden 264 GB GPU RAM benötigt, was 4 A100 GPUs entspricht.
Das Modell mit 65 Milliarden Parametern benötigt 7 A100 GPUs für das Training und 2 A100 GPUs für die Nutzung.
Aufgrund der vielversprechenden ZeroShot Performance wird in dieser Arbeit das 33 Milliarden Parameter Modell gewählt.
Die Modelle sind jedoch nicht öffentlich verfügbar, sondern werden nur zu Forschungszwecken über einen Antrag veröffentlicht.
Dieser Antrag wurde im Rahmen dieser Arbeit gestellt und genehmigt.\\

Eine Alternative zu den LLaMa-Modellen ist OpenLLaMA, veröffentlicht unter \citet{openllama}.
Die hier verfügbaren Modelle mit 3 Milliarden, 7 Milliarden und 13 Milliarden Parametern entsprechen in ihrer Leistungsfähigkeit den LLaMa-Modellen, sind aber unter der Open-Source-Lizenz Apache 2.0 verfügbar. 
Da jedoch kein Modell mit 33 Milliarden Parametern verfügbar ist, wird auf diese Modelle verzichtet.\\


- Modelle mit speziellen Eigenschaften
    - Transformer-Architektur
    - Decoder-basiert
    - Autoregressiv
    - Gewichte sind verfügbar

- Modelle mit gewünschten Eigenschaften
    - gute Zero-Shot Performance
    - ohne Fine-Tuning nutzbar
    - überschreitet nicht Kapazität des Rechnernetzes

- Modelle zur Auswahl
    - GPT-2
    - GPT-3
    - GPT-4
    - GPT-J
    - GPT-NeoX
    - LLaMa

- Modelle die nicht zur Auswahl stehen
    - BERT-basierend (Text Klassifikation): BERT, RoBERTa, DistillBERT
    - ZeroShot Classification: BART
    - Übersetzung: T5, Opus
    - Zusammenfassung: Pegasus
    - Dialogbasierend: DialoGPT, Blenderbot
    - Text-2-Text: FLan-T5


- Auswahlentscheidung
    - GPT-2: klein, einfach zu trainieren, öffentlich verfügbar, keine gute ZeroShot Performance
    - GPT-3 \& GPT-4: groß, sehr gute Zeroshot Performance, nicht öffentlich verfügbar, überschreitet Kapazität des Rechnernetzes
    - GPT-J: zwischen GPT-2 und GPT-3, öffentlich verfügbar, keine gute ZeroShot Performance
    - GPT-NeoX: groß, sehr gute Zeroshot Performance, öffentlich verfügbar, überschreitet Kapazität des Rechnernetzes
    - LLaMa: verschiedene Größen - optimale Größe auswählbar, sehr gute Zeroshot Performance, öffentlich verfügbar*, wird ausgewählt

\section{Datenkuration}

- Formate die das Modell nicht verarbeiten kann
    - Bilder

Textpassagen ohne Wissen oder Kontext:
    - Bild-Beschreibun
    - Seitenzahlen
    - Vorwort
    - Autorenvorstellung

Optionale Textentfernung mit potentiell besserer Leistung:
    - Inhaltsverzeichnis
    - Literaturverzeichnis
    - Stichwortverzeichnis
    - Überschriften

\section{Unüberwachtes Weitertrainieren}
Um das LLaMa Modell zu trainieren, wird ein Programm benötigt, dass den Datensatz in Tokens umwandelt, und diese dann in Batches dem Modell übergibt. Auf Basis dieser Batches wird das Modell genutzt um die Gewichte mit Backpropagation anzupassen. Dieser Prozess stellt demnach eine Epoche dar. Bei kleineren Datensätzen und Fine-Tuning wird häufig mehrere Epochen gewählt, während größere Datensätze (mehrere hundert Gigabyte) nur eine Epoche oder nicht einmal eine Epoche benötigen. Mit steigender Epoche passt das Modell sich weiter an den Text der Literatur an, wobei ab einem bestimmten Zeitpunkt eine Grenze überschritten wird. Diese Grenze wird häufig als \enquote{Overfitting} bezeichnet, und repräsentiert ein Modell, welches sich zu sehr an einen Datensatz angepasst hat. Dadurch verliert das Modell die Fähigkeit, auf unbekannte Daten zu generalisieren, welches im diesem Anwendungsfall dazuführt, dass keine Fragen mehr beantwortet werden können.\\

Um Overfitting zu vermeiden, wird der Datensatz in Trainings- und Validierungsdatensatz aufgeteilt. Hierbei wird eine Unterteilung von 90\% Training, 10\% Validierung genutzt. Das Modell trainiert nur auf dem Trainingsdatensatz, wird jedoch in regelmäßigen Abschnitten mit dem Validierungsdatensatz überprüft. Die Ergebnisse des Validierungsdatensatz haben keinen Einfluss auf die Anpassung der Gewichte sondern dienen nur zur Überprüfung, in wie fern das Modell generalisieren kann.
Erwartet wird ein Abfall der Kost-Funktion sowohl auf dem Trainings- als auch auf dem Validierungsdatensatz. Dieser Abfall ist ein Indikator dafür, dass das Modell sich an den Datensatz anpasst. Sobald der Abfall auf dem Validierungsdatensatz aufhört, ist das Training abgeschlossen. Die Ergebnisse zeigen in den meisten Fällen, dass die Kostfunktion des Trainingsdatensatzes tiefer als die des Validierungsdatensatzes ist. Solange die Differenz zwischen den beiden Kostfunktionen nicht zu groß ist, konnte das Modell erfolgreich trainiert werden.\\

Zur Durchführung dieser Trainingsschritte wird die Programmiersprache Python genutzt, da sie eine große Anzahl an Bibliotheken für die Verarbeitung von Texten und die Nutzung von neuronalen Netzen bietet. Die Bibliothek HuggingFace bietet eine Vielzahl an Modellen und Trainingsmethoden und baut auf der populären Bibliothek PyTorch auf.\\
- HuggingFace Trainer
- Parallelisierung auf verschiedenen GPUS
- Kommunikation zwischen GPUS mit Paket

- Alternativen zu Trainer
    - PyTorch
    - PyTorchLighning
    - Tensorflow

- Alternativen zu SLURM: keine, vorgegeben

\section{Ausführen des Training-Programme}
- Nutzung von SLURM
- Kommunikation zwischen GPUS und Nodes
- Long-Time Nodes
- Speicherung von Modellen mit Checkpoints
- Größe der Modelle = Anzahl notwendiger GPUs

\section{Klausurfragen}
- Klausurfragen Antwortenerstellung
- Umformulierung zu Prompt (da Finetuning nicht vorhanden)

\section{Modellvergleich}
- Berechnung von Precision, Recall, F-Score
- Aggregation der Ergebnisse

Nutzung von Omar Kriterien:
    - Correctness
    - Determinism
    - Robustness
    - Explainability
    - Question understanding

Nicht genutzte Omar Kriterien
    - Incorporating recent Information
    - Generailty across different domains
    
- Vergleich der Modelle
- Vergleich mit SOTA Modell
    - Nutzung von Kontext als Input?

