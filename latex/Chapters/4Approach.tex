%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************
Diese Arbeit beschäftigt sich mit der Beantwortung von Wissensfragen zu \citet{bb} mit Hilfe eines vortrainierten Sprachmodells,
das unter Verwendung des Buches \citet{bb} auf diese Domäne trainiert wird.
Die Aufgabenstellung unterteilt sich in fünf Teile:
\begin{enumerate}
    \item Die Auswahl eines Sprachmodells, welches die Aufgabe lösen kann.
    \item Die Kuration und Umwandlung des Buches von \citet{bb} in eine von dem Modell verständliche Form.
    \item Die Durchführung des Trainings des ausgewählten Modells.
    \item Die Erstellung eines Evaluierungsdatensatzes auf Basis von Klausuren, welche inhaltlich auf dem Buch von \citet{bb} basieren.
    \item Die Evaluierung des Modells mit Hilfe des erstellten Evaluierungsdatensatzes.
\end{enumerate}

\section{Auswahl von Sprachmodellen}
Für die Auswahl eines Sprachmodells müssen verschiedene Kriterien erfüllt sein.
Ausgehend von der Aufgabenstellung in \cref{sec:zielsetzung} stehen verschiedene Modelle der Transformer-Architektur zur Auswahl.
Die Auswahl beschränkt sich auf Decoder-basierte Modelle, da diese die Eigenschaften eines autoregressiven Modells besitzen.
Wie bereits in \cref{sec:grundlagen:transformer} beschrieben, ist das Ziel die Generierung von Text auf Basis einer Fragestellung.
Diese Fragestellung wird zu Beginn als Eingabe zur Verfügung gestellt, woraufhin das Modell kontinuierlich neue Tokens generiert.
Neu generierte Tokens werden zusammen mit der ursprünglichen Frage als Eingabe verwendet, um den nächsten Token zu generieren.
Dieses Verhalten entspricht autoregressiven Modellen und erfordert Decoder-basierte Modelle.
Ein Decoder-basiertes Modell verwendet sowohl Encoder als auch Decoder-Schichten um Eingaben zu verstehen und daraus Text zu generieren.
Encoder-basierte Modelle besitzen in der Regel keine Decoder-Schichten und sind nur für die Textvervollständigung oder Textklassifikation gedacht, aber nicht für die Textgenerierung.\\

Zusätzlich sind folgende Eigenschaften erwünscht.
Das Modell soll eine gute ZeroShot-Performance haben und nicht nur eine gute FewShot- oder OneShot-Performance.
Diese Arbeit untersucht die Fähigkeit eines Modells, Wissen aus domänenspezifischer Literatur (hier aus \citet{bb}) zu reproduzieren.
Indem frühere Fragen und Antworten als Kontext verwendet werden, werden die Ergebnisse synthetisch durch zusätzliches Wissen im Kontext verbessert.
Einige Fragen könnten nicht durch das Modell mit Hilfe des gelernten Wissens beantwortet werden, sondern durch das Wissen im Kontext.
Das Modell soll auch ohne Fine-Tuning eine gute Leistung erbringen.
Da ein Fine-Tuning in dieser Arbeit aufgrund der fehlenden Datenmenge nicht möglich ist, soll ein Modell gewählt werden, das auch ohne dieses Fine-Tuning eine gute Performance erreicht.\\

\begin{table}
    \centering
    \setlength{\tabcolsep}{5pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lllrrr}
        \toprule
        Modell      & Verfügbarkeit     & ZeroShot  & Größe                    & Training & Inferenz\\
        \midrule
        GPT-2       & frei              & schlecht  & \SI{1,5}{Mrd.~Parameter} & \SI{24}{\giga\byte}    & \SI{6}{\giga\byte}\\
        GPT-3       & nur Inferenz      & gut       & \SI{175}{Mrd.~Parameter} & \SI{1,4}{\tera\byte}   & \SI{350}{\giga\byte}\\
        GPT-4       & nur Inferenz      & sehr gut  & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} \\
        GPT-J       & frei              & gut       & \SI{6,7}{Mrd.~Parameter} & \SI{104}{\giga\byte}   & \SI{26}{\giga\byte}\\
        GPT-NeoX    & frei              & gut       & \SI{20}{Mrd.~Parameter}  & \SI{160}{\giga\byte}   & \SI{40}{\giga\byte}\\
        \multirow{4}{*}{LLaMA}
                    & limitiert         & gut       & \SI{7}{Mrd.~Parameter}   & \SI{56}{\giga\byte}    & \SI{14}{\giga\byte}\\
                    & limitiert         & gut       & \SI{13}{Mrd.~Parameter}  & \SI{104}{\giga\byte}   & \SI{23}{\giga\byte}\\
                    & limitiert         & sehr gut  & \SI{33}{Mrd.~Parameter}  & \SI{264}{\giga\byte}   & \SI{66}{\giga\byte}\\
                    & limitiert         & sehr gut  & \SI{65}{Mrd.~Parameter}  & \SI{520}{\giga\byte}   & \SI{130}{\giga\byte}\\
        \bottomrule
    \end{tabular}}
    \caption{Übersicht über die zur Auswahl stehenden Modelle.\label{tab:approach:models}}
\end{table}

Hier zur Auswahl stehendene Modelle sind GPT-2, GPT-3, GPT-4, GPT-J, GPT-NeoX und LLaMA, gezeigt in \cref{tab:approach:models}.
Die angegebenen Speichergrößen beziehen sich hier auf die notwendige Speichermenge zum Training des Modells und der Inferenz.
Berechnet wird die Speicheranforderung zum Training mit der \cref{eq:speicher}.
\begin{equation}\label{eq:speicher}
    \text{Training Speicherbedarf}=\text{Anzahl Parameter} \cdot \frac{\text{Parameterformat}}{8} \cdot 4
\end{equation}
Sprachmodelle benutzen die Parameterformate FP16 oder FP32, welche 2 bzw. 4 Byte pro Parameter benötigen. Zusätzlich muss das Modell während des Trainings circa 4 mal in den Speicher geladen werden, um die Backpropagation durchzuführen. 
Zur Inferenz der Modelle ist dementsprechend ein Speicherbedarf von $\frac{1}{4}$ der Trainingswerte notwendig.
Modelle, die sehr populär geworden sind, aber nicht zur Auswahl stehen, sind BERT, RoBERTa, DistillBERT, BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5.
Wenn die Aufgabe des Question Answering gelöst werden soll, ist ein BERT-basiertes Modell die erste Wahl.\\

Modelle wie BERT, RoBERTa und DistillBERT besitzen aufgrund ihrer Encoder-Architektur optimale Eigenschaften für die Informationsextraktion aus Text.
Dieser Ansatz ist jedoch in zwei Punkten eingeschränkt.
Die Modelle lernen nicht Fakten und können auf Basis einer Frage neue Antworten formulieren, sondern finden Textstellen in einem Kontext, die die Frage beantworten.
Es findet keine Textgenerierung statt, sondern es werden Ausschnitte aus einem gegebenen Textkontext als Antwort gefunden.
Dieser Kontext ist wiederum durch die Größe des Modells begrenzt und beschränkt sich in den meisten Fällen auf 512 Tokens.
Ist die Antwort auf die Frage nicht im Kontext enthalten, kann das Modell keine Antwort finden.
Da die Fragen unabhängig vom Kontext beantwortet werden sollen, können diese Modelle nicht verwendet werden.
Andere Modelle wie BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5 wurden für andere Aufgaben entwickelt und können nicht auf die vorliegende Aufgabe angewendet werden.
Sie sind für Textklassifikation, Textübersetzung, Textzusammenfassung, Dialoge oder Text-2-Text-Aufgaben konzipiert.\\

GPT-2 ist das kleinste Modell der OpenAI GPT-Serie und wurde erstmals in \citet{gpt2} vorgestellt.
Aufgrund seiner geringen Größe kann das Modell auf einzelnen \ac{gpu}s trainiert und verwendet (inferiert) werden, was die Nutzung des Modells erheblich vereinfacht.
Außerdem ist es frei verfügbar und kann ohne weitere Maßnahmen verwendet werden.
Allerdings ist die Performance in der Einstellung ZeroShot nicht ausreichend.
Das Modell erreicht eine Imitation von Texten, aber kein Verständnis und keine Wiedergabe von Fakten.\\

GPT-3 und das darauf basierende GPT-3.5 (ChatGPT), vorgestellt in \citet{gpt3}, ist zum Zeitpunkt der Arbeit frei nutzbar, hat eine sehr gute ZeroShot-Performance und ist groß genug, um als QA-Modell zu dienen.
Das Modell ist jedoch nicht frei verfügbar.
Es existiert zwar eine \ac{api}, diese dient jedoch nur zur Inferenz des Modells, ein Training ist nicht möglich.
Damit scheidet das Modell bereits aus.
Ein weiteres Problem ist die Größe des Modells.
Bei ca. 175 Milliarden Parametern ist eine Nutzung des Modells erst mit 8 A100 \ac{gpu}s möglich (eine Nvidia A100 \ac{gpu} besitzt \SI{80}{\giga\byte} VRAM).
Das Training benötigt zusätzlich etwa den 4-fache Speicher.
Diesen Speicher kann zwar vom Rechenzentrum der Universität Leipzig zur Verfügung gestellt werden, verhindert aber den Einsatz des Modells in einer längerfristigen Umgebung.\\

GPT-4 ist das mächtigste Modell von OpenAI und wurde in \citet{gpt4} vorgestellt.
Auch dieses Modell ist nicht frei verfügbar und kann nur über eine \ac{api} verwendet werden.
Die Leistungsfähigkeit von GPT-4 ist jedoch deutlich höher als die von GPT-3, weshalb die Untersuchung dieses Modells als Ersatz für ein speziell trainiertes Modell geplant ist.
Die Größe des Modells wurde von OpenAI nicht veröffentlicht, ist aber definitiv größer als GPT-3, weshalb eine Verwendung hier allein aufgrund der Größe ausscheidet.\\

GPT-J erreicht mit \SI{6,7}{} Milliarden Parametern die Größe des kleinsten Modells der GPT-3 Serie, zeigt aber eine deutlich bessere Performance als GPT-2.
Das Modell ist unter \citet{gptj} veröffentlicht und frei verfügbar.
Die Leistungsfähigkeit des Modells ist im Vergleich zu den anderen Modellen in der Auswahl geringer und wird daher ausgeschlossen.\\

GPT-NeoX wurde in \citet{gpt_neox} vorgestellt und erreicht mit einer Größe von 20 Milliarden Parametern die Leistungsfähigkeit von GPT-3 mit 175 Milliarden Parametern.
Das Modell ist frei verfügbar und kann ohne weitere Maßnahmen verwendet werden.
Die Leistung des Modells ist vergleichbar mit GPT-3, während die Größe des Modells ein Training mit 8 A100 \ac{gpu}s erlaubt.
Diese Leistung wird jedoch von LLaMA übertroffen und daher nicht ausgewählt.\\

Die in \citet{llama} vorgestellten LLaMA-Modelle haben einen besonderen Ansatz.
Durch den Fokus auf längeres Training und größere Datensätze bieten sie parametereffiziente Modelle, die sehr gute Leistungen erzielen können.
Die Modelle von LLaMA werden ausgewählt, um die bestmögliche Performance mit den kleinstmöglichen Modellen zu erreichen.
Hier stellt sich die Frage, welches Modell aus der LLaMA-Reihe verwendet werden soll.
Die Modelle unterscheiden sich sowohl in der Größe als auch proportional in der Leistung.\\

\begin{table}[t!]
    \centering
    \setlength{\tabcolsep}{5pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrccccccccc}
    \toprule
    & & BoolQ & PIQA & SIQA & \hspace{-0.3cm} HellaSwag \hspace{-0.2cm} & \hspace{-0.2cm} WinoGrande \hspace{-0.3cm} & ARC-e & ARC-c & OBQA \\
    \midrule
    GPT-3        & 175B & \SI{60,5}{} & \SI{81,0}{} & -    & \SI{78,9}{} & \SI{70,2}{} & \SI{68,8}{} & \SI{51,4}{} & \SI{57,6}{} \\
    Gopher       & 280B & \SI{79,3}{} & \SI{81,8}{} & \SI{50,6}{} & \SI{79,2}{} & \SI{70,1}{} & -    & -    & -    \\
    Chinchilla   & 70B  & \SI{83,7}{} & \SI{81,8}{} & \SI{51,3}{} & \SI{80,8}{} & \SI{74,9}{} & -    & -    & -    \\
    PaLM         & 62B  & \SI{84,8}{} & \SI{80,5}{} & -    & \SI{79,7}{} & \SI{77,0}{} & \SI{75,2}{} & \SI{52,5}{} & \SI{50,4}{} \\
    PaLM-cont    & 62B  & \SI{83,9}{} & \SI{81,4}{} & -    & \SI{80,6}{} & \SI{77,0}{} & -    & -    & -    \\
    PaLM         & 540B & \textbf{88,0} & \SI{82,3}{} & - & \SI{83,4}{} & \textbf{81,1} & \SI{76,6}{} & \SI{53,0}{} & \SI{53,4}{} \\
    \midrule
    \multirow{4}{*}{LLaMA}
       & 7B   & \SI{76,5}{} & \SI{79,8}{} & \SI{48,9}{} & \SI{76,1}{} & \SI{70,1}{} & \SI{72,8}{}          & \SI{47,6}{}          & \SI{57,2}{} \\
       & 13B  & \SI{78,1}{} & \SI{80,1}{} & \SI{50,4}{} & \SI{79,2}{} & \SI{73,0}{} & \SI{74,8}{}          & \SI{52,7}{}          & \SI{56,4}{} \\
       & 33B  & \SI{83,1}{} & \SI{82,3}{} & \SI{50,4}{} & \SI{82,8}{} & \SI{76,0}{} & \textbf{80,0} & \textbf{57,8} & \SI{58,6}{} \\
       & 65B  & \SI{85,3}{} & \textbf{82,8} & \textbf{52,3} & \textbf{84,2} & \SI{77,0}{} & \SI{78,9}{} & \SI{56,0}{} & \textbf{60,2} \\
  \bottomrule
    \end{tabular}}
\caption{
Zero-Shot Leistung verschiedener Modelle in Begründungsaufgaben für gesunden Menschenverstand.
Veröffentlicht in \citet{llama}\label{tab:llama_naturalquestion}
}
\end{table}

Die \cref{tab:llama_naturalquestion} zeigt die Leistungsfähigkeit der Modelle im Vergleich zu anderen Modellen und bei verschiedenen Einstellungen (ZeroShot bis 64Shot).
Selbst das kleinste Modell mit 7 Milliarden Parametern übertrifft die Leistung von GPT-3 und kann während der Inferenz auf einer einzelnen V100 \ac{gpu} verwendet werden (eine Nvidia V100 \ac{gpu} besitzt \SI{32}{\giga\byte} VRAM).
Nach Angaben der Autoren kann auch das Modell mit 13 Milliarden Parametern auf einer einzigen V100 \ac{gpu} eingesetzt werden, was für die Wahl dieses Modells spricht.
Das 33-Milliarden-Parameter-Modell erzielt im ZeroShot in Teilen sogar etwas bessere Ergebnisse als das 65-Milliarden-Parameter-Modell, weshalb eine Wahl des 33 Milliarden oder 65 Milliarden Parameter Modells hier sinnvoll ist.
Mit 33 Milliarden Parametern und der Verwendung von FP16 Werten (auch Half-Precision genannt) benötigt das Modell \SI{66}{\giga\byte} \ac{gpu} RAM, um für die Inferenz verwendet werden zu können.
Für das Training werden \SI{264}{\giga\byte} \ac{gpu} RAM benötigt, was 4 A100 \ac{gpu}s entspricht.
Das Modell mit 65 Milliarden Parametern benötigt 7 A100 \ac{gpu}s für das Training und 2 A100 \ac{gpu}s für den Betrieb.
Die Modelle sind jedoch nicht öffentlich verfügbar, sondern werden nur zu Forschungszwecken über einen Antrag weitergegeben.
Dieser Antrag wurde im Rahmen dieser Arbeit gestellt und genehmigt.\\

Eine Alternative zu den LLaMA-Modellen ist OpenLLaMA, veröffentlicht unter \citet{openllama}.
Die hier verfügbaren Modelle mit 3 Milliarden, 7 Milliarden und 13 Milliarden Parametern entsprechen in ihrer Leistungsfähigkeit den LLaMA-Modellen, sind aber unter der Open-Source-Lizenz Apache 2.0 verfügbar. Da hier jedoch keine Publikation über den Prozess der Erstellung dieser Modelle vorhanden ist, wird von der Verwendung dieser Modelle vorerst abgesehen.\\

In dieser Arbeit wird zunächst das Modell LLaMA 7B ausgewählt, da es eine sehr gute Leistung mit minimalen Ressourcenanforderungen bietet.
Eine Auswahl dieser Modellgröße begründet sich daher, dass eine Nutzung der Modelle ab 33 Milliarden Parameter auf einzelnen GPUs nicht möglich ist, und dadurch die Nutzbarkeit eingeschränkt wird.
Ebenso übersteigen die technischen Speicheranforderungen größerer Modelle die Kapazitäten einzelner Knoten im genutzten GPU-Computing Cluster, weshalb dadurch die technische Umsetzung des Trainings drastisch in ihrer Komplexität zunimmt.
Einzelne Tests der Knoten-übergreifenden Kommunikation zeigten grundlegende Probleme und sollten deshalb vermieden werden.
Auf weitere Einzelheiten über den Aufbau der verwendenten GPU-Computing Clusters wird in \cref{sec:approach:training} eingegangen.\\

\section{Datenkuration}\label{sec:datenkuration}
Um die Literatur optimal nutzen zu können, muss eine Datenkuration durchgeführt werden.
Die Daten liegen im Word-Format vor, können aber in diesem Format nicht verarbeitet werden, da das Modell nur Text verarbeiten kann.
Die Datenkuration umfasst mehrere Schritte, die im Folgenden beschrieben werden.\\

\subsection{Extraktion des Textes}
Die Extraktion des Textes wird händisch durchgeführt.
Dazu wird das Word-Dokument geöffnet und der Text kopiert.
Der Text wird in einem Texteditor eingefügt und als \texttt{.md} Datei gespeichert.
\texttt{.md}-Dateien folgen dem Markdown-Format\footnote{\url{https://markdown.de/} (abgerufen am 28.6.2023)} und haben die besondere Eigenschaft, mit Hilfe von Text-Symbolen Formatierungen wie Überschriften, Aufzählungen und Tabellen darzustellen.

\subsection{Unverständliche Formate}
Während der Extraktion des Textes fallen alle Bilder als auch Kopf- und Fußzeilen weg.
Da eine einzelne Text-Datei keine sinnvolle Unterteilung in Seiten hat, sind Kopf- und Fußzeilen und damit einhergehend Seitenzahlen nicht relevant.
Bilder können nicht von dem Modell verarbeitet werden und werden daher entfernt.
Die Bildunterschriften als auch Referenzen auf Bilder stehen somit ohne Kontext in den Kapiteln und werden ebenso entfernt.

\subsection{Textpassagen ohne Wissen oder Kontext}
Teile des Dokuments enthalten kein Wissen, stehen in keinem Zusammenhang oder sind für das Modell unverständlich formatiert.
Die Titelseite wird auf den Buchtitel reduziert, das Vorwort wird weggelassen, da es kein Wissen zum Thema enthält, Tabellen- und Abbildungsverzeichnisse werden ebenfalls entfernt.
Bild- und Tabellenverzeichnisse enthalten zwar Wissen, können aber nicht in Kontext gesetzt werden, da die verwendeten Attention-Mechanismen des Modells diese Verzeichnisse nicht mit Texten in späteren Kapiteln verknüpfen können, da der Kontext zu groß ist.
Auch die Vorstellung der Autoren enthält kein Wissen zum Thema und wird daher entfernt.
Zusätzlich werden fehlerhafte Textpassagen wie fehlende Verweise entfernt.
Diese Textstellen existieren nur in der Word-Version des Buches, nicht aber in der endgültigen PDF-Version.
Da eine Textextraktion aus PDF-Dateien jedoch zu gravierenden Problemen in der Textformatierung führt, wird darauf verzichtet.

\subsection{Optionale Textentfernungen}
Es soll geprüft werden, ob das Weglassen von Literaturverweisen, Überschriften und Tabellen zu einer besseren Leistung führt.
Literaturverzeichnisse beziehen sich auf das vorhergehende Kapitel, haben aber ein schwieriges Format.
Insbesondere \ac{url}s müssen vom Modell in Tokens zerlegt werden, die nicht den Inhalt der \ac{url}s widerspiegeln. 
Überschriften haben im Vergleich zum Text nur einen minimalen Anteil an der Gesamtzahl der Tokens, wodurch das in ihnen enthaltene Wissen verloren geht.
Die menschenlesbare Formatierung einer Überschrift hat keinen Einfluss auf die Bewertung durch das Modell.
Tabellen liegen in einem Format vor, das vom Modell nicht nativ verarbeitet werden kann.
Selbst die Tabellendarstellung in Markdown ist zwar einlesbar, aber es ist fraglich, ob sie verständlich ist.
Aufgrund der geringen Größe des Datensatzes ist auch nicht zu erwarten, dass dieses Format erlernt wird.
Daher wird hier eine Aufzählung als Tabellenersatz verwendet.

\subsection{Bleibende Texte}
Das Abkürzungsverzeichnis und das Glossar bleiben im Text enthalten, da sie für das Verständnis der Fragen wichtige Begriffe definieren.
Einige Fragen können Akronyme oder Fachbegriffe verwenden, die vom Modell verstanden werden müssen.

\subsection{Formatierung von Text}
Die Formatierung des Textes im Word-Dokument kann nicht berücksichtigt werden.
Daher muss hier ein Ersatz in Form einer Markdown-Formatierung gefunden werden.
Überschriften werden durch \enquote{\texttt{\#}}, Aufzählungen durch \enquote{\texttt{-}} und \enquote{\texttt{1.}} dargestellt.
Tabellen werden durch eine Aufzählung dargestellt, wobei die erste Zeile die Überschriften enthält und die zweite Zeile die Trennung zwischen Überschriften und Inhalt darstellt.
Die Trennung zwischen den Spalten erfolgt durch \enquote{\texttt{|}}.
Hoch- und tiefgestellte Zahlen werden durch \enquote{\texttt{\^}} und \enquote{\texttt{\_}} dargestellt.
Formeln werden in den \LaTeX{} Mathe-Modus umgewandelt.

\subsection{Potentielle Extraktion von Fragen}
Neben der Verwendung des Textes als Eingabe in das Modell ist auch eine weitergehende Nutzung möglich.
Die Übungsabschnitte sowie das Glossar und das Abkürzungsverzeichnis können als Quelle für Fragen genutzt werden,
um das Modell und andere Modelle besser bewerten zu können.

\section{Unüberwachtes Weitertrainieren}
Um das LLaMA-Modell zu trainieren, wird ein Programm benötigt, das den Datensatz in Tokens umwandelt und diese dann in Batches an das Modell übergibt.
Basierend auf diesen Batches wird das Modell verwendet, um die Gewichte mittels Backpropagation anzupassen.
Dieser Prozess stellt somit eine Epoche dar.
Bei kleineren Datensätzen und Fine-Tuning werden oft mehrere Epochen gewählt, während größere Datensätze (mehrere hundert Gigabyte) nur eine Epoche oder nicht einmal eine Epoche benötigen.
Mit zunehmender Epoche passt sich das Modell immer mehr an die Literatur an, bis eine Grenze überschritten wird.
Diese Grenze wird oft als \enquote{Overfitting} bezeichnet und stellt ein Modell dar, das sich zu sehr an einen Datensatz angepasst hat.
Dadurch verliert das Modell die Fähigkeit, auf unbekannte Daten zu verallgemeinern, was in diesem Anwendungsfall dazu führt, dass keine Fragen mehr beantwortet werden können.\\

Um Overfitting zu vermeiden, wird der Datensatz in einen Trainings- und einen Validierungsdatensatz aufgeteilt.
Dabei wird eine Aufteilung von \SI{90}{\percent} Training und \SI{10}{\percent} Validierung verwendet.
Das Modell wird nur auf dem Trainingsdatensatz trainiert, aber in regelmäßigen Abständen mit dem Validierungsdatensatz überprüft.
Die Ergebnisse des Validierungsdatensatzes haben keinen Einfluss auf die Anpassung der Gewichte, sondern dienen nur zur Überprüfung der Generalisierungsfähigkeit des Modells.
Sowohl für den Trainings- als auch für den Validierungsdatensatz wird ein Absinken der Fehlerfunktion erwartet.
Dieses Absinken ist ein Indikator dafür, dass das Modell sich an den Datensatz anpasst.
Sobald das Absinken auf dem Validierungsdatensatz aufhört, ist das Training abgeschlossen.
Die Ergebnisse zeigen in den meisten Fällen, dass die Fehlerfunktion des Trainingsdatensatzes niedriger als die des Validierungsdatensatzes ist.
Solange der Unterschied zwischen den beiden Fehlerfunktion nicht zu groß ist, konnte das Modell erfolgreich trainiert werden.\\

Für die Umsetzung dieser Trainingsschritte wird die Programmiersprache Python verwendet, da sie eine Vielzahl von Bibliotheken für die Textverarbeitung und den Einsatz von neuronalen Netzen bietet.
Die Bibliothek Transformers von HuggingFace\footnote{\url{https://huggingface.co/docs/transformers/index} (abgerufen am 28.6.2023)} bietet eine Vielzahl von Modellen und Trainingsmethoden und basiert auf der populären Bibliothek PyTorch\footnote{\url{https://pytorch.org/} (abgerufen am 28.6.2023)}.
PyTorch bietet mit Hilfe von CUDA die Möglichkeit, Trainingsschritte auf einer Nvidia \ac{gpu} durchzuführen, was die in \cref{subsec:grundlagen:training} beschriebenen Schritte um ein Vielfaches beschleunigt.
Mit Hilfe von DeepSpeed\footnote{\url{https://www.deepspeed.ai/} (abgerufen am 28.6.2023)} kann dieses Training auch auf mehrere \ac{gpu}s verteilt werden.
Dies ist notwendig, da das verwendete Modell mit 33 Milliarden Parametern zu groß ist, um in den Arbeitsspeicher einer einzelnen \ac{gpu} zu passen.\\

Vor Beginn des Trainings müssen einige Konfigurationsparameter festgelegt werden.
Diese Parameter sind in der \cref{tab:training:parameter} aufgelistet und folgen den Trainingsparametern von \citet{llama}.\\
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Wert} \\
        \midrule
        Lernrate & $3e^{-4}$\\
        AdamW $\beta_1$ & \SI{0,9}{}\\
        AdamW $\beta_2$ & \SI{0,95}{}\\
        Gewichtsreduktion & \SI{0,1}{}\\
        Gradientenlimitierung & \SI{1,0}{}\\
        Aufwärmphase & \SI{0}{}\\
        Epochen & \SI{2}{}\\
        \bottomrule
    \end{tabular}
    \caption{Parameter für das Training des LLaMA 7B-Modells}\label{tab:training:parameter}
\end{table}

Die Lernrate (engl. \enquote{Learning Rate}) ist ein Parameter, der das Ausmaß der Anpassung der Gewichte bestimmt (siehe \cref{def:lernrate}).
Sie wird während des Trainings angepasst, um eine schnelle Konvergenz zu ermöglichen.
Diese Anpassung folgt dem Kosinus-Lernratenschema, so dass die endgültige Lernrate \SI{10}{\percent} des Anfangswertes beträgt.
Die Lernraten hängen von der Modellgröße ab.
Im Allgemeinen benötigen größere Modelle eine niedrigere Lernrate.
Der Optimierer AdamW berücksichtigt den zeitlichen Abfall der Gewichte und wurde in \citet{adamw} vorgestellt.
Er ist die häufigste Wahl in aktuellen Modellen und wurde auch für das Training der LLaMA-Modelle verwendet.\\

Gewichtsabnahme (engl. \enquote{Weight Decay}) ist ein Parameter, der die Gewichte während des Trainings reduziert.
Er stellt eine Regularisierung dar, die Overfitting verhindern soll.
\SI{0,1}{} entspricht einer Gewichtsreduktion von \SI{10}{\percent}.\\

Die Gradientenbegrenzung (engl. \enquote{Gradient Clipping}) ist ein Parameter, der die Größe der Gradienten begrenzt.
Dies ist notwendig, da bei großen Modellen die Gradienten während der Backpropagation mit zunehmender Tiefe immer größer werden.
Ohne eine Begrenzung würden die Gradienten hier exponentiell wachsen.
Eine Gradientenbegrenzung beschränkt die Gradienten auf \SI{1,0}{}\\

Die Aufwärmphase beschreibt die Anzahl der Schritte, die das Modell benötigt, um die Lernrate von $0$ linear auf den Anfangswert zu erhöhen.
Dies ist notwendig, da sich die Modelle zu Beginn des Trainings zu schnell an nicht universelle Muster anpassen.
Normalerweise wird dies durch ein längeres Training korrigiert, bei dem die zuerst gelernten Muster wieder verlernt werden müssen.
Durch die Aufwärmphase wird ein längeres Training vermieden.
Da in diesem Fall das Modell bereits auf einem großen Datensatz trainiert wurde, ist eine Aufwärmphase nicht notwendig.\\

Der Artikel von \citet{llama} beschreibt ein Training über eine Epoche.
Da in diesem Fall jedoch der Datensatz im Vergleich minimal ist, wird eine größere Anzahl von Epochen verwendet.\\

\subsection{Ausführen der Training-Programme}\label{sec:approach:training}

Zur Ausführung der Trainingsprogramme wird SLURM\footnote{\url{https://slurm.schedmd.com/overview.html} (abgerufen am 16.6.2023)} verwendet.
Mit Hilfe von SLURM-Skripten können Programme über mehrere Knoten, \ac{cpu}s und \ac{gpu}s verteilt berechnet werden.
Diese Skriptsprache wird vom Rechenzentrum der Universität Leipzig verwendet, um verschiedene Programme auf den Supercomputern Clara und Paula laufen zu lassen.\\

Die Supercomputer Clara und Paula sind Rechennetze, die auf \ac{gpu} beschleunigte Berechnungen spezialisiert sind.
Paula besteht aus 12 Knoten mit je 8 Nvidia Tesla A30 \ac{gpu}s.
Clara ist zweigeteilt und besteht aus 8 Nodes mit je 4 Nvidia Tesla V100 \ac{gpu}s und 22 Nodes mit je 8 Nvidia GeForce RTX 2080 TI \ac{gpu}s.
Die Knoten sind über ein Infiniband-Netzwerk verbunden, das eine hohe Bandbreite und geringe Latenz bietet.\\

Supercomputer sind gemeinsam genutzte Ressourcen, die von mehreren Nutzern gebucht werden können.
Die Auswahl der Supercomputer hängt also davon ab, welche Knoten gerade frei sind.
Generell werden Clara-Nodes mit V100 \ac{gpu}s bevorzugt, da hier der größtmögliche Speicher pro \ac{gpu} zur Verfügung steht.
Dies hat zur Folge, dass weniger \ac{gpu}s gleichzeitig reserviert werden müssen, was die Wartezeit verkürzt und die Kommunikation zwischen den \ac{gpu}s minimiert.\\

Die Trainingsprogramme müssen auch in der Lage sein, mit der Multi-\ac{gpu}-Architektur umzugehen.
Dazu wird die Bibliothek DeepSpeed \citep{deepspeed} in Kombination mit der Bibliothek Transformers verwendet.
DeepSpeed erlaubt verschiedene Arten der Parallelisierung, der Fokus liegt hier auf ZeRO 2 \citep{ZeRO}.
Die ZeRO Parallelisierung erlaubt eine verteilte parallele Datenverarbeitung, bei der das Modell auf mehrere \ac{gpu}s aufgeteilt wird.
Ohne diese Option ist ein Training nicht möglich, da, wie oben beschrieben, einzelne Modelle nicht vollständig auf eine \ac{gpu} passen.
Genauere Erläuterungen zur Konfiguration von DeepSpeed und zur Verwendung der Transformer-Bibliothek sind in \cref{ch:solution} beschrieben.
% TODO: Kapitel Solutions reminder

\section{Klausurfragen}\label{sec:approach:questions}
Zur Überprüfung der Modelle werden Klausurfragen verwendet.
Diese Klausurfragen ergeben sich aus den mündlichen Prüfungsfragen des Moduls \enquote{Architektur von Informationssystemen im Gesundheitswesen},
den schriftlichen Prüfungsfragen des Moduls \enquote{Informationssysteme in medizinischer Versorgung und Forschung} und aus Teilen des Buches \citet{bb}.
Die Fragen sind in drei Kategorien unterteilt:
\begin{enumerate}
    \item Einzelfragen, die einen bestimmten Sachverhalt abfragen
    \item Multi-Fakten-Fragen, die mehrere Fakten abfragen
    \item Transferfragen, die einen Sachverhalt von einem Kontext in einen anderen übertragen
\end{enumerate}

Allerdings ist die Anzahl der durchgeführten Klausuren zum Zeitpunkt der Erstellung dieser Arbeit geringer als erwartet, da der Studiengang erst seit 2021 existiert.
Dies führt zu einer geringeren Qualität der Bewertung,
hat aber keinen Einfluss auf die Qualität des Modells.\\

Die LLaMA-Modelle sind ohne weiteres Fine-Tuning verfügbar und wurden daher nicht an die Aufgabe der Beantwortung von Fragen angepasst.
Dies hat zur Folge, dass die Modelle bei einer Frage wie \enquote{Welche Farbe hat der Himmel?} nicht die Antwort liefern, sondern den Text im gleichen Stil mit weiteren Fragen fortsetzen.
Um eine Antwort auf eine Frage zu erhalten, muss die Frage in eine unvollständige Aussage umformuliert werden.
Im gegebenen Beispiel wird aus \enquote{Welche Farbe hat der Himmel?} die Aussage \enquote{Der Himmel hat die Farbe}, die dann vom Modell mit \enquote{blau.} beantwortet wird.
Dieser Prozess der Umformulierung von Fragen wird für alle vorhandenen Fragen durchgeführt.
Das \ac{gpt}-4-Modell wurde auf die Aufgabe der Beantwortung von Fragen trainiert und kann daher mit normalen Fragen evaluiert werden.\\

Einige Fragen erfordern das Zeichnen von Systemen oder das Verbinden von Begriffen mit Pfeilen.
Da das Modell keine Bilder erzeugen kann, werden diese Fragen so umformuliert, dass sie als Textfragen beantwortet werden können.
Eine Skizze wird entsprechend in eine Systembeschreibung umformuliert und eine Begriffsverbindung in eine Begriffsliste.
Andere Fragen bauen auf Lösungen der vorhergehenden Frage auf.
Dies ist nur dann sinnvoll, wenn das Modell den Verlauf der Fragestellungen kennt, was nicht der Fall ist.
Diese Fragen müssen ebenfalls umformuliert werden, um unabhängig von anderen Fragen zu sein.

\section{Modellevaluation}\label{sec:approach:comparison}
Der Vergleich der Modelle erfolgt im Rahmen einer manuellen Bewertung.
Dazu werden die in \cref{sec:approach:questions} definierten Fragen verwendet.
Diese Fragen werden von dem LLaMA Modell 7B sowohl vor als auch nach dem Continual Pretraining und von GPT-4 beantwortet.
Die Antworten werden manuell ausgewertet und mit den Antworten aus dem Buch \citet{bb} verglichen.
Die Lösungen werden ebenfalls in drei Kategorien eingeteilt:
\begin{enumerate}
    \item Richtig, wenn die Antwort mit der Antwort im Buch übereinstimmt
    \item Falsch, wenn die Antwort falsches Wissen enthält
    \item Nicht beantwortet, wenn der generierte Text keinen Bezug zur Frage hat
\end{enumerate}

Die Modelle werden mit Hilfe der micro-F1 Bewertung verglichen.
Die Definition folgt den unter \citet{chatgpt_qas} beschriebenen Formeln:
\begin{ceqn}
\begin{align}
    C &= S \cap G \\
    prec &= \frac{|C_{q}|}{|S_{q}|} \\
    recall &= \frac{|C_{q}|}{|G_{q}|} \\
    F1 &= 2\frac{prec \cdot recall}{prec + recall}
\end{align}
\end{ceqn}
Dabei ist $S_q$ die Menge der für die Fragen $q$ generierten Antworten, $G_q$ die Menge der richtigen Antworten aus dem Buch \citet{bb} und $C_q$ die Menge der richtigen Antworten, die sowohl in $S_q$ als auch in $G_q$ enthalten sind.
$prec$ ist die Präzision, $recall$ der Recall und $F1$ der F1-Wert pro Frage.
\begin{definition}
    \textbf{Präzision} ist der Anteil der richtigen Antworten an allen Antworten des Modells.
\end{definition}

\begin{definition}
    \textbf{Recall} ist der Anteil der richtig Antworten an alle korrekten Antworten des Evaluierungsdatensatzes.
\end{definition}

\begin{definition}
    Der \textbf{F1} Wert ist das harmonische Mittel aus Präzision und Recall.
\end{definition}

\begin{definition}\label{def:micro-f1}
    Der \textbf{Micro-F1} Wert beschreibt einen Gesamt-F1 Wert über alle Fragen und deren Antworten.
\end{definition}

\begin{definition}\label{def:macro-f1}
    Der \textbf{Macro-F1} Wert beschreibt das Mittel aller F1 Werte für jede Frage.
    In einem unbalancierten Datensatz ist dies eine bessere Metrik, um alle Fragen gleich zu gewichten.
\end{definition}

Fragen gelten als beantwortet, wenn sie den Kategorien 1 und 2 entsprechen.
Dieselben Berechnungen werden insgesamt und getrennt für alle drei Fragekategorien durchgeführt.
Es wird der Makro-F1 Wert genutzt um eine bessere Metrik unabhängig einzelner Ausreißer zu erhalten.
Wie in \citet{qald9} beschrieben, werden größtenteils Makro-F1 Werte in Evaluierung verwendet.
Der Mikro-F1 Wert ist sensitiv zu einzelnen Fehlschlägen des Modells.
Generiert z.B. das Modell $1000$ falsche Antworten zu einer aus $100$ Fragen, beantwortet jedoch alle anderen Fragen korrekt, sinkt der Mikro-F1 Wert stark.
Jedoch ist das Modell in diesem Fall als sehr gut anzusehen, da es $99\%$ der Fragen korrekt beantwortet.
Der Makro-F1 Wert ist hier repräsentiver, da er jede einzelne Frage gleich gewichtet.\\

Den in \citet{chatgpt_qas} beschriebenen Ansatz folgend werden die Modelle auf die Kriterien Korrektheit, Determinismus, Robustheit, Erklärbarkeit und Fragenverständnis evaluiert.
Die Kriterien \enquote{Nutzung aktueller Informationen} und \enquote{Generalisierung über verschiedene Domänen} werden nicht betrachtet, da die Modelle keinen Zugriff auf aktuelle Informationen und verschiedene Domänen haben.\\

\noindent\textbf{Korrektheit:}\newline
Die Korrektheit wird durch die oben beschriebenen Formeln berechnet und verglichen.
Eine höhere Korrektheit entspricht einem besseren Ergebnis.
Bei der Anwendung des Modells ist eine hohe Präzision wichtiger als ein hoher Recall, da eine falsche Antwort für einen unkundigen Anwender wesentlich schädlicher ist als keine Antwort.
Die Modelle sollen immer keine Antwort einer falschen Antwort vorziehen.\\

\noindent\textbf{Determinismus:}\newline
Determinismus beschreibt die Reproduzierbarkeit von Antworten.
Für die gleiche Frage und das gleiche Modell sollen die Antworten immer gleich sein.
Sprachmodelle zeigen ein gewisses Maß an nichtdeterministischem Verhalten, daher ist diese Statistik ein wichtiges Kriterium.
Um dies zu erreichen, werden Fragen 3 mal an die Modelle gestellt und die beste Antwort wird zur Berechnung der Korrektheit verwendet.
Dabei wird auch diese Statistik berechnet.
Eine Antwort gilt als deterministisch, wenn die gleichen Fakten mit leichten Umformulierungen in der Antwort enthalten sind.\\

\noindent\textbf{Robustheit:}\newline
Die Robustheit beschreibt die Fähigkeit des Modells, auch bei fehlerhaften Eingaben korrekte Antworten zu generieren.
Dazu werden grammatikalische Fehler sowie Rechtschreibfehler in eine Teilmenge der Fragen eingefügt und die Korrektheit der Antworten berechnet.
Eine Antwort wird als robust bezeichnet, wenn sie auch bei fehlerhaften Eingaben korrekt ist.\\

\noindent\textbf{Erklärbarkeit:}\newline
Die Erklärbarkeit beschreibt die Fähigkeit des Modells, die erzeugten Antworten zu erklären.
Dazu werden die Modellantworten manuell daraufhin überprüft, ob sie eine Erklärung enthalten.
Ist dies nicht der Fall, wird eine zusätzliche Eingabe erzeugt, der nach einer Erklärung fragt.
Eine Antwort gilt als erklärbar, wenn sie eine Erklärung enthält.
Die Ergebnisse werden in \enquote{ohne zusätzliche Eingabe} und \enquote{mit zusätzlicher Eingabe} unterschieden.\\

\noindent\textbf{Fragenverständnis:}\newline
Das Fragenverständnis beschreibt die Fähigkeit des Modells, die Frage zu verstehen.
Dazu werden die Antworten der Modelle manuell daraufhin überprüft, ob sie die Frage verstanden haben. Eine Frage gilt als verstanden, wenn sich die Antwort inhaltlich auf die Frage und deren Sachverhalt bezieht. Die Antwort muss nicht korrekt sein.
