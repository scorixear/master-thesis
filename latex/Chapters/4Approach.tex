%*****************************************
\chapter{Lösungsansatz}\label{ch:approach}
%*****************************************
\section*{Vorgehen}
- Rerchnernetz Aufbau und Konfiguration (warum reicht nicht eigener Computer)

- Auswahl Autoregressives Modell
- Datenkuration Blaues Buch
- Unüberwacht Weitertrainiert = Continual Pretraining
    - Nutzung von HuggingFace Trainer
    - Parallelisierung auf Rechennetz

- Auswahl Beispielklausuren
- Beantwortung von Fragen

- Modellvergleich
    - Omar folgende Kriterien
    - Precision, Recall, F-Score als Benchmark
- Modellvergleich GPT4
    - Omar folgende Kriterien

\section{Auswahl von Sprachmodellen}

Für die Auswahl eines Sprachmodells müssen verschiedene Kriterien erfüllt sein.
Ausgehend von der Aufgabenstellung in \ref{sec:zielsetzung} stehen verschiedene Modelle der Transformer-Architektur zur Auswahl.
Die Auswahl beschränkt sich auf Decoder-basierte Modelle, da diese die Eigenschaften eines autoregressiven Modells besitzen.
Wie bereits in \ref{sec:grundlagen:transformer} beschrieben, ist das Ziel die Generierung von Text auf Basis einer Fragestellung. % TODO: grundlagen Kapitel + Label
Diese Fragestellung wird zu Beginn als Eingabe zur Verfügung gestellt, woraufhin das Modell kontinuierlich neue Tokens generiert.
Neu generierte Tokens werden in Verbindung mit der ursprünglichen Fragestellung als Eingabe verwendet, um das nächste Token zu generieren.
Dieses Verhalten entspricht autoregressiven Modellen und erfordert Decoder-basierte Modelle.
Ein Decoder-basiertes Modell verwendet alle vorhergehenden Tokens zur Generierung von Tokens (auch als Vorhersage von Tokens bezeichnet), während ein Encoder-Modell sowohl vorhergehende als auch nachfolgende Tokens verwendet.
Encoder-basierte Modelle sind für die Textvervollständigung oder Textklassifikation gedacht, aber nicht für die Textgenerierung.\\

Darüber hinaus sind folgende Eigenschaften erwünscht.
Das Modell sollte eine gute ZeroShot-Performance haben und nicht nur eine gute FewShot- oder OneShot-Performance.
Diese Arbeit untersucht die Fähigkeit eines Modells, Wissen aus domänenspezifischer Literatur (hier aus \citet{bb}) zu reproduzieren.
Durch die Verwendung früherer Fragen und Antworten als Kontext werden die Ergebnisse synthetisch durch zusätzliches Wissen im Kontext verbessert.
Einige Fragen könnten nicht durch das Modell mit Hilfe des gelernten Wissens beantwortet werden, sondern durch das Wissen im Kontext.
Das Modell sollte auch ohne Fine-Tuning eine gute Leistung erbringen.
Da ein Fine-Tuning in dieser Arbeit aufgrund der fehlenden Datenmenge nicht möglich ist, sollte ein Modell gewählt werden, das auch ohne diess Fine-Tuning eine gute Performance erreicht.\\

Die hier zur Auswahl stehenden Modelle sind GPT-2, GPT-3, GPT-4, GPT-J, GPT-NeoX und LLaMa.
Modelle, die sehr populär geworden sind, aber nicht zur Auswahl stehen, sind BERT, RoBERTa, DistillBERT, BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5.
Wenn die Aufgabe des Question Answering gelöst werden soll, ist ein BERT-basiertes Modell die erste Wahl.

Modelle wie BERT, RoBERTa und DistillBERT besitzen aufgrund ihrer Encoder-Architektur eine optimale Eigenschaft zur Informationsextraktion aus Text.
Allerdings ist dieser Ansatz in zwei Punkten eingeschränkt.
Die Modelle lernen nicht Fakten und können auf Basis einer Fragestellung neue Antworten formulieren, sondern finden Textstellen in einem Kontext, die die Fragestellung beantworten.
Es findet keine Textgenerierung statt, sondern es werden Ausschnitte aus einem gegebenen Textkontext als Antwort gefunden.
Dieser Kontext ist wiederum durch die Größe des Modells begrenzt und beschränkt sich in den meisten Fällen auf 512 Tokens.
Ist die Antwort auf die Frage nicht im Kontext enthalten, kann das Modell keine Antwort finden.
Da die Fragen unabhängig vom Kontext beantwortet werden sollen, können diese Modelle nicht verwendet werden.
Andere Modelle wie BART, T5, Opus, Pegasus, DialoGPT, Blenderbot und Flan-T5 sind für andere Aufgabenstellungen konzipiert und können nicht auf die gegebene Aufgabenstellung angewendet werden.
Sie sind für Textklassifikation, Textübersetzung, Textzusammenfassung, Dialoge oder Text-2-Text-Aufgaben ausgelegt.\\

GPT-2 ist das kleinste Modell der OpenAI GPT-Serie und wurde erstmals in \citet{gpt2} vorgestellt.
Aufgrund seiner geringen Größe kann das Modell auf einzelnen GPUs trainiert und verwendet (inferiert) werden, was die Nutzung des Modells erheblich vereinfacht.
Außerdem ist es frei verfügbar und kann ohne weitere Maßnahmen verwendet werden.
Allerdings ist die Performance in der Einstellung ZeroShot nicht ausreichend.
Das Modell erreicht eine Imitation von Texten, aber kein Verständnis und keine Wiedergabe von Fakten.\\

GPT-3 und das darauf aufbauende GPT-3.5 (ChatGPT), vorgestellt in \citet{gpt3}, kann zum Zeitpunkt der Arbeit kostenlos genutzt werden, hat eine sehr gute ZeroShot-Performance und besitzt die notwendige Größe, um als QA-Modell zu fungieren.
Allerdings ist das Modell nicht frei verfügbar.
Es existiert zwar eine \ac{api}, diese dient jedoch nur zur Interferenz des Modells, ein Training ist nicht möglich.
Damit fällt das Modell bereits aus der Auswahl.
Ein weiteres Problem stellt die Größe des Modells dar.
Mit ca.
175 Milliarden Parametern ist eine Nutzung des Modells nur mit 8 A100 GPUs möglich.
Das Training benötigt zusätzlich etwa die 4-fache Leistung.
Diese Leistung kann zwar vom Rechenzentrum der Universität Leipzig bereitgestellt werden, verhindert aber den Einsatz des Modells in einer längerfristigen Umgebung.\\

GPT-4 ist das mächtigste Modell von OpenAI und wurde in \citet{gpt4} vorgestellt.
Auch dieses Modell ist nicht frei verfügbar und kann nur über eine \ac{api} genutzt werden.
Die Leistungsfähigkeit von GPT-4 ist jedoch deutlich höher als die von GPT-3, weshalb die Untersuchung dieses Modells als Ersatz für ein eigens trainiertes Modell geplant ist.
Die Größe des Modells wurde von OpenAI nicht veröffentlicht, ist aber definitiv größer als GPT-3, weshalb eine Verwendung hier allein aufgrund der Größe ausscheidet.\\

GPT-J erreicht mit 6,7 Milliarden Parametern die Größe des kleinsten Modells GPT-3, zeigt aber eine deutlich bessere Performance als GPT-2.
Das Modell ist unter \citet{gptj} veröffentlicht und frei verfügbar. % TODO: Citation
Die Leistungsfähigkeit des Modells ist im Vergleich zu den anderen Modellen in der Auswahl geringer und wird daher ausgeschlossen.\\

GPT-NeoX wurde in \citet{gpt_neox} vorgestellt und erreicht mit einer Größe von 20 Milliarden Parametern die Leistungsfähigkeit von GPT-3 mit 175 Milliarden Parametern.
Das Modell ist frei verfügbar und kann ohne weitere Maßnahmen verwendet werden.
Die Leistung des Modells ist vergleichbar mit GPT-3, während die Größe des Modells ein Training mit 8 A100 GPUs erlaubt.
Diese Leistung wird jedoch von LLaMa übertroffen und daher nicht ausgewählt.\\

Die in \citet{llama} vorgestellten LLaMa-Modelle haben einen besonderen Ansatz.
Durch die Ausrichtung auf längeres Training und größere Datensätze bieten sie parametereffiziente Modelle, die sehr gute Leistungen erzielen können.
Um die bestmögliche Performance mit den kleinstmöglichen Modellen zu erreichen, werden die Modelle von LLaMa ausgewählt.
Hier stellt sich die Frage, welches Modell aus der LLaMa-Reihe verwendet werden soll.
Die Modelle unterscheiden sich sowohl in der Größe als auch proportional in der Leistung.
Abbildung \ref{llama_naturalquestion} zeigt die Leistung der Modelle im Vergleich zu anderen Modellen und in verschiedenen Einstellungen (ZeroShot bis 64Shot). % TODO: Abbildung aus LLama Paper
Selbst das kleinste Modell mit 7 Milliarden Parametern übertrifft die Leistung von GPT-3 und kann während der Interferenz auf einer einzelnen V100 GPU verwendet werden.
Nach Angaben der Autoren kann auch das 13 Milliarden Parameter Modell auf einer einzelnen V100 GPU genutzt werden, was für die Wahl dieses Modells spricht.
Das 33 Milliarden Parameter Modell erzielt sogar etwas bessere Ergebnisse im ZeroShot als das 65 Milliarden Parameter Modell, weshalb hier die Wahl des 33 Milliarden Parameter Modells sinnvoll ist.
Mit 33 Milliarden Parametern und der Verwendung von Float16 Werten (auch Half-Precision genannt) benötigt das Modell 66 GB GPU RAM, um für die Interferenz verwendet werden zu können.
Für das Training werden 264 GB GPU RAM benötigt, was 4 A100 GPUs entspricht.
Das Modell mit 65 Milliarden Parametern benötigt 7 A100 GPUs für das Training und 2 A100 GPUs für die Nutzung.
Aufgrund der vielversprechenden ZeroShot Performance wird in dieser Arbeit das 33 Milliarden Parameter Modell gewählt.
Die Modelle sind jedoch nicht öffentlich verfügbar, sondern werden nur zu Forschungszwecken über einen Antrag veröffentlicht.
Dieser Antrag wurde im Rahmen dieser Arbeit gestellt und genehmigt.\\

Eine Alternative zu den LLaMa-Modellen ist OpenLLaMA, veröffentlicht unter \citet{openllama}. % TODO: Citation
Die hier verfügbaren Modelle mit 3 Milliarden, 7 Milliarden und 13 Milliarden Parametern entsprechen in ihrer Leistungsfähigkeit den LLaMa-Modellen, sind aber unter der Open-Source-Lizenz Apache 2.0 verfügbar.
Da jedoch kein Modell mit 33 Milliarden Parametern verfügbar ist, wird auf diese Modelle verzichtet.\\

\section{Datenkuration}
Um die Literatur optimal nutzen zu können, muss eine Datenkuration durchgeführt werden.
Die Daten liegen im Word-Format vor, können jedoch nicht in diesem Format verarbeitet werden, da dass Modell lediglich Text verarbeiten kann.
Die Datenkuration enthält mehrere Schritte die im Folgenden beschrieben werden.\\

\subsection{Extraktion der Texte}
Die Extraktion der Text wird händisch durchgeführt.
Dazu wird das Word-Dokument geöffnet und der Text kopiert.
Der Text wird in einem Texteditor eingefügt und als \texttt{.md} Datei gespeichert.
\texttt{.md}-Dateien folgen dem Markdown-Format und haben die besondere Eigenschaft, mit Hilfe von Text-Symbolen Formatierungen wie Überschriften, Aufzählungen und Tabellen darzustellen.

\subsection{Unverständliche Formate}
Während der Extraktion des Textes fallen alle Bilder als auch Kopf- und Fußzeilen weg.
Da ein einzelne Text-Datei keine sinnvolle Unterteilung in Seiten hat, sind Kopf- und Fußzeilen und damit einhergehend Seitenzahlen nicht relevant.
Bilder können nicht von dem Modell verarbeitet werden und werden daher entfernt.
Die Bildunterschriften als auch Referenzen auf Bilder stehen somit ohne Kontext in den Kapiteln und werden ebenso entfernt.

\subsection{Textpassagen ohne Wissen oder Kontext}
Teile des Dokumentes enthalten kein Wissen, stehen in keinem Kontext oder sind für das Modell unverständlich formatiert.
Die Titel-Seite wird reduziert auf den Titel des Buches, das Vorwort entfällt da es kein Wissen zur Thematik enthält und Tabellen- und Bildverzeichnise entfallen ebenfalls.
Bild- und Tabellenverzeichnise enthalten Wissen, können aber in keinem Kontext gesetzt werden, da die genutzten Attention-Mechanismen des Modells nicht diese Verzeichnisse mit Texten in späteren Kapiteln verbinden können, da der Kontext zu groß ist.
Die Autorenvorstellung enthält ebenfalls kein Wissen zur Thematik und wird daher entfernt.
Zusätzlich werden fehlerhafte Textstellen wie fehlende Verweise entfernt.
Diese Textstellen existieren nur in der Word-Version des Buches, jedoch nicht in der finalen PDF-Version.
Da eine Extraktion von Text aus PDF-Dateien jedoch zu groben Problemen in der Formatierung von Text führt, wurde darauf verzichtet.

\subsection{Optionale Textentfernungen}
Es ist zu untersuchen, ob die Entfernung von Literaturverzeichnissen, Überschriften und Tabellen zu einer besseren Leistung führen.
Literaturverzeichnisse beziehen sich auf das vorangegangen Kapitel, sind jedoch in einem schwierigen Format.
Inbesondere \ac{url}s müssen vom Modell in Tokens zerlegt werden, die den Inhalt der \ac{url} nicht wiederspiegelt. 
Überschriften haben im Vergleich zum Text einen minimalen Anteil der Gesamtmenge an Tokens, wordurch ihr enthaltenes Wissen durchaus verloren geht.
Die für den Menschen verständliche Formatierung einer Überschrift hat keinen Einfluss auf die Bewertung des Modells.
Tabellen sind in einem Format, das vom Modell nicht nativ verarbeitet werden kann.
Selbst die Tabellendarstellung in Markdown ist zwar einlesbar, aber zu bezweifeln ob diese verständlich ist.
Durch die geringe Größe des Datensatzes, ist auch nicht zu erwarten, dass dieses Format verstanden wird.
Es wird daher hier eine Aufzählung als Ersatz für Tabellen genutzt.

\subsection{Bleibende Texte}
Das Abkürzungsverzeichnis und das Glossar bleiben im Text enthalten, da hier wichtiges Wissen zum Verständnis von eventuellen Fragen definiert wird.
Einige Fragen können Akronyme oder Fachbegriffe benutzen, welche von dem Modell verstanden werden sollten.

\subsection{Formatierung von Text}
Die Formatierung des Textes in dem Word-Dokument kann nicht beachtet werden.
Deshalb muss hier ein Ersatz in Form von Markdown-Formatierung gefunden werden.
Überschriften werden durch \texttt{\#} dargestellt, Aufzählungen durch \texttt{-} und \texttt{1.}.
Tabellen werden durch eine Aufzählung dargestellt, wobei die erste Zeile die Überschriften enthält und die zweite Zeile die Trennung zwischen Überschriften und Inhalt darstellt.
Die Trennung zwischen den Spalten wird durch \texttt{|} dargestellt.
Hochgestellte und tiefgestellte Zahlen werden durch \texttt{\^} und \texttt{\_} dargestellt.
Formeln werden in den \LaTeX Math-Modus umformuliert.

\subsection{Potentielle Extraktion von Fragen}
Neben der Nutzung des Textes als Eingabe in das Modell kann ebenso weiterer Nutzung gezogen werden.
Abschnitte, welche Übungsaufgaben darstellen, als auch das Glossar und das Abkürzungsverzeichnis können als Quelle für Fragen genutzt werden,
um das Modell und andere Modelle besser zu evaluieren.

\section{Unüberwachtes Weitertrainieren}
Um das LLaMa Modell zu trainieren, wird ein Programm benötigt, dass den Datensatz in Tokens umwandelt, und diese dann in Batches dem Modell übergibt.
Auf Basis dieser Batches wird das Modell genutzt um die Gewichte mit Backpropagation anzupassen.
Dieser Prozess stellt demnach eine Epoche dar.
Bei kleineren Datensätzen und Fine-Tuning wird häufig mehrere Epochen gewählt, während größere Datensätze (mehrere hundert Gigabyte) nur eine Epoche oder nicht einmal eine Epoche benötigen.
Mit steigender Epoche passt das Modell sich weiter an den Text der Literatur an, wobei ab einem bestimmten Zeitpunkt eine Grenze überschritten wird.
Diese Grenze wird häufig als \enquote{Overfitting} bezeichnet, und repräsentiert ein Modell, welches sich zu sehr an einen Datensatz angepasst hat.
Dadurch verliert das Modell die Fähigkeit, auf unbekannte Daten zu generalisieren, welches im diesem Anwendungsfall dazuführt, dass keine Fragen mehr beantwortet werden können.\\

Um Overfitting zu vermeiden, wird der Datensatz in Trainings- und Validierungsdatensatz aufgeteilt.
Hierbei wird eine Unterteilung von 90\% Training, 10\% Validierung genutzt.
Das Modell trainiert nur auf dem Trainingsdatensatz, wird jedoch in regelmäßigen Abschnitten mit dem Validierungsdatensatz überprüft.
Die Ergebnisse des Validierungsdatensatz haben keinen Einfluss auf die Anpassung der Gewichte sondern dienen nur zur Überprüfung, in wie fern das Modell generalisieren kann.
Erwartet wird ein Abfall der Kost-Funktion sowohl auf dem Trainings- als auch auf dem Validierungsdatensatz.
Dieser Abfall ist ein Indikator dafür, dass das Modell sich an den Datensatz anpasst.
Sobald der Abfall auf dem Validierungsdatensatz aufhört, ist das Training abgeschlossen.
Die Ergebnisse zeigen in den meisten Fällen, dass die Kostfunktion des Trainingsdatensatzes tiefer als die des Validierungsdatensatzes ist.
Solange die Differenz zwischen den beiden Kostfunktionen nicht zu groß ist, konnte das Modell erfolgreich trainiert werden.\\

Zur Durchführung dieser Trainingsschritte wird die Programmiersprache Python genutzt, da sie eine große Anzahl an Bibliotheken für die Verarbeitung von Texten und die Nutzung von neuronalen Netzen bietet.
Die Bibliothek Transformers von HuggingFace bietet eine Vielzahl an Modellen und Trainingsmethoden und baut auf der populären Bibliothek PyTorch auf.
PyTorch bietet mit Hilfe von CUDA die Möglichkeit, Trainingsschritte auf einer NVIDIA GPU durchzuführen, welches die Rechenschritte beschrieben in \ref{sec:grundlagen:training} um ein Vielfaches beschleunigt. % TODO: Rechenschritte in Grundlagen erklären, label hinzufügen
Ebenso kann mit Hilfe von DeepSpeed dieses Training auf mehrere GPUs aufgeteilt werden.
Das ist notwendig, da das verwendete Modell mit 33 Milliarden Parametern zu groß ist, um in den RAM-Speicher einer einzelnen GPU zu passen.

Bevor das Training beginnt, müssen einige Konfigurationsparameter festgelegt werden.
Diese Parameter sind in Tabelle~\ref{tab:training:parameter} aufgelistet und folgen den Training-Parametern von \citet{llama}.\\

\begin{table}
    \centering
    \begin{tabular}{ll}
        \hline
        \textbf{Parameter} & \textbf{Wert} \\
        \hline
        Lernrate & $1,5e^{-4}$\\
        AdamW $\beta_1$ & 0,9\\
        AdamW $\beta_2$ & 0,95\\
        Gewichtsreduktion & 0,1\\
        Gradientenlimitierung & 1,0\\
        Aufwärmphase & 0\\
        Epochen & 2\\
        \hline
    \end{tabular}
    \caption{Parameter für das Training des LLaMa 33B-Modells}
    \label{tab:training:parameter}
\end{table}

Die Lernrate (engl. \enquote{Learning Rate}) ist ein Parameter, der die Größe der Anpassung der Gewichte bestimmt.
Mit einer hohen Lernrate werden Gewichte stärker angepasst.
Dies kann dazu führen, dass das Modell nicht konvergiert, sondern über den optimalen Punkt hinausschießt.
Mit einer zu niedrigen Lernrate konvergiert das Modell nur sehr langsam.
Die Lernrate wird während des Trainings angepasst, um eine schnelle Konvergenz zu ermöglichen.
Diese Anpassing folgt der Cosinus-Learning-Rate Schedule, sodass die endgüligte Lernrate bei 10\% des ursprünglichen Wertes liegt.
Lernraten sind abhängig von der Größe des Modells.
Generell gilt, dass größere Modelle eine niedrigere Lernrate benötigen.\\
Der Optimierer AdamW übernimmt den zeitlichen Abfall der Gewichte und wurde  in \citet{adamw} vorgestellt. % TODO: Citation
Er ist in aktuellen Modellen die häufigste Wahl und wurde ebenso für das Training der LLaMa Modelle genutzt.\\

Gewichtsreduktion (engl. \enquote{Weight Decay}) ist ein Parameter, der die Gewichte während des Trainings reduziert.
Er stellt eine Regularisierung dar, die Overfitting verhindern soll.
$0,1$ entspricht einer Gewichtsreduktion von 10\%.\\

Gradientenlimitierung (engl. \enquote{Gradient Clipping}) ist ein Parameter, der die Größe der Gradienten begrenzt.
Dies ist notwendig, da die Gradienten bei großen Modellen während der Backpropagation mit steigender Tiefe immer größer werden.
Ohne eine Begrenzung würden die Gradienten hier exdponentiell steigen.
Ein Gradientenlimitierung limitiert die Gradienten auf $1.0$\\

Die Aufwärmphase beschreibt die Anzahl der Schritte, die das Modell benötigt, um die Lernrate von $0$ auf den ursprünglichen Wert linear zu erhöhen.
Dies ist notwendig, da zu Beginn eines Trainings Modelle zu schnell auf nicht allgemeingültige Muster anpassen.
Normalerweise wird dies durch ein längeres Training behoben, wodurch die zuerst gelernten Muster verlernt werden müssen.
Mit Hilfe der Aufwärmphase verhindert man ein verlängertes Training.
Da in diesem Fall das Modell bereits auf einem großen Datensatz trainiert wurde, ist eine Aufwärmphase nicht notwendig.\\

Der Artikel von \citet{llama} beschreibt ein Training über ein Epoche.
Da in diesem Fall jedoch der Datensatz minimal im Vergleich ist, wird eine höhere Anzahl an Epochen genutzt.\\

\subsection{Ausführen der Training-Programme}

Zur Ausführung der Trainingsprogramme wird SLURM genutzt \footnote{https://slurm.schedmd.com/overview.html (abgerufen am 16.6.2023)}.
Mit Hilfe von SLURM-Skripten können Programm über mehrere Nodes, CPUs und GPUs verteilt berechnet werden.
Diese Skript-Sprache wird vom Rechenzentrum der Universität Leipzig genutzt, um verschiedene Programme auf den Supercomputern Clara und Paula auszuführen.\\

Die Supercomputer Clara und Paula sind Rechennetzwerke, spezialisiert für GPU-beschleunigte Berechnung.
Paula besteht aus 12 Nodes, welche jeweils 8 Nvidia Tesla A30 GPUs besitzen.
Clara ist zweigeteilt und besteht aus 8 Nodes mit jeweils 4 Nvidia Tesla V100 GPUs und 22 Nodes mit jeweils 8 Nvidid GeForce RTX 2080 TI GPUs.
Die Nodes sind über ein Infiniband-Netzwerk verbunden, welches eine hohe Bandbreite und geringe Latenz bietet.\\

Supercomputer sind geteilte Ressourcen und können von mehreren Nutzern reserviert werden.
Daher ist die Auswahl der Supercomputer abhängig davon, welche Nodes zu welchem Zeitpunkt frei sind.
Generell werden Nodes von Clara mit V100 GPUs bevorzugt, da hier der größtmögliche Speicher zur Verfügung pro GPU zur Verfügung steht.
Dies führt dazu, dass weniger Nodes gleichzeitig reserviert werden müssen, somit die Wartezeit verkürzt wird und die Kommunikation zwischen Nodes minimiert wird.\\

Die Trainingsprogramme müssen ebenso mit der Multi-GPU Architektur umgehen können.
Hierfür wird die Bibliothek DeepSpeed \citep{deepspeed} in Kombination mit der Transformers Bibliothek genutzt. % TODO: Citation
DeepSpeed ermöglicht verschiedene Arten der Parallelisierung, der Fokus ist hier ZeRO 2 \citep{ZeRO}. Die ZeRO Parallelisierung erlaubt verteilte parallele Datenverarbeitung, wodurch das Modell auf mehrere GPUs aufgeteilt wird. % TODO: Citation
Ohne diese Option ist ein Training nicht möglich, da wie bereits beschrieben einzelne Modelle nicht vollständig auf eine GPU passen.
Genauere Erklärung zur Konfiguration von DeepSpeed und der Nutzung der Transformers Bibliothek sind in \ref{ch:solution} beschrieben.
% TODO: Kapitel Solutions reminder

\section{Klausurfragen}
- Klausurfragen Antwortenerstellung
- Umformulierung zu Prompt (da Finetuning nicht vorhanden)
% TODO: Klausurfragen Zugang noch ausstehend

\section{Modellvergleich}
- Berechnung von Precision, Recall, F-Score
- Aggregation der Ergebnisse

Nutzung von Omar Kriterien:
    - Correctness
    - Determinism
    - Robustness
    - Explainability
    - Question understanding

Nicht genutzte Omar Kriterien
    - Incorporating recent Information
    - Generailty across different domains
    
- Vergleich der Modelle
- Vergleich mit SOTA Modell
    - Nutzung von Kontext als Input?

