#!/bin/bash
#SBATCH --job-name=distribution-test        # name
#SBATCH --nodes=2                    # nodes
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=4           # number of cores per tasks
#SBATCH --partition=clara
#SBATCH --gres=gpu:v100:2            # number of gpus
#SBATCH --time 0:15:00               # maximum execution time (HH:MM:SS)
#SBATCH --output=%x-%j.out           # output file name

# load modules
module load Python
pip install --user -r requirements.txt
export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
# export MASTER_PORT=2025
export WORLD_SIZE=4
# Retrieve the IP address of the current node
IP_ADDRESS=$(srun hostname --ip-address | head -n 1)
echo "IP_ADDRESS="$IP_ADDRESS

GPUS_PER_NODE=2
echo "NODELIST="${SLURM_NODELIST}
MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
echo "MASTER_ADDR="$MASTER_ADDR

echo "LOGLEVEL=INFO python -m torch.distributed.run --rdzv_id=$SLURM_JOBID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES  torch-distributed-gpu-test.py"
# echo "python -m torch.distributed.run --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID torch-distributed-gpu-test.py"
LOGLEVEL=INFO python -m torch.distributed.launch --rdzv_id=$SLURM_JOBID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR\:$MASTER_PORT --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES  torch-distributed-gpu-test.py

# srun --jobid $SLURM_JOBID bash -c "python -m torch.distributed.run --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID torch-distributed-gpu-test.py"